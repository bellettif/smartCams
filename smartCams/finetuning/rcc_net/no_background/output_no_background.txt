Last login: Thu Apr 30 13:05:34 on ttys002
C02MX066FD58:~ cusgadmin$ ls
Applications			beats
Box Sync			caffe
Collect_tweets			data_e-mission
Creative Cloud Files		duckcurve
Creative Cloud Files (unknown)	e-mission
Desktop				eclipse_kepler
Documents			eclipse_luna
Downloads			matlab_crash_dump.16825-1
Dropbox				matlab_crash_dump.46603-1
Google Drive			mongo_log.s
Google Earth.app		nano.save
Grammar_TS			new_collect.py
Library				nohup.out
Movies				null.txt
MultimodePIF			pf_project
Music				projectARZ
Objective-C			root
Pictures			scripts_e-mission
Public				smartCams
Qt				temp.txt
Untitled0.ipynb
C02MX066FD58:~ cusgadmin$ ls
Applications			beats
Box Sync			caffe
Collect_tweets			data_e-mission
Creative Cloud Files		duckcurve
Creative Cloud Files (unknown)	e-mission
Desktop				eclipse_kepler
Documents			eclipse_luna
Downloads			matlab_crash_dump.16825-1
Dropbox				matlab_crash_dump.46603-1
Google Drive			mongo_log.s
Google Earth.app		nano.save
Grammar_TS			new_collect.py
Library				nohup.out
Movies				null.txt
MultimodePIF			pf_project
Music				projectARZ
Objective-C			root
Pictures			scripts_e-mission
Public				smartCams
Qt				temp.txt
Untitled0.ipynb
C02MX066FD58:~ cusgadmin$ cd smartCams/
C02MX066FD58:smartCams cusgadmin$ cd Wksp/
C02MX066FD58:Wksp cusgadmin$ git add -A *.prototxt
C02MX066FD58:Wksp cusgadmin$ git add -A *.ipynb
C02MX066FD58:Wksp cusgadmin$ git add -A *.py
C02MX066FD58:Wksp cusgadmin$ git add -A *.txt
C02MX066FD58:Wksp cusgadmin$ git commit -am "Created featurizer"
[master d172a94] Created featurizer
 25 files changed, 148116 insertions(+), 4 deletions(-)
 create mode 100644 smartCams/NIN_net/solver.prototxt
 create mode 100644 smartCams/NIN_net/train_val.prototxt
 create mode 100644 smartCams/deep_learning/.ipynb_checkpoints/net_surgery-checkpoint.ipynb
 create mode 100644 smartCams/deep_learning/features_rcnn.py
 create mode 100644 smartCams/deep_learning/net_surgery_example.ipynb
 create mode 100644 smartCams/deep_learning/net_surgery_example/bvlc_caffenet_full_conv.prototxt
 create mode 100644 smartCams/deep_learning/net_surgery_example/conv.prototxt
 create mode 100644 smartCams/deep_learning/net_surgery_example/deploy.prototxt
 create mode 100644 smartCams/deep_learning/null.txt
 create mode 100644 smartCams/deep_learning/rcnn_model/deploy.prototxt
 create mode 100644 smartCams/deep_learning/rcnn_model/train_val.prototxt
 create mode 100644 smartCams/finetuning/VOC_cropped_warped_test_all.txt
 create mode 100644 smartCams/finetuning/VOC_cropped_warped_train_all.txt
 create mode 100644 smartCams/train_VOC2012/VOC_cropped_warped_test_all.txt
 create mode 100644 smartCams/train_VOC2012/VOC_cropped_warped_test_small.txt
 create mode 100644 smartCams/train_VOC2012/VOC_cropped_warped_train_all.txt
 create mode 100644 smartCams/train_VOC2012/VOC_cropped_warped_train_small.txt
 create mode 100644 smartCams/train_VOC2012/VOC_windows.txt
 create mode 100644 smartCams/train_VOC2012/VOC_windows_small.txt
 create mode 100644 smartCams/train_VOC2012/compute_mean_image.py
 create mode 100644 smartCams/train_VOC2012/compute_substracted_images.py
 create mode 100644 smartCams/train_VOC2012/read_me.txt
C02MX066FD58:Wksp cusgadmin$ git push origin master
Counting objects: 32, done.
Delta compression using up to 8 threads.
Compressing objects: 100% (32/32), done.
Writing objects: 100% (32/32), 1.04 MiB | 0 bytes/s, done.
Total 32 (delta 11), reused 0 (delta 0)
To https://github.com/bellettif/smartCams.git
   4e07e54..d172a94  master -> master
C02MX066FD58:Wksp cusgadmin$ cd smartCams/deep_learning/rcnn_model/
C02MX066FD58:rcnn_model cusgadmin$ ls
bvlc_reference_rcnn_ilsvrc13.caffemodel	train_val.prototxt
deploy.prototxt
C02MX066FD58:rcnn_model cusgadmin$ cd ..
C02MX066FD58:deep_learning cusgadmin$ cd ..
C02MX066FD58:smartCams cusgadmin$ cd finetuning/
C02MX066FD58:finetuning cusgadmin$ ls
VOC_cropped_warped_test_all.txt		VOC_windows.txt
VOC_cropped_warped_test_small.txt	rcc_net
VOC_cropped_warped_train_all.txt	warped_data
VOC_cropped_warped_train_small.txt
C02MX066FD58:finetuning cusgadmin$ cd rcc_net/
C02MX066FD58:rcc_net cusgadmin$ ls
bvlc_reference_rcnn_ilsvrc13.caffemodel
bvlc_reference_rcnn_ilsvrc13.caffemodel.bak
caffenet_train
deploy.prototxt
deploy.prototxt.bak
solver.prototxt
train_val.prototxt
train_val.prototxt.bak
C02MX066FD58:rcc_net cusgadmin$ rsync -avz cs280-aw@s349-12:~/finetuning/rcc_net$ pwd
cs280-aw@s349-12's password: 
rsync error: received SIGINT, SIGTERM, or SIGHUP (code 20) at /SourceCache/rsync/rsync-45/rsync/rsync.c(244) [receiver=2.6.9]
C02MX066FD58:rcc_net cusgadmin$ rsync -avz * cs280-aw@s349-12:~/finetuning/rcc_net/
cs280-aw@s349-12's password: 
building file list ... done
bvlc_reference_rcnn_ilsvrc13.caffemodel
bvlc_reference_rcnn_ilsvrc13.caffemodel.bak
deploy.prototxt
deploy.prototxt.bak
solver.prototxt
train_val.prototxt
train_val.prototxt.bak

sent 428222028 bytes  received 174 bytes  3160311.45 bytes/sec
total size is 461519914  speedup is 1.08
C02MX066FD58:rcc_net cusgadmin$ cd ..
C02MX066FD58:finetuning cusgadmin$ ls
rcc_net
C02MX066FD58:finetuning cusgadmin$ cd ..
C02MX066FD58:smartCams cusgadmin$ ls
ImageNet		image_dump		processed_imgs
VOC2012			input_output		region_proposals
deep_learning		misc_caffe		segmentation
finetuning		preprocess.ipynb	train_VOC2012
C02MX066FD58:smartCams cusgadmin$ ls
ImageNet		image_dump		processed_imgs
VOC2012			input_output		region_proposals
deep_learning		misc_caffe		segmentation
finetuning		preprocess.ipynb	train_VOC2012
C02MX066FD58:smartCams cusgadmin$ rsync -avz finetuning/ cs280-aw@s349-13:~/
The authenticity of host 's349-13 (128.32.112.247)' can't be established.
RSA key fingerprint is 91:72:ec:91:af:e0:95:54:79:d0:37:bc:51:06:71:6f.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added 's349-13' (RSA) to the list of known hosts.
cs280-aw@s349-13's password: 
building file list ... done
./
.DS_Store
cropped_ms/
cropped_ms/pict_0.jpg
cropped_ms/pict_1.jpg
cropped_ms/pict_10.jpg
cropped_ms/pict_100.jpg
cropped_ms/pict_1000.jpg
cropped_ms/pict_10000.jpg
cropped_ms/pict_10001.jpg
cropped_ms/pict_10002.jpg
cropped_ms/pict_10003.jpg
cropped_ms/pict_10004.jpg
cropped_ms/pict_10005.jpg
cropped_ms/pict_10006.jpg
cropped_ms/pict_10007.jpg
cropped_ms/pict_10008.jpg
cropped_ms/pict_10009.jpg
cropped_ms/pict_1001.jpg
cropped_ms/pict_10010.jpg
cropped_ms/pict_10011.jpg
cropped_ms/pict_10012.jpg
cropped_ms/pict_10013.jpg
cropped_ms/pict_10014.jpg
cropped_ms/pict_10015.jpg
cropped_ms/pict_10016.jpg
cropped_ms/pict_10017.jpg
cropped_ms/pict_10018.jpg
cropped_ms/pict_10019.jpg
cropped_ms/pict_1002.jpg
cropped_ms/pict_10020.jpg
cropped_ms/pict_10021.jpg
cropped_ms/pict_10022.jpg
cropped_ms/pict_10023.jpg
cropped_ms/pict_10024.jpg
cropped_ms/pict_10025.jpg
cropped_ms/pict_10026.jpg
cropped_ms/pict_10027.jpg
cropped_ms/pict_10028.jpg
cropped_ms/pict_10029.jpg
cropped_ms/pict_1003.jpg
cropped_ms/pict_10030.jpg
cropped_ms/pict_10031.jpg
cropped_ms/pict_10032.jpg
cropped_ms/pict_10033.jpg
cropped_ms/pict_10034.jpg
cropped_ms/pict_10035.jpg
cropped_ms/pict_10036.jpg
cropped_ms/pict_10037.jpg
cropped_ms/pict_10038.jpg
cropped_ms/pict_10039.jpg
cropped_ms/pict_1004.jpg
cropped_ms/pict_10040.jpg
cropped_ms/pict_10041.jpg
cropped_ms/pict_10042.jpg
cropped_ms/pict_10043.jpg
cropped_ms/pict_10044.jpg
cropped_ms/pict_10045.jpg
cropped_ms/pict_10046.jpg
cropped_ms/pict_10047.jpg
cropped_ms/pict_10048.jpg
cropped_ms/pict_10049.jpg
cropped_ms/pict_1005.jpg
cropped_ms/pict_10050.jpg
cropped_ms/pict_10051.jpg
cropped_ms/pict_10052.jpg
cropped_ms/pict_10053.jpg
cropped_ms/pict_10054.jpg
cropped_ms/pict_10055.jpg
cropped_ms/pict_10056.jpg
cropped_ms/pict_10057.jpg
cropped_ms/pict_10058.jpg
cropped_ms/pict_10059.jpg
cropped_ms/pict_1006.jpg
cropped_ms/pict_10060.jpg
cropped_ms/pict_10061.jpg
cropped_ms/pict_10062.jpg
cropped_ms/pict_10063.jpg
cropped_ms/pict_10064.jpg
cropped_ms/pict_10065.jpg
cropped_ms/pict_10066.jpg
cropped_ms/pict_10067.jpg
cropped_ms/pict_10068.jpg
cropped_ms/pict_10069.jpg
cropped_ms/pict_1007.jpg
cropped_ms/pict_10070.jpg
cropped_ms/pict_10071.jpg
cropped_ms/pict_10072.jpg
cropped_ms/pict_10073.jpg
cropped_ms/pict_10074.jpg
cropped_ms/pict_10075.jpg
cropped_ms/pict_10076.jpg
cropped_ms/pict_10077.jpg
cropped_ms/pict_10078.jpg
cropped_ms/pict_10079.jpg
cropped_ms/pict_1008.jpg
cropped_ms/pict_10080.jpg
cropped_ms/pict_10081.jpg
cropped_ms/pict_10082.jpg
cropped_ms/pict_10083.jpg
cropped_ms/pict_10084.jpg
cropped_ms/pict_10085.jpg
cropped_ms/pict_10086.jpg
cropped_ms/pict_10087.jpg
cropped_ms/pict_10088.jpg
cropped_ms/pict_10089.jpg
cropped_ms/pict_1009.jpg
cropped_ms/pict_10090.jpg
cropped_ms/pict_10091.jpg
cropped_ms/pict_10092.jpg
cropped_ms/pict_10093.jpg
cropped_ms/pict_10094.jpg
cropped_ms/pict_10095.jpg
cropped_ms/pict_10096.jpg
cropped_ms/pict_10097.jpg
cropped_ms/pict_10098.jpg
cropped_ms/pict_10099.jpg
cropped_ms/pict_101.jpg
cropped_ms/pict_1010.jpg
cropped_ms/pict_10100.jpg
cropped_ms/pict_10101.jpg
cropped_ms/pict_10102.jpg
cropped_ms/pict_10103.jpg
cropped_ms/pict_10104.jpg
cropped_ms/pict_10105.jpg
cropped_ms/pict_10106.jpg
cropped_ms/pict_10107.jpg
cropped_ms/pict_10108.jpg
cropped_ms/pict_10109.jpg
cropped_ms/pict_1011.jpg
cropped_ms/pict_10110.jpg
cropped_ms/pict_10111.jpg
cropped_ms/pict_10112.jpg
cropped_ms/pict_10113.jpg
cropped_ms/pict_10114.jpg
cropped_ms/pict_10115.jpg
cropped_ms/pict_10116.jpg
cropped_ms/pict_10117.jpg
cropped_ms/pict_10118.jpg
cropped_ms/pict_10119.jpg
cropped_ms/pict_1012.jpg
cropped_ms/pict_10120.jpg
cropped_ms/pict_10121.jpg
cropped_ms/pict_10122.jpg
cropped_ms/pict_10123.jpg
cropped_ms/pict_10124.jpg
cropped_ms/pict_10125.jpg
cropped_ms/pict_10126.jpg
cropped_ms/pict_10127.jpg
cropped_ms/pict_10128.jpg
cropped_ms/pict_10129.jpg
cropped_ms/pict_1013.jpg
cropped_ms/pict_10130.jpg
cropped_ms/pict_10131.jpg
cropped_ms/pict_10132.jpg
cropped_ms/pict_10133.jpg
cropped_ms/pict_10134.jpg
cropped_ms/pict_10135.jpg
cropped_ms/pict_10136.jpg
cropped_ms/pict_10137.jpg
cropped_ms/pict_10138.jpg
cropped_ms/pict_10139.jpg
cropped_ms/pict_1014.jpg
cropped_ms/pict_10140.jpg
cropped_ms/pict_10141.jpg
cropped_ms/pict_10142.jpg
cropped_ms/pict_10143.jpg
cropped_ms/pict_10144.jpg
cropped_ms/pict_10145.jpg
cropped_ms/pict_10146.jpg
cropped_ms/pict_10147.jpg
cropped_ms/pict_10148.jpg
cropped_ms/pict_10149.jpg
cropped_ms/pict_1015.jpg
cropped_ms/pict_10150.jpg
cropped_ms/pict_10151.jpg
cropped_ms/pict_10152.jpg
cropped_ms/pict_10153.jpg
cropped_ms/pict_10154.jpg
cropped_ms/pict_10155.jpg
cropped_ms/pict_10156.jpg
cropped_ms/pict_10157.jpg
cropped_ms/pict_10158.jpg
cropped_ms/pict_10159.jpg
cropped_ms/pict_1016.jpg
cropped_ms/pict_10160.jpg
cropped_ms/pict_10161.jpg
cropped_ms/pict_10162.jpg
cropped_ms/pict_10163.jpg
cropped_ms/pict_10164.jpg
cropped_ms/pict_10165.jpg
cropped_ms/pict_10166.jpg
cropped_ms/pict_10167.jpg
cropped_ms/pict_10168.jpg
cropped_ms/pict_10169.jpg
cropped_ms/pict_1017.jpg
cropped_ms/pict_10170.jpg
cropped_ms/pict_10171.jpg
cropped_ms/pict_10172.jpg
cropped_ms/pict_10173.jpg
cropped_ms/pict_10174.jpg
cropped_ms/pict_10175.jpg
cropped_ms/pict_10176.jpg
cropped_ms/pict_10177.jpg
cropped_ms/pict_10178.jpg
cropped_ms/pict_10179.jpg
cropped_ms/pict_1018.jpg
cropped_ms/pict_10180.jpg
cropped_ms/pict_10181.jpg
cropped_ms/pict_10182.jpg
cropped_ms/pict_10183.jpg
cropped_ms/pict_10184.jpg
cropped_ms/pict_10185.jpg
cropped_ms/pict_10186.jpg
cropped_ms/pict_10187.jpg
cropped_ms/pict_10188.jpg
cropped_ms/pict_10189.jpg
cropped_ms/pict_1019.jpg
cropped_ms/pict_10190.jpg
cropped_ms/pict_10191.jpg
cropped_ms/pict_10192.jpg
cropped_ms/pict_10193.jpg
cropped_ms/pict_10194.jpg
cropped_ms/pict_10195.jpg
cropped_ms/pict_10196.jpg
cropped_ms/pict_10197.jpg
cropped_ms/pict_10198.jpg
cropped_ms/pict_10199.jpg
cropped_ms/pict_102.jpg
cropped_ms/pict_1020.jpg
cropped_ms/pict_10200.jpg
cropped_ms/pict_10201.jpg
cropped_ms/pict_10202.jpg
cropped_ms/pict_10203.jpg
cropped_ms/pict_10204.jpg
cropped_ms/pict_10205.jpg
cropped_ms/pict_10206.jpg
cropped_ms/pict_10207.jpg
cropped_ms/pict_10208.jpg
cropped_ms/pict_10209.jpg
cropped_ms/pict_1021.jpg
cropped_ms/pict_10210.jpg
cropped_ms/pict_10211.jpg
cropped_ms/pict_10212.jpg
cropped_ms/pict_10213.jpg
cropped_ms/pict_10214.jpg
cropped_ms/pict_10215.jpg
cropped_ms/pict_10216.jpg
cropped_ms/pict_10217.jpg
cropped_ms/pict_10218.jpg
cropped_ms/pict_10219.jpg
cropped_ms/pict_1022.jpg
cropped_ms/pict_10220.jpg
cropped_ms/pict_10221.jpg
cropped_ms/pict_10222.jpg
cropped_ms/pict_10223.jpg
cropped_ms/pict_10224.jpg
cropped_ms/pict_10225.jpg
cropped_ms/pict_10226.jpg
cropped_ms/pict_10227.jpg
cropped_ms/pict_10228.jpg
cropped_ms/pict_10229.jpg
cropped_ms/pict_1023.jpg
cropped_ms/pict_10230.jpg
cropped_ms/pict_10231.jpg
cropped_ms/pict_10232.jpg
cropped_ms/pict_10233.jpg
cropped_ms/pict_10234.jpg
cropped_ms/pict_10235.jpg
cropped_ms/pict_10236.jpg
cropped_ms/pict_10237.jpg
cropped_ms/pict_10238.jpg
cropped_ms/pict_10239.jpg
cropped_ms/pict_1024.jpg
cropped_ms/pict_10240.jpg
cropped_ms/pict_10241.jpg
cropped_ms/pict_10242.jpg
cropped_ms/pict_10243.jpg
cropped_ms/pict_10244.jpg
cropped_ms/pict_10245.jpg
cropped_ms/pict_10246.jpg
cropped_ms/pict_10247.jpg
cropped_ms/pict_10248.jpg
cropped_ms/pict_10249.jpg
cropped_ms/pict_1025.jpg
cropped_ms/pict_10250.jpg
cropped_ms/pict_10251.jpg
cropped_ms/pict_10252.jpg
cropped_ms/pict_10253.jpg
cropped_ms/pict_10254.jpg
cropped_ms/pict_10255.jpg
cropped_ms/pict_10256.jpg
cropped_ms/pict_10257.jpg
cropped_ms/pict_10258.jpg
cropped_ms/pict_10259.jpg
cropped_ms/pict_1026.jpg
cropped_ms/pict_10260.jpg
cropped_ms/pict_10261.jpg
cropped_ms/pict_10262.jpg
cropped_ms/pict_10263.jpg
cropped_ms/pict_10264.jpg
cropped_ms/pict_10265.jpg
cropped_ms/pict_10266.jpg
cropped_ms/pict_10267.jpg
cropped_ms/pict_10268.jpg
cropped_ms/pict_10269.jpg
cropped_ms/pict_1027.jpg
cropped_ms/pict_10270.jpg
cropped_ms/pict_10271.jpg
cropped_ms/pict_10272.jpg
cropped_ms/pict_10273.jpg
cropped_ms/pict_10274.jpg
cropped_ms/pict_10275.jpg
cropped_ms/pict_10276.jpg
cropped_ms/pict_10277.jpg
cropped_ms/pict_10278.jpg
cropped_ms/pict_10279.jpg
cropped_ms/pict_1028.jpg
cropped_ms/pict_10280.jpg
cropped_ms/pict_10281.jpg
cropped_ms/pict_10282.jpg
cropped_ms/pict_10283.jpg
cropped_ms/pict_10284.jpg
cropped_ms/pict_10285.jpg
cropped_ms/pict_10286.jpg
cropped_ms/pict_10287.jpg
cropped_ms/pict_10288.jpg
cropped_ms/pict_10289.jpg
cropped_ms/pict_1029.jpg
cropped_ms/pict_10290.jpg
cropped_ms/pict_10291.jpg
cropped_ms/pict_10292.jpg
cropped_ms/pict_10293.jpg
cropped_ms/pict_10294.jpg
cropped_ms/pict_10295.jpg
cropped_ms/pict_10296.jpg
cropped_ms/pict_10297.jpg
cropped_ms/pict_10298.jpg
cropped_ms/pict_10299.jpg
cropped_ms/pict_103.jpg
cropped_ms/pict_1030.jpg
cropped_ms/pict_10300.jpg
cropped_ms/pict_10301.jpg
cropped_ms/pict_10302.jpg
cropped_ms/pict_10303.jpg
cropped_ms/pict_10304.jpg
cropped_ms/pict_10305.jpg
cropped_ms/pict_10306.jpg
cropped_ms/pict_10307.jpg
cropped_ms/pict_10308.jpg
cropped_ms/pict_10309.jpg
cropped_ms/pict_1031.jpg
cropped_ms/pict_10310.jpg
cropped_ms/pict_10311.jpg
cropped_ms/pict_10312.jpg
cropped_ms/pict_10313.jpg
cropped_ms/pict_10314.jpg
cropped_ms/pict_10315.jpg
cropped_ms/pict_10316.jpg
cropped_ms/pict_10317.jpg
cropped_ms/pict_10318.jpg
cropped_ms/pict_10319.jpg
cropped_ms/pict_1032.jpg
cropped_ms/pict_10320.jpg
cropped_ms/pict_10321.jpg
cropped_ms/pict_10322.jpg
cropped_ms/pict_10323.jpg
cropped_ms/pict_10324.jpg
cropped_ms/pict_10325.jpg
cropped_ms/pict_10326.jpg
cropped_ms/pict_10327.jpg
cropped_ms/pict_10328.jpg
cropped_ms/pict_10329.jpg
cropped_ms/pict_1033.jpg
cropped_ms/pict_10330.jpg
cropped_ms/pict_10331.jpg
cropped_ms/pict_10332.jpg
cropped_ms/pict_10333.jpg
cropped_ms/pict_10334.jpg
cropped_ms/pict_10335.jpg
cropped_ms/pict_10336.jpg
cropped_ms/pict_10337.jpg
cropped_ms/pict_10338.jpg
cropped_ms/pict_10339.jpg
cropped_ms/pict_1034.jpg
cropped_ms/pict_10340.jpg
cropped_ms/pict_10341.jpg
cropped_ms/pict_10342.jpg
cropped_ms/pict_10343.jpg
cropped_ms/pict_10344.jpg
cropped_ms/pict_10345.jpg
cropped_ms/pict_10346.jpg
cropped_ms/pict_10347.jpg
cropped_ms/pict_10348.jpg
cropped_ms/pict_10349.jpg
cropped_ms/pict_1035.jpg
cropped_ms/pict_10350.jpg
cropped_ms/pict_10351.jpg
cropped_ms/pict_10352.jpg
cropped_ms/pict_10353.jpg
cropped_ms/pict_10354.jpg
cropped_ms/pict_10355.jpg
cropped_ms/pict_10356.jpg
cropped_ms/pict_10357.jpg
cropped_ms/pict_10358.jpg
cropped_ms/pict_10359.jpg
cropped_ms/pict_1036.jpg
cropped_ms/pict_10360.jpg
cropped_ms/pict_10361.jpg
cropped_ms/pict_10362.jpg
cropped_ms/pict_10363.jpg
cropped_ms/pict_10364.jpg
cropped_ms/pict_10365.jpg
cropped_ms/pict_10366.jpg
cropped_ms/pict_10367.jpg
cropped_ms/pict_10368.jpg
cropped_ms/pict_10369.jpg
cropped_ms/pict_1037.jpg
cropped_ms/pict_10370.jpg
cropped_ms/pict_10371.jpg
cropped_ms/pict_10372.jpg
cropped_ms/pict_10373.jpg
cropped_ms/pict_10374.jpg
cropped_ms/pict_10375.jpg
cropped_ms/pict_10376.jpg
cropped_ms/pict_10377.jpg
cropped_ms/pict_10378.jpg
cropped_ms/pict_10379.jpg
cropped_ms/pict_1038.jpg
cropped_ms/pict_10380.jpg
cropped_ms/pict_10381.jpg
cropped_ms/pict_10382.jpg
cropped_ms/pict_10383.jpg
cropped_ms/pict_10384.jpg
cropped_ms/pict_10385.jpg
cropped_ms/pict_10386.jpg
cropped_ms/pict_10387.jpg
cropped_ms/pict_10388.jpg
cropped_ms/pict_10389.jpg
cropped_ms/pict_1039.jpg
cropped_ms/pict_10390.jpg
cropped_ms/pict_10391.jpg
cropped_ms/pict_10392.jpg
cropped_ms/pict_10393.jpg
cropped_ms/pict_10394.jpg
cropped_ms/pict_10395.jpg
cropped_ms/pict_10396.jpg
cropped_ms/pict_10397.jpg
cropped_ms/pict_10398.jpg
cropped_ms/pict_10399.jpg
cropped_ms/pict_104.jpg
cropped_ms/pict_1040.jpg
cropped_ms/pict_10400.jpg
cropped_ms/pict_10401.jpg
cropped_ms/pict_10402.jpg
cropped_ms/pict_10403.jpg
cropped_ms/pict_10404.jpg
cropped_ms/pict_10405.jpg
cropped_ms/pict_10406.jpg
cropped_ms/pict_10407.jpg
cropped_ms/pict_10408.jpg
cropped_ms/pict_10409.jpg
cropped_ms/pict_1041.jpg
cropped_ms/pict_10410.jpg
cropped_ms/pict_10411.jpg
cropped_ms/pict_10412.jpg
cropped_ms/pict_10413.jpg
cropped_ms/pict_10414.jpg
cropped_ms/pict_10415.jpg
cropped_ms/pict_10416.jpg
cropped_ms/pict_10417.jpg
cropped_ms/pict_10418.jpg
cropped_ms/pict_10419.jpg
cropped_ms/pict_1042.jpg
cropped_ms/pict_10420.jpg
cropped_ms/pict_10421.jpg
cropped_ms/pict_10422.jpg
cropped_ms/pict_10423.jpg
cropped_ms/pict_10424.jpg
cropped_ms/pict_10425.jpg
cropped_ms/pict_10426.jpg
cropped_ms/pict_10427.jpg
cropped_ms/pict_10428.jpg
cropped_ms/pict_10429.jpg
cropped_ms/pict_1043.jpg
cropped_ms/pict_10430.jpg
cropped_ms/pict_10431.jpg
cropped_ms/pict_10432.jpg
cropped_ms/pict_10433.jpg
cropped_ms/pict_10434.jpg
cropped_ms/pict_10435.jpg
cropped_ms/pict_10436.jpg
cropped_ms/pict_10437.jpg
cropped_ms/pict_10438.jpg
cropped_ms/pict_10439.jpg
cropped_ms/pict_1044.jpg
cropped_ms/pict_10440.jpg
cropped_ms/pict_10441.jpg
cropped_ms/pict_10442.jpg
cropped_ms/pict_10443.jpg
cropped_ms/pict_10444.jpg
cropped_ms/pict_10445.jpg
cropped_ms/pict_10446.jpg
cropped_ms/pict_10447.jpg
cropped_ms/pict_10448.jpg
cropped_ms/pict_10449.jpg
cropped_ms/pict_1045.jpg
cropped_ms/pict_10450.jpg
cropped_ms/pict_10451.jpg
cropped_ms/pict_10452.jpg
cropped_ms/pict_10453.jpg
cropped_ms/pict_10454.jpg
cropped_ms/pict_10455.jpg
cropped_ms/pict_10456.jpg
cropped_ms/pict_10457.jpg
cropped_ms/pict_10458.jpg
cropped_ms/pict_10459.jpg
cropped_ms/pict_1046.jpg
cropped_ms/pict_10460.jpg
cropped_ms/pict_10461.jpg
cropped_ms/pict_10462.jpg
cropped_ms/pict_10463.jpg
cropped_ms/pict_10464.jpg
cropped_ms/pict_10465.jpg
cropped_ms/pict_10466.jpg
cropped_ms/pict_10467.jpg
cropped_ms/pict_10468.jpg
cropped_ms/pict_10469.jpg
cropped_ms/pict_1047.jpg
cropped_ms/pict_10470.jpg
cropped_ms/pict_10471.jpg
cropped_ms/pict_10472.jpg
cropped_ms/pict_10473.jpg
cropped_ms/pict_10474.jpg
cropped_ms/pict_10475.jpg
cropped_ms/pict_10476.jpg
cropped_ms/pict_10477.jpg
cropped_ms/pict_10478.jpg
cropped_ms/pict_10479.jpg
cropped_ms/pict_1048.jpg
cropped_ms/pict_10480.jpg
cropped_ms/pict_10481.jpg
cropped_ms/pict_10482.jpg
cropped_ms/pict_10483.jpg
cropped_ms/pict_10484.jpg
cropped_ms/pict_10485.jpg
cropped_ms/pict_10486.jpg
cropped_ms/pict_10487.jpg
cropped_ms/pict_10488.jpg
cropped_ms/pict_10489.jpg
cropped_ms/pict_1049.jpg
cropped_ms/pict_10490.jpg
cropped_ms/pict_10491.jpg
cropped_ms/pict_10492.jpg
cropped_ms/pict_10493.jpg
cropped_ms/pict_10494.jpg
cropped_ms/pict_10495.jpg
cropped_ms/pict_10496.jpg
cropped_ms/pict_10497.jpg
cropped_ms/pict_10498.jpg
cropped_ms/pict_10499.jpg
cropped_ms/pict_105.jpg
cropped_ms/pict_1050.jpg
cropped_ms/pict_10500.jpg
cropped_ms/pict_10501.jpg
cropped_ms/pict_10502.jpg
cropped_ms/pict_10503.jpg
cropped_ms/pict_10504.jpg
cropped_ms/pict_10505.jpg
cropped_ms/pict_10506.jpg
cropped_ms/pict_10507.jpg
cropped_ms/pict_10508.jpg
cropped_ms/pict_10509.jpg
cropped_ms/pict_1051.jpg
cropped_ms/pict_10510.jpg
cropped_ms/pict_10511.jpg
cropped_ms/pict_10512.jpg
cropped_ms/pict_10513.jpg
cropped_ms/pict_10514.jpg
cropped_ms/pict_10515.jpg
cropped_ms/pict_10516.jpg
cropped_ms/pict_10517.jpg
cropped_ms/pict_10518.jpg
cropped_ms/pict_10519.jpg
cropped_ms/pict_1052.jpg
cropped_ms/pict_10520.jpg
cropped_ms/pict_10521.jpg
cropped_ms/pict_10522.jpg
cropped_ms/pict_10523.jpg
cropped_ms/pict_10524.jpg
cropped_ms/pict_10525.jpg
cropped_ms/pict_10526.jpg
cropped_ms/pict_10527.jpg
cropped_ms/pict_10528.jpg
cropped_ms/pict_10529.jpg
cropped_ms/pict_1053.jpg
cropped_ms/pict_10530.jpg
cropped_ms/pict_10531.jpg
cropped_ms/pict_10532.jpg
cropped_ms/pict_10533.jpg
cropped_ms/pict_10534.jpg
cropped_ms/pict_10535.jpg
cropped_ms/pict_10536.jpg
cropped_ms/pict_10537.jpg
cropped_ms/pict_10538.jpg
cropped_ms/pict_10539.jpg
cropped_ms/pict_1054.jpg
cropped_ms/pict_10540.jpg
cropped_ms/pict_10541.jpg
cropped_ms/pict_10542.jpg
cropped_ms/pict_10543.jpg
cropped_ms/pict_10544.jpg
cropped_ms/pict_10545.jpg
cropped_ms/pict_10546.jpg
cropped_ms/pict_10547.jpg
cropped_ms/pict_10548.jpg
cropped_ms/pict_10549.jpg
cropped_ms/pict_1055.jpg
cropped_ms/pict_10550.jpg
cropped_ms/pict_10551.jpg
cropped_ms/pict_10552.jpg
cropped_ms/pict_10553.jpg
cropped_ms/pict_10554.jpg
cropped_ms/pict_10555.jpg
cropped_ms/pict_10556.jpg
cropped_ms/pict_10557.jpg
cropped_ms/pict_10558.jpg
cropped_ms/pict_10559.jpg
cropped_ms/pict_1056.jpg
cropped_ms/pict_10560.jpg
cropped_ms/pict_10561.jpg
cropped_ms/pict_10562.jpg
cropped_ms/pict_10563.jpg
cropped_ms/pict_10564.jpg
cropped_ms/pict_10565.jpg
cropped_ms/pict_10566.jpg
cropped_ms/pict_10567.jpg
cropped_ms/pict_10568.jpg
cropped_ms/pict_10569.jpg
cropped_ms/pict_1057.jpg
cropped_ms/pict_10570.jpg
cropped_ms/pict_10571.jpg
cropped_ms/pict_10572.jpg
cropped_ms/pict_10573.jpg
cropped_ms/pict_10574.jpg
cropped_ms/pict_10575.jpg
cropped_ms/pict_10576.jpg
cropped_ms/pict_10577.jpg
cropped_ms/pict_10578.jpg
cropped_ms/pict_10579.jpg
cropped_ms/pict_1058.jpg
cropped_ms/pict_10580.jpg
cropped_ms/pict_10581.jpg
cropped_ms/pict_10582.jpg
cropped_ms/pict_10583.jpg
cropped_ms/pict_10584.jpg
cropped_ms/pict_10585.jpg
cropped_ms/pict_10586.jpg
cropped_ms/pict_10587.jpg
cropped_ms/pict_10588.jpg
cropped_ms/pict_10589.jpg
cropped_ms/pict_1059.jpg
cropped_ms/pict_10590.jpg
cropped_ms/pict_10591.jpg
cropped_ms/pict_10592.jpg
cropped_ms/pict_10593.jpg
cropped_ms/pict_10594.jpg
cropped_ms/pict_10595.jpg
cropped_ms/pict_10596.jpg
cropped_ms/pict_10597.jpg
cropped_ms/pict_10598.jpg
cropped_ms/pict_10599.jpg
cropped_ms/pict_106.jpg
cropped_ms/pict_1060.jpg
cropped_ms/pict_10600.jpg
cropped_ms/pict_10601.jpg
cropped_ms/pict_10602.jpg
cropped_ms/pict_10603.jpg
cropped_ms/pict_10604.jpg
cropped_ms/pict_10605.jpg
cropped_ms/pict_10606.jpg
cropped_ms/pict_10607.jpg
cropped_ms/pict_10608.jpg
cropped_ms/pict_10609.jpg
cropped_ms/pict_1061.jpg
cropped_ms/pict_10610.jpg
cropped_ms/pict_10611.jpg
cropped_ms/pict_10612.jpg
cropped_ms/pict_10613.jpg
cropped_ms/pict_10614.jpg
cropped_ms/pict_10615.jpg
cropped_ms/pict_10616.jpg
cropped_ms/pict_10617.jpg
cropped_ms/pict_10618.jpg
cropped_ms/pict_10619.jpg
cropped_ms/pict_1062.jpg
cropped_ms/pict_10620.jpg
cropped_ms/pict_10621.jpg
cropped_ms/pict_10622.jpg
cropped_ms/pict_10623.jpg
cropped_ms/pict_10624.jpg
cropped_ms/pict_10625.jpg
cropped_ms/pict_10626.jpg
cropped_ms/pict_10627.jpg
cropped_ms/pict_10628.jpg
cropped_ms/pict_10629.jpg
cropped_ms/pict_1063.jpg
cropped_ms/pict_10630.jpg
cropped_ms/pict_10631.jpg
cropped_ms/pict_10632.jpg
cropped_ms/pict_10633.jpg
cropped_ms/pict_10634.jpg
cropped_ms/pict_10635.jpg
cropped_ms/pict_10636.jpg
cropped_ms/pict_10637.jpg
cropped_ms/pict_10638.jpg
cropped_ms/pict_10639.jpg
cropped_ms/pict_1064.jpg
cropped_ms/pict_10640.jpg
cropped_ms/pict_10641.jpg
cropped_ms/pict_10642.jpg
cropped_ms/pict_10643.jpg
cropped_ms/pict_10644.jpg
cropped_ms/pict_10645.jpg
cropped_ms/pict_10646.jpg
cropped_ms/pict_10647.jpg
cropped_ms/pict_10648.jpg
cropped_ms/pict_10649.jpg
cropped_ms/pict_1065.jpg
cropped_ms/pict_10650.jpg
cropped_ms/pict_10651.jpg
cropped_ms/pict_10652.jpg
cropped_ms/pict_10653.jpg
cropped_ms/pict_10654.jpg
cropped_ms/pict_10655.jpg
cropped_ms/pict_10656.jpg
cropped_ms/pict_10657.jpg
cropped_ms/pict_10658.jpg
cropped_ms/pict_10659.jpg
cropped_ms/pict_1066.jpg
cropped_ms/pict_10660.jpg
cropped_ms/pict_10661.jpg
cropped_ms/pict_10662.jpg
cropped_ms/pict_10663.jpg
cropped_ms/pict_10664.jpg
cropped_ms/pict_10665.jpg
cropped_ms/pict_10666.jpg
cropped_ms/pict_10667.jpg
cropped_ms/pict_10668.jpg
cropped_ms/pict_10669.jpg
cropped_ms/pict_1067.jpg
cropped_ms/pict_10670.jpg
cropped_ms/pict_10671.jpg
cropped_ms/pict_10672.jpg
cropped_ms/pict_10673.jpg
cropped_ms/pict_10674.jpg
cropped_ms/pict_10675.jpg
cropped_ms/pict_10676.jpg
cropped_ms/pict_10677.jpg
cropped_ms/pict_10678.jpg
cropped_ms/pict_10679.jpg
cropped_ms/pict_1068.jpg
cropped_ms/pict_10680.jpg
cropped_ms/pict_10681.jpg
cropped_ms/pict_10682.jpg
cropped_ms/pict_10683.jpg
cropped_ms/pict_10684.jpg
cropped_ms/pict_10685.jpg
cropped_ms/pict_10686.jpg
cropped_ms/pict_10687.jpg
cropped_ms/pict_10688.jpg
cropped_ms/pict_10689.jpg
cropped_ms/pict_1069.jpg
cropped_ms/pict_10690.jpg
cropped_ms/pict_10691.jpg
cropped_ms/pict_10692.jpg
cropped_ms/pict_10693.jpg
cropped_ms/pict_10694.jpg
cropped_ms/pict_10695.jpg
cropped_ms/pict_10696.jpg
cropped_ms/pict_10697.jpg
cropped_ms/pict_10698.jpg
cropped_ms/pict_10699.jpg
cropped_ms/pict_107.jpg
cropped_ms/pict_1070.jpg
cropped_ms/pict_10700.jpg
cropped_ms/pict_10701.jpg
cropped_ms/pict_10702.jpg
cropped_ms/pict_10703.jpg
cropped_ms/pict_10704.jpg
cropped_ms/pict_10705.jpg
cropped_ms/pict_10706.jpg
cropped_ms/pict_10707.jpg
cropped_ms/pict_10708.jpg
cropped_ms/pict_10709.jpg
cropped_ms/pict_1071.jpg
cropped_ms/pict_10710.jpg
cropped_ms/pict_10711.jpg
cropped_ms/pict_10712.jpg
cropped_ms/pict_10713.jpg
cropped_ms/pict_10714.jpg
cropped_ms/pict_10715.jpg
cropped_ms/pict_10716.jpg
cropped_ms/pict_10717.jpg
cropped_ms/pict_10718.jpg
cropped_ms/pict_10719.jpg
cropped_ms/pict_1072.jpg
cropped_ms/pict_10720.jpg
cropped_ms/pict_10721.jpg
cropped_ms/pict_10722.jpg
cropped_ms/pict_10723.jpg
cropped_ms/pict_10724.jpg
cropped_ms/pict_10725.jpg
cropped_ms/pict_10726.jpg
cropped_ms/pict_10727.jpg
cropped_ms/pict_10728.jpg
cropped_ms/pict_10729.jpg
cropped_ms/pict_1073.jpg
cropped_ms/pict_10730.jpg
cropped_ms/pict_10731.jpg
cropped_ms/pict_10732.jpg
cropped_ms/pict_10733.jpg
cropped_ms/pict_10734.jpg
cropped_ms/pict_10735.jpg
cropped_ms/pict_10736.jpg
cropped_ms/pict_10737.jpg
cropped_ms/pict_10738.jpg
cropped_ms/pict_10739.jpg
cropped_ms/pict_1074.jpg
cropped_ms/pict_10740.jpg
cropped_ms/pict_10741.jpg
cropped_ms/pict_10742.jpg
cropped_ms/pict_10743.jpg
cropped_ms/pict_10744.jpg
cropped_ms/pict_10745.jpg
cropped_ms/pict_10746.jpg
cropped_ms/pict_10747.jpg
cropped_ms/pict_10748.jpg
cropped_ms/pict_10749.jpg
cropped_ms/pict_1075.jpg
cropped_ms/pict_10750.jpg
cropped_ms/pict_10751.jpg
cropped_ms/pict_10752.jpg
cropped_ms/pict_10753.jpg
cropped_ms/pict_10754.jpg
cropped_ms/pict_10755.jpg
cropped_ms/pict_10756.jpg
cropped_ms/pict_10757.jpg
cropped_ms/pict_10758.jpg
cropped_ms/pict_10759.jpg
cropped_ms/pict_1076.jpg
cropped_ms/pict_10760.jpg
cropped_ms/pict_10761.jpg
cropped_ms/pict_10762.jpg
cropped_ms/pict_10763.jpg
cropped_ms/pict_10764.jpg
cropped_ms/pict_10765.jpg
cropped_ms/pict_10766.jpg
cropped_ms/pict_10767.jpg
cropped_ms/pict_10768.jpg
cropped_ms/pict_10769.jpg
cropped_ms/pict_1077.jpg
cropped_ms/pict_10770.jpg
cropped_ms/pict_10771.jpg
cropped_ms/pict_10772.jpg
cropped_ms/pict_10773.jpg
cropped_ms/pict_10774.jpg
cropped_ms/pict_10775.jpg
cropped_ms/pict_10776.jpg
cropped_ms/pict_10777.jpg
cropped_ms/pict_10778.jpg
cropped_ms/pict_10779.jpg
cropped_ms/pict_1078.jpg
cropped_ms/pict_10780.jpg
cropped_ms/pict_10781.jpg
cropped_ms/pict_10782.jpg
cropped_ms/pict_10783.jpg
cropped_ms/pict_10784.jpg
cropped_ms/pict_10785.jpg
cropped_ms/pict_10786.jpg
cropped_ms/pict_10787.jpg
cropped_ms/pict_10788.jpg
cropped_ms/pict_10789.jpg
cropped_ms/pict_1079.jpg
cropped_ms/pict_10790.jpg
cropped_ms/pict_10791.jpg
cropped_ms/pict_10792.jpg
cropped_ms/pict_10793.jpg
cropped_ms/pict_10794.jpg
cropped_ms/pict_10795.jpg
cropped_ms/pict_10796.jpg
cropped_ms/pict_10797.jpg
cropped_ms/pict_10798.jpg
cropped_ms/pict_10799.jpg
cropped_ms/pict_108.jpg
cropped_ms/pict_1080.jpg
cropped_ms/pict_10800.jpg
cropped_ms/pict_10801.jpg
cropped_ms/pict_10802.jpg
cropped_ms/pict_10803.jpg
cropped_ms/pict_10804.jpg
cropped_ms/pict_10805.jpg
cropped_ms/pict_10806.jpg
cropped_ms/pict_10807.jpg
cropped_ms/pict_10808.jpg
cropped_ms/pict_10809.jpg
cropped_ms/pict_1081.jpg
cropped_ms/pict_10810.jpg
cropped_ms/pict_10811.jpg
cropped_ms/pict_10812.jpg
cropped_ms/pict_10813.jpg
cropped_ms/pict_10814.jpg
cropped_ms/pict_10815.jpg
cropped_ms/pict_10816.jpg
cropped_ms/pict_10817.jpg
cropped_ms/pict_10818.jpg
cropped_ms/pict_10819.jpg
cropped_ms/pict_1082.jpg
cropped_ms/pict_10820.jpg
cropped_ms/pict_10821.jpg
cropped_ms/pict_10822.jpg
cropped_ms/pict_10823.jpg
cropped_ms/pict_10824.jpg
cropped_ms/pict_10825.jpg
cropped_ms/pict_10826.jpg
cropped_ms/pict_10827.jpg
cropped_ms/pict_10828.jpg
cropped_ms/pict_10829.jpg
cropped_ms/pict_1083.jpg
cropped_ms/pict_10830.jpg
cropped_ms/pict_10831.jpg
cropped_ms/pict_10832.jpg
cropped_ms/pict_10833.jpg
cropped_ms/pict_10834.jpg
cropped_ms/pict_10835.jpg
cropped_ms/pict_10836.jpg
cropped_ms/pict_10837.jpg
cropped_ms/pict_10838.jpg
cropped_ms/pict_10839.jpg
cropped_ms/pict_1084.jpg
cropped_ms/pict_10840.jpg
cropped_ms/pict_10841.jpg
cropped_ms/pict_10842.jpg
cropped_ms/pict_10843.jpg
cropped_ms/pict_10844.jpg
cropped_ms/pict_10845.jpg
cropped_ms/pict_10846.jpg
cropped_ms/pict_10847.jpg
cropped_ms/pict_10848.jpg
cropped_ms/pict_10849.jpg
cropped_ms/pict_1085.jpg
cropped_ms/pict_10850.jpg
cropped_ms/pict_10851.jpg
cropped_ms/pict_10852.jpg
cropped_ms/pict_10853.jpg
cropped_ms/pict_10854.jpg
cropped_ms/pict_10855.jpg
cropped_ms/pict_10856.jpg
cropped_ms/pict_10857.jpg
cropped_ms/pict_10858.jpg
cropped_ms/pict_10859.jpg
cropped_ms/pict_1086.jpg
cropped_ms/pict_10860.jpg
cropped_ms/pict_10861.jpg
cropped_ms/pict_10862.jpg
cropped_ms/pict_10863.jpg
cropped_ms/pict_10864.jpg
cropped_ms/pict_10865.jpg
cropped_ms/pict_10866.jpg
cropped_ms/pict_10867.jpg
cropped_ms/pict_10868.jpg
cropped_ms/pict_10869.jpg
cropped_ms/pict_1087.jpg
cropped_ms/pict_10870.jpg
cropped_ms/pict_10871.jpg
cropped_ms/pict_10872.jpg
cropped_ms/pict_10873.jpg
cropped_ms/pict_10874.jpg
cropped_ms/pict_10875.jpg
cropped_ms/pict_10876.jpg
cropped_ms/pict_10877.jpg
cropped_ms/pict_10878.jpg
cropped_ms/pict_10879.jpg
cropped_ms/pict_1088.jpg
cropped_ms/pict_10880.jpg
cropped_ms/pict_10881.jpg
cropped_ms/pict_10882.jpg
cropped_ms/pict_10883.jpg
cropped_ms/pict_10884.jpg
cropped_ms/pict_10885.jpg
cropped_ms/pict_10886.jpg
cropped_ms/pict_10887.jpg
cropped_ms/pict_10888.jpg
cropped_ms/pict_10889.jpg
cropped_ms/pict_1089.jpg
cropped_ms/pict_10890.jpg
cropped_ms/pict_10891.jpg
cropped_ms/pict_10892.jpg
cropped_ms/pict_10893.jpg
cropped_ms/pict_10894.jpg
cropped_ms/pict_10895.jpg
cropped_ms/pict_10896.jpg
cropped_ms/pict_10897.jpg
cropped_ms/pict_10898.jpg
cropped_ms/pict_10899.jpg
cropped_ms/pict_109.jpg
cropped_ms/pict_1090.jpg
cropped_ms/pict_10900.jpg
cropped_ms/pict_10901.jpg
cropped_ms/pict_10902.jpg
cropped_ms/pict_10903.jpg
cropped_ms/pict_10904.jpg
cropped_ms/pict_10905.jpg
cropped_ms/pict_10906.jpg
cropped_ms/pict_10907.jpg
cropped_ms/pict_10908.jpg
cropped_ms/pict_10909.jpg
cropped_ms/pict_1091.jpg
cropped_ms/pict_10910.jpg
cropped_ms/pict_10911.jpg
cropped_ms/pict_10912.jpg
cropped_ms/pict_10913.jpg
cropped_ms/pict_10914.jpg
cropped_ms/pict_10915.jpg
cropped_ms/pict_10916.jpg
cropped_ms/pict_10917.jpg
cropped_ms/pict_10918.jpg
cropped_ms/pict_10919.jpg
cropped_ms/pict_1092.jpg
cropped_ms/pict_10920.jpg
cropped_ms/pict_10921.jpg
cropped_ms/pict_10922.jpg
cropped_ms/pict_10923.jpg
cropped_ms/pict_10924.jpg
cropped_ms/pict_10925.jpg
cropped_ms/pict_10926.jpg
cropped_ms/pict_10927.jpg
cropped_ms/pict_10928.jpg
cropped_ms/pict_10929.jpg
cropped_ms/pict_1093.jpg
cropped_ms/pict_10930.jpg
cropped_ms/pict_10931.jpg
cropped_ms/pict_10932.jpg
cropped_ms/pict_10933.jpg
cropped_ms/pict_10934.jpg
cropped_ms/pict_10935.jpg
cropped_ms/pict_10936.jpg
cropped_ms/pict_10937.jpg
cropped_ms/pict_10938.jpg
cropped_ms/pict_10939.jpg
cropped_ms/pict_1094.jpg
cropped_ms/pict_10940.jpg
cropped_ms/pict_10941.jpg
cropped_ms/pict_10942.jpg
cropped_ms/pict_10943.jpg
cropped_ms/pict_10944.jpg
cropped_ms/pict_10945.jpg
cropped_ms/pict_10946.jpg
cropped_ms/pict_10947.jpg
cropped_ms/pict_10948.jpg
cropped_ms/pict_10949.jpg
cropped_ms/pict_1095.jpg
cropped_ms/pict_10950.jpg
cropped_ms/pict_10951.jpg
cropped_ms/pict_10952.jpg
cropped_ms/pict_10953.jpg
cropped_ms/pict_10954.jpg
cropped_ms/pict_10955.jpg
cropped_ms/pict_10956.jpg
cropped_ms/pict_10957.jpg
cropped_ms/pict_10958.jpg
cropped_ms/pict_10959.jpg
cropped_ms/pict_1096.jpg
cropped_ms/pict_10960.jpg
cropped_ms/pict_10961.jpg
cropped_ms/pict_10962.jpg
cropped_ms/pict_10963.jpg
cropped_ms/pict_10964.jpg
cropped_ms/pict_10965.jpg
cropped_ms/pict_10966.jpg
cropped_ms/pict_10967.jpg
cropped_ms/pict_10968.jpg
cropped_ms/pict_10969.jpg
cropped_ms/pict_1097.jpg
cropped_ms/pict_10970.jpg
cropped_ms/pict_10971.jpg
cropped_ms/pict_10972.jpg
cropped_ms/pict_10973.jpg
cropped_ms/pict_10974.jpg
cropped_ms/pict_10975.jpg
cropped_ms/pict_10976.jpg
cropped_ms/pict_10977.jpg
cropped_ms/pict_10978.jpg
cropped_ms/pict_10979.jpg
cropped_ms/pict_1098.jpg
cropped_ms/pict_10980.jpg
cropped_ms/pict_10981.jpg
cropped_ms/pict_10982.jpg
cropped_ms/pict_10983.jpg
cropped_ms/pict_10984.jpg
cropped_ms/pict_10985.jpg
cropped_ms/pict_10986.jpg
cropped_ms/pict_10987.jpg
cropped_ms/pict_10988.jpg
cropped_ms/pict_10989.jpg
cropped_ms/pict_1099.jpg
cropped_ms/pict_10990.jpg
cropped_ms/pict_10991.jpg
cropped_ms/pict_10992.jpg
cropped_ms/pict_10993.jpg
cropped_ms/pict_10994.jpg
cropped_ms/pict_10995.jpg
cropped_ms/pict_10996.jpg
cropped_ms/pict_10997.jpg
cropped_ms/pict_10998.jpg
cropped_ms/pict_10999.jpg
cropped_ms/pict_11.jpg
cropped_ms/pict_110.jpg
cropped_ms/pict_1100.jpg
cropped_ms/pict_11000.jpg
cropped_ms/pict_11001.jpg
cropped_ms/pict_11002.jpg
cropped_ms/pict_11003.jpg
cropped_ms/pict_11004.jpg
cropped_ms/pict_11005.jpg
cropped_ms/pict_11006.jpg
cropped_ms/pict_11007.jpg
cropped_ms/pict_11008.jpg
cropped_ms/pict_11009.jpg
cropped_ms/pict_1101.jpg
cropped_ms/pict_11010.jpg
cropped_ms/pict_11011.jpg
cropped_ms/pict_11012.jpg
cropped_ms/pict_11013.jpg
cropped_ms/pict_11014.jpg
cropped_ms/pict_11015.jpg
cropped_ms/pict_11016.jpg
cropped_ms/pict_11017.jpg
cropped_ms/pict_11018.jpg
cropped_ms/pict_11019.jpg
cropped_ms/pict_1102.jpg
cropped_ms/pict_11020.jpg
cropped_ms/pict_11021.jpg
cropped_ms/pict_11022.jpg
cropped_ms/pict_11023.jpg
cropped_ms/pict_11024.jpg
cropped_ms/pict_11025.jpg
cropped_ms/pict_11026.jpg
cropped_ms/pict_11027.jpg
cropped_ms/pict_11028.jpg
cropped_ms/pict_11029.jpg
cropped_ms/pict_1103.jpg
cropped_ms/pict_11030.jpg
cropped_ms/pict_11031.jpg
cropped_ms/pict_11032.jpg
cropped_ms/pict_11033.jpg
cropped_ms/pict_11034.jpg
cropped_ms/pict_11035.jpg
cropped_ms/pict_11036.jpg
cropped_ms/pict_11037.jpg
cropped_ms/pict_11038.jpg
cropped_ms/pict_11039.jpg
cropped_ms/pict_1104.jpg
cropped_ms/pict_11040.jpg
cropped_ms/pict_11041.jpg
cropped_ms/pict_11042.jpg
cropped_ms/pict_11043.jpg
cropped_ms/pict_11044.jpg
cropped_ms/pict_11045.jpg
cropped_ms/pict_11046.jpg
cropped_ms/pict_11047.jpg
cropped_ms/pict_11048.jpg
cropped_ms/pict_11049.jpg
cropped_ms/pict_1105.jpg
cropped_ms/pict_11050.jpg
cropped_ms/pict_11051.jpg
cropped_ms/pict_11052.jpg
cropped_ms/pict_11053.jpg
cropped_ms/pict_11054.jpg
cropped_ms/pict_11055.jpg
cropped_ms/pict_11056.jpg
cropped_ms/pict_11057.jpg
cropped_ms/pict_11058.jpg
cropped_ms/pict_11059.jpg
cropped_ms/pict_1106.jpg
cropped_ms/pict_11060.jpg
cropped_ms/pict_11061.jpg
cropped_ms/pict_11062.jpg
cropped_ms/pict_11063.jpg
cropped_ms/pict_11064.jpg
cropped_ms/pict_11065.jpg
cropped_ms/pict_11066.jpg
cropped_ms/pict_11067.jpg
cropped_ms/pict_11068.jpg
cropped_ms/pict_11069.jpg
cropped_ms/pict_1107.jpg
cropped_ms/pict_11070.jpg
cropped_ms/pict_11071.jpg
cropped_ms/pict_11072.jpg
cropped_ms/pict_11073.jpg
cropped_ms/pict_11074.jpg
cropped_ms/pict_11075.jpg
cropped_ms/pict_11076.jpg
cropped_ms/pict_11077.jpg
cropped_ms/pict_11078.jpg
cropped_ms/pict_11079.jpg
cropped_ms/pict_1108.jpg
cropped_ms/pict_11080.jpg
cropped_ms/pict_11081.jpg
cropped_ms/pict_11082.jpg
cropped_ms/pict_11083.jpg
cropped_ms/pict_11084.jpg
cropped_ms/pict_11085.jpg
cropped_ms/pict_11086.jpg
cropped_ms/pict_11087.jpg
cropped_ms/pict_11088.jpg
cropped_ms/pict_11089.jpg
cropped_ms/pict_1109.jpg
cropped_ms/pict_11090.jpg
cropped_ms/pict_11091.jpg
cropped_ms/pict_11092.jpg
cropped_ms/pict_11093.jpg
cropped_ms/pict_11094.jpg
cropped_ms/pict_11095.jpg
cropped_ms/pict_11096.jpg
cropped_ms/pict_11097.jpg
cropped_ms/pict_11098.jpg
cropped_ms/pict_11099.jpg
cropped_ms/pict_111.jpg
cropped_ms/pict_1110.jpg
cropped_ms/pict_11100.jpg
cropped_ms/pict_11101.jpg
cropped_ms/pict_11102.jpg
cropped_ms/pict_11103.jpg
cropped_ms/pict_11104.jpg
cropped_ms/pict_11105.jpg
cropped_ms/pict_11106.jpg
cropped_ms/pict_11107.jpg
cropped_ms/pict_11108.jpg
cropped_ms/pict_11109.jpg
cropped_ms/pict_1111.jpg
cropped_ms/pict_11110.jpg
cropped_ms/pict_11111.jpg
cropped_ms/pict_11112.jpg
cropped_ms/pict_11113.jpg
cropped_ms/pict_11114.jpg
cropped_ms/pict_11115.jpg
cropped_ms/pict_11116.jpg
cropped_ms/pict_11117.jpg
cropped_ms/pict_11118.jpg
cropped_ms/pict_11119.jpg
cropped_ms/pict_1112.jpg
cropped_ms/pict_11120.jpg
cropped_ms/pict_11121.jpg
cropped_ms/pict_11122.jpg
cropped_ms/pict_11123.jpg
cropped_ms/pict_11124.jpg
cropped_ms/pict_11125.jpg
cropped_ms/pict_11126.jpg
cropped_ms/pict_11127.jpg
cropped_ms/pict_11128.jpg
cropped_ms/pict_11129.jpg
cropped_ms/pict_1113.jpg
cropped_ms/pict_11130.jpg
cropped_ms/pict_11131.jpg
cropped_ms/pict_11132.jpg
cropped_ms/pict_11133.jpg
cropped_ms/pict_11134.jpg
cropped_ms/pict_11135.jpg
cropped_ms/pict_11136.jpg
cropped_ms/pict_11137.jpg
cropped_ms/pict_11138.jpg
cropped_ms/pict_11139.jpg
cropped_ms/pict_1114.jpg
cropped_ms/pict_11140.jpg
cropped_ms/pict_11141.jpg
cropped_ms/pict_11142.jpg
cropped_ms/pict_11143.jpg
cropped_ms/pict_11144.jpg
cropped_ms/pict_11145.jpg
cropped_ms/pict_11146.jpg
cropped_ms/pict_11147.jpg
cropped_ms/pict_11148.jpg
cropped_ms/pict_11149.jpg
cropped_ms/pict_1115.jpg
cropped_ms/pict_11150.jpg
cropped_ms/pict_11151.jpg
cropped_ms/pict_11152.jpg
cropped_ms/pict_11153.jpg
cropped_ms/pict_11154.jpg
cropped_ms/pict_11155.jpg
cropped_ms/pict_11156.jpg
cropped_ms/pict_11157.jpg
cropped_ms/pict_11158.jpg
cropped_ms/pict_11159.jpg
cropped_ms/pict_1116.jpg
cropped_ms/pict_11160.jpg
cropped_ms/pict_11161.jpg
cropped_ms/pict_11162.jpg
cropped_ms/pict_11163.jpg
cropped_ms/pict_11164.jpg
cropped_ms/pict_11165.jpg
cropped_ms/pict_11166.jpg
cropped_ms/pict_11167.jpg
cropped_ms/pict_11168.jpg
cropped_ms/pict_11169.jpg
cropped_ms/pict_1117.jpg
cropped_ms/pict_11170.jpg
cropped_ms/pict_11171.jpg
cropped_ms/pict_11172.jpg
cropped_ms/pict_11173.jpg
cropped_ms/pict_11174.jpg
cropped_ms/pict_11175.jpg
cropped_ms/pict_11176.jpg
cropped_ms/pict_11177.jpg
cropped_ms/pict_11178.jpg
cropped_ms/pict_11179.jpg
cropped_ms/pict_1118.jpg
cropped_ms/pict_11180.jpg
cropped_ms/pict_11181.jpg
cropped_ms/pict_11182.jpg
cropped_ms/pict_11183.jpg
cropped_ms/pict_11184.jpg
cropped_ms/pict_11185.jpg
cropped_ms/pict_11186.jpg
cropped_ms/pict_11187.jpg
cropped_ms/pict_11188.jpg
cropped_ms/pict_11189.jpg
cropped_ms/pict_1119.jpg
cropped_ms/pict_11190.jpg
cropped_ms/pict_11191.jpg
cropped_ms/pict_11192.jpg
cropped_ms/pict_11193.jpg
cropped_ms/pict_11194.jpg
cropped_ms/pict_11195.jpg
cropped_ms/pict_11196.jpg
cropped_ms/pict_11197.jpg
cropped_ms/pict_11198.jpg
cropped_ms/pict_11199.jpg
cropped_ms/pict_112.jpg
cropped_ms/pict_1120.jpg
cropped_ms/pict_11200.jpg
cropped_ms/pict_11201.jpg
cropped_ms/pict_11202.jpg
cropped_ms/pict_11203.jpg
cropped_ms/pict_11204.jpg
cropped_ms/pict_11205.jpg
cropped_ms/pict_11206.jpg
cropped_ms/pict_11207.jpg
cropped_ms/pict_11208.jpg
cropped_ms/pict_11209.jpg
cropped_ms/pict_1121.jpg
cropped_ms/pict_11210.jpg
cropped_ms/pict_11211.jpg
cropped_ms/pict_11212.jpg
cropped_ms/pict_11213.jpg
cropped_ms/pict_11214.jpg
cropped_ms/pict_11215.jpg
cropped_ms/pict_11216.jpg
cropped_ms/pict_11217.jpg
cropped_ms/pict_11218.jpg
cropped_ms/pict_11219.jpg
cropped_ms/pict_1122.jpg
cropped_ms/pict_11220.jpg
cropped_ms/pict_11221.jpg
cropped_ms/pict_11222.jpg
cropped_ms/pict_11223.jpg
cropped_ms/pict_11224.jpg
cropped_ms/pict_11225.jpg
cropped_ms/pict_11226.jpg
cropped_ms/pict_11227.jpg
cropped_ms/pict_11228.jpg
cropped_ms/pict_11229.jpg
cropped_ms/pict_1123.jpg
cropped_ms/pict_11230.jpg
cropped_ms/pict_11231.jpg
cropped_ms/pict_11232.jpg
cropped_ms/pict_11233.jpg
cropped_ms/pict_11234.jpg
cropped_ms/pict_11235.jpg
cropped_ms/pict_11236.jpg
cropped_ms/pict_11237.jpg
cropped_ms/pict_11238.jpg
cropped_ms/pict_11239.jpg
cropped_ms/pict_1124.jpg
cropped_ms/pict_11240.jpg
cropped_ms/pict_11241.jpg
cropped_ms/pict_11242.jpg
cropped_ms/pict_11243.jpg
cropped_ms/pict_11244.jpg
cropped_ms/pict_11245.jpg
cropped_ms/pict_11246.jpg
cropped_ms/pict_11247.jpg
cropped_ms/pict_11248.jpg
cropped_ms/pict_11249.jpg
cropped_ms/pict_1125.jpg
cropped_ms/pict_11250.jpg
cropped_ms/pict_11251.jpg
cropped_ms/pict_11252.jpg
cropped_ms/pict_11253.jpg
cropped_ms/pict_11254.jpg
cropped_ms/pict_11255.jpg
cropped_ms/pict_11256.jpg
cropped_ms/pict_11257.jpg
cropped_ms/pict_11258.jpg
cropped_ms/pict_11259.jpg
cropped_ms/pict_1126.jpg
cropped_ms/pict_11260.jpg
cropped_ms/pict_11261.jpg
cropped_ms/pict_11262.jpg
cropped_ms/pict_11263.jpg
cropped_ms/pict_11264.jpg
cropped_ms/pict_11265.jpg
cropped_ms/pict_11266.jpg
cropped_ms/pict_11267.jpg
cropped_ms/pict_11268.jpg
cropped_ms/pict_11269.jpg
cropped_ms/pict_1127.jpg
cropped_ms/pict_11270.jpg
cropped_ms/pict_11271.jpg
cropped_ms/pict_11272.jpg
cropped_ms/pict_11273.jpg
cropped_ms/pict_11274.jpg
cropped_ms/pict_11275.jpg
cropped_ms/pict_11276.jpg
cropped_ms/pict_11277.jpg
cropped_ms/pict_11278.jpg
cropped_ms/pict_11279.jpg
cropped_ms/pict_1128.jpg
cropped_ms/pict_11280.jpg
cropped_ms/pict_11281.jpg
cropped_ms/pict_11282.jpg
cropped_ms/pict_11283.jpg
cropped_ms/pict_11284.jpg
cropped_ms/pict_11285.jpg
cropped_ms/pict_11286.jpg
cropped_ms/pict_11287.jpg
cropped_ms/pict_11288.jpg
cropped_ms/pict_11289.jpg
cropped_ms/pict_1129.jpg
cropped_ms/pict_11290.jpg
cropped_ms/pict_11291.jpg
cropped_ms/pict_11292.jpg
cropped_ms/pict_11293.jpg
cropped_ms/pict_11294.jpg
cropped_ms/pict_11295.jpg
cropped_ms/pict_11296.jpg
cropped_ms/pict_11297.jpg
cropped_ms/pict_11298.jpg
cropped_ms/pict_11299.jpg
cropped_ms/pict_113.jpg
cropped_ms/pict_1130.jpg
cropped_ms/pict_11300.jpg
cropped_ms/pict_11301.jpg
cropped_ms/pict_11302.jpg
cropped_ms/pict_11303.jpg
cropped_ms/pict_11304.jpg
cropped_ms/pict_11305.jpg
cropped_ms/pict_11306.jpg
cropped_ms/pict_11307.jpg
cropped_ms/pict_11308.jpg
cropped_ms/pict_11309.jpg
cropped_ms/pict_1131.jpg
cropped_ms/pict_11310.jpg
cropped_ms/pict_11311.jpg
cropped_ms/pict_11312.jpg
cropped_ms/pict_11313.jpg
cropped_ms/pict_11314.jpg
cropped_ms/pict_11315.jpg
cropped_ms/pict_11316.jpg
cropped_ms/pict_11317.jpg
cropped_ms/pict_11318.jpg
cropped_ms/pict_11319.jpg
cropped_ms/pict_1132.jpg
cropped_ms/pict_11320.jpg
cropped_ms/pict_11321.jpg
cropped_ms/pict_11322.jpg
cropped_ms/pict_11323.jpg
cropped_ms/pict_11324.jpg
cropped_ms/pict_11325.jpg
cropped_ms/pict_11326.jpg
cropped_ms/pict_11327.jpg
cropped_ms/pict_11328.jpg
cropped_ms/pict_11329.jpg
cropped_ms/pict_1133.jpg
cropped_ms/pict_11330.jpg
cropped_ms/pict_11331.jpg
cropped_ms/pict_11332.jpg
cropped_ms/pict_11333.jpg
cropped_ms/pict_11334.jpg
cropped_ms/pict_11335.jpg
cropped_ms/pict_11336.jpg
cropped_ms/pict_11337.jpg
cropped_ms/pict_11338.jpg
cropped_ms/pict_11339.jpg
cropped_ms/pict_1134.jpg
cropped_ms/pict_11340.jpg
cropped_ms/pict_11341.jpg
cropped_ms/pict_11342.jpg
cropped_ms/pict_11343.jpg
cropped_ms/pict_11344.jpg
cropped_ms/pict_11345.jpg
cropped_ms/pict_11346.jpg
cropped_ms/pict_11347.jpg
cropped_ms/pict_11348.jpg
cropped_ms/pict_11349.jpg
cropped_ms/pict_1135.jpg
cropped_ms/pict_11350.jpg
cropped_ms/pict_11351.jpg
cropped_ms/pict_11352.jpg
cropped_ms/pict_11353.jpg
cropped_ms/pict_11354.jpg
cropped_ms/pict_11355.jpg
cropped_ms/pict_11356.jpg
cropped_ms/pict_11357.jpg
cropped_ms/pict_11358.jpg
cropped_ms/pict_11359.jpg
cropped_ms/pict_1136.jpg
cropped_ms/pict_11360.jpg
cropped_ms/pict_11361.jpg
cropped_ms/pict_11362.jpg
cropped_ms/pict_11363.jpg
cropped_ms/pict_11364.jpg
cropped_ms/pict_11365.jpg
cropped_ms/pict_11366.jpg
cropped_ms/pict_11367.jpg
cropped_ms/pict_11368.jpg
cropped_ms/pict_11369.jpg
cropped_ms/pict_1137.jpg
cropped_ms/pict_11370.jpg
cropped_ms/pict_11371.jpg
cropped_ms/pict_11372.jpg
cropped_ms/pict_11373.jpg
cropped_ms/pict_11374.jpg
cropped_ms/pict_11375.jpg
cropped_ms/pict_11376.jpg
cropped_ms/pict_11377.jpg
cropped_ms/pict_11378.jpg
cropped_ms/pict_11379.jpg
cropped_ms/pict_1138.jpg
cropped_ms/pict_11380.jpg
cropped_ms/pict_11381.jpg
cropped_ms/pict_11382.jpg
cropped_ms/pict_11383.jpg
cropped_ms/pict_11384.jpg
cropped_ms/pict_11385.jpg
cropped_ms/pict_11386.jpg
cropped_ms/pict_11387.jpg
cropped_ms/pict_11388.jpg
cropped_ms/pict_11389.jpg
cropped_ms/pict_1139.jpg
cropped_ms/pict_11390.jpg
cropped_ms/pict_11391.jpg
cropped_ms/pict_11392.jpg
cropped_ms/pict_11393.jpg
cropped_ms/pict_11394.jpg
cropped_ms/pict_11395.jpg
cropped_ms/pict_11396.jpg
cropped_ms/pict_11397.jpg
cropped_ms/pict_11398.jpg
cropped_ms/pict_11399.jpg
cropped_ms/pict_114.jpg
cropped_ms/pict_1140.jpg
cropped_ms/pict_11400.jpg
cropped_ms/pict_11401.jpg
cropped_ms/pict_11402.jpg
cropped_ms/pict_11403.jpg
cropped_ms/pict_11404.jpg
cropped_ms/pict_11405.jpg
cropped_ms/pict_11406.jpg
cropped_ms/pict_11407.jpg
cropped_ms/pict_11408.jpg
cropped_ms/pict_11409.jpg
cropped_ms/pict_1141.jpg
cropped_ms/pict_11410.jpg
cropped_ms/pict_11411.jpg
cropped_ms/pict_11412.jpg
cropped_ms/pict_11413.jpg
cropped_ms/pict_11414.jpg
cropped_ms/pict_11415.jpg
cropped_ms/pict_11416.jpg
cropped_ms/pict_11417.jpg
cropped_ms/pict_11418.jpg
cropped_ms/pict_11419.jpg
cropped_ms/pict_1142.jpg
cropped_ms/pict_11420.jpg
cropped_ms/pict_11421.jpg
cropped_ms/pict_11422.jpg
cropped_ms/pict_11423.jpg
cropped_ms/pict_11424.jpg
cropped_ms/pict_11425.jpg
cropped_ms/pict_11426.jpg
cropped_ms/pict_11427.jpg
cropped_ms/pict_11428.jpg
cropped_ms/pict_11429.jpg
cropped_ms/pict_1143.jpg
cropped_ms/pict_11430.jpg
cropped_ms/pict_11431.jpg
cropped_ms/pict_11432.jpg
cropped_ms/pict_11433.jpg
cropped_ms/pict_11434.jpg
cropped_ms/pict_11435.jpg
cropped_ms/pict_11436.jpg
cropped_ms/pict_11437.jpg
cropped_ms/pict_11438.jpg
cropped_ms/pict_11439.jpg
cropped_ms/pict_1144.jpg
cropped_ms/pict_11440.jpg
cropped_ms/pict_11441.jpg
cropped_ms/pict_11442.jpg
cropped_ms/pict_11443.jpg
cropped_ms/pict_11444.jpg
cropped_ms/pict_11445.jpg
cropped_ms/pict_11446.jpg
cropped_ms/pict_11447.jpg
cropped_ms/pict_11448.jpg
cropped_ms/pict_11449.jpg
cropped_ms/pict_1145.jpg
cropped_ms/pict_11450.jpg
cropped_ms/pict_11451.jpg
cropped_ms/pict_11452.jpg
cropped_ms/pict_11453.jpg
cropped_ms/pict_11454.jpg
cropped_ms/pict_11455.jpg
cropped_ms/pict_11456.jpg
cropped_ms/pict_11457.jpg
cropped_ms/pict_11458.jpg
cropped_ms/pict_11459.jpg
cropped_ms/pict_1146.jpg
cropped_ms/pict_11460.jpg
cropped_ms/pict_11461.jpg
cropped_ms/pict_11462.jpg
cropped_ms/pict_11463.jpg
cropped_ms/pict_11464.jpg
cropped_ms/pict_11465.jpg
cropped_ms/pict_11466.jpg
cropped_ms/pict_11467.jpg
cropped_ms/pict_11468.jpg
cropped_ms/pict_11469.jpg
cropped_ms/pict_1147.jpg
cropped_ms/pict_11470.jpg
cropped_ms/pict_11471.jpg
cropped_ms/pict_11472.jpg
cropped_ms/pict_11473.jpg
cropped_ms/pict_11474.jpg
cropped_ms/pict_11475.jpg
cropped_ms/pict_11476.jpg
cropped_ms/pict_11477.jpg
cropped_ms/pict_11478.jpg
cropped_ms/pict_11479.jpg
cropped_ms/pict_1148.jpg
cropped_ms/pict_11480.jpg
cropped_ms/pict_11481.jpg
cropped_ms/pict_11482.jpg
cropped_ms/pict_11483.jpg
cropped_ms/pict_11484.jpg
cropped_ms/pict_11485.jpg
cropped_ms/pict_11486.jpg
cropped_ms/pict_11487.jpg
cropped_ms/pict_11488.jpg
cropped_ms/pict_11489.jpg
cropped_ms/pict_1149.jpg
cropped_ms/pict_11490.jpg
cropped_ms/pict_11491.jpg
cropped_ms/pict_11492.jpg
cropped_ms/pict_11493.jpg
cropped_ms/pict_11494.jpg
cropped_ms/pict_11495.jpg
cropped_ms/pict_11496.jpg
cropped_ms/pict_11497.jpg
cropped_ms/pict_11498.jpg
cropped_ms/pict_11499.jpg
cropped_ms/pict_115.jpg
cropped_ms/pict_1150.jpg
cropped_ms/pict_11500.jpg
cropped_ms/pict_11501.jpg
cropped_ms/pict_11502.jpg
cropped_ms/pict_11503.jpg
cropped_ms/pict_11504.jpg
cropped_ms/pict_11505.jpg
cropped_ms/pict_11506.jpg
cropped_ms/pict_11507.jpg
cropped_ms/pict_11508.jpg
cropped_ms/pict_11509.jpg
cropped_ms/pict_1151.jpg
cropped_ms/pict_11510.jpg
cropped_ms/pict_11511.jpg
cropped_ms/pict_11512.jpg
cropped_ms/pict_11513.jpg
cropped_ms/pict_11514.jpg
cropped_ms/pict_11515.jpg
cropped_ms/pict_11516.jpg
cropped_ms/pict_11517.jpg
cropped_ms/pict_11518.jpg
cropped_ms/pict_11519.jpg
cropped_ms/pict_1152.jpg
cropped_ms/pict_11520.jpg
cropped_ms/pict_11521.jpg
cropped_ms/pict_11522.jpg
cropped_ms/pict_11523.jpg
cropped_ms/pict_11524.jpg
cropped_ms/pict_11525.jpg
cropped_ms/pict_11526.jpg
cropped_ms/pict_11527.jpg
cropped_ms/pict_11528.jpg
cropped_ms/pict_11529.jpg
cropped_ms/pict_1153.jpg
cropped_ms/pict_11530.jpg
cropped_ms/pict_11531.jpg
cropped_ms/pict_11532.jpg
cropped_ms/pict_11533.jpg
cropped_ms/pict_11534.jpg
cropped_ms/pict_11535.jpg
cropped_ms/pict_11536.jpg
cropped_ms/pict_11537.jpg
cropped_ms/pict_11538.jpg
cropped_ms/pict_11539.jpg
cropped_ms/pict_1154.jpg
cropped_ms/pict_11540.jpg
cropped_ms/pict_11541.jpg
cropped_ms/pict_11542.jpg
cropped_ms/pict_11543.jpg
cropped_ms/pict_11544.jpg
cropped_ms/pict_11545.jpg
cropped_ms/pict_11546.jpg
cropped_ms/pict_11547.jpg
cropped_ms/pict_11548.jpg
cropped_ms/pict_11549.jpg
cropped_ms/pict_1155.jpg
cropped_ms/pict_11550.jpg
cropped_ms/pict_11551.jpg
cropped_ms/pict_11552.jpg
cropped_ms/pict_11553.jpg
cropped_ms/pict_11554.jpg
cropped_ms/pict_11555.jpg
cropped_ms/pict_11556.jpg
cropped_ms/pict_11557.jpg
cropped_ms/pict_11558.jpg
cropped_ms/pict_11559.jpg
cropped_ms/pict_1156.jpg
cropped_ms/pict_11560.jpg
cropped_ms/pict_11561.jpg
cropped_ms/pict_11562.jpg
cropped_ms/pict_11563.jpg
cropped_ms/pict_11564.jpg
cropped_ms/pict_11565.jpg
cropped_ms/pict_11566.jpg
cropped_ms/pict_11567.jpg
cropped_ms/pict_11568.jpg
cropped_ms/pict_11569.jpg
cropped_ms/pict_1157.jpg
cropped_ms/pict_11570.jpg
cropped_ms/pict_11571.jpg
cropped_ms/pict_11572.jpg
cropped_ms/pict_11573.jpg
cropped_ms/pict_11574.jpg
cropped_ms/pict_11575.jpg
cropped_ms/pict_11576.jpg
cropped_ms/pict_11577.jpg
cropped_ms/pict_11578.jpg
cropped_ms/pict_11579.jpg
cropped_ms/pict_1158.jpg
cropped_ms/pict_11580.jpg
cropped_ms/pict_11581.jpg
cropped_ms/pict_11582.jpg
cropped_ms/pict_11583.jpg
cropped_ms/pict_11584.jpg
cropped_ms/pict_11585.jpg
cropped_ms/pict_11586.jpg
cropped_ms/pict_11587.jpg
cropped_ms/pict_11588.jpg
cropped_ms/pict_11589.jpg
cropped_ms/pict_1159.jpg
cropped_ms/pict_11590.jpg
cropped_ms/pict_11591.jpg
cropped_ms/pict_11592.jpg
cropped_ms/pict_11593.jpg
cropped_ms/pict_11594.jpg
cropped_ms/pict_11595.jpg
cropped_ms/pict_11596.jpg
cropped_ms/pict_11597.jpg
cropped_ms/pict_11598.jpg
cropped_ms/pict_11599.jpg
cropped_ms/pict_116.jpg
cropped_ms/pict_1160.jpg
cropped_ms/pict_11600.jpg
cropped_ms/pict_11601.jpg
cropped_ms/pict_11602.jpg
cropped_ms/pict_11603.jpg
cropped_ms/pict_11604.jpg
cropped_ms/pict_11605.jpg
cropped_ms/pict_11606.jpg
cropped_ms/pict_11607.jpg
cropped_ms/pict_11608.jpg
cropped_ms/pict_11609.jpg
cropped_ms/pict_1161.jpg
cropped_ms/pict_11610.jpg
cropped_ms/pict_11611.jpg
cropped_ms/pict_11612.jpg
cropped_ms/pict_11613.jpg
cropped_ms/pict_11614.jpg
cropped_ms/pict_11615.jpg
cropped_ms/pict_11616.jpg
cropped_ms/pict_11617.jpg
cropped_ms/pict_11618.jpg
cropped_ms/pict_11619.jpg
cropped_ms/pict_1162.jpg
cropped_ms/pict_11620.jpg
cropped_ms/pict_11621.jpg
cropped_ms/pict_11622.jpg
cropped_ms/pict_11623.jpg
cropped_ms/pict_11624.jpg
cropped_ms/pict_11625.jpg
cropped_ms/pict_11626.jpg
cropped_ms/pict_11627.jpg
cropped_ms/pict_11628.jpg
cropped_ms/pict_11629.jpg
cropped_ms/pict_1163.jpg
cropped_ms/pict_11630.jpg
cropped_ms/pict_11631.jpg
cropped_ms/pict_11632.jpg
cropped_ms/pict_11633.jpg
cropped_ms/pict_11634.jpg
cropped_ms/pict_11635.jpg
cropped_ms/pict_11636.jpg
cropped_ms/pict_11637.jpg
cropped_ms/pict_11638.jpg
cropped_ms/pict_11639.jpg
cropped_ms/pict_1164.jpg
cropped_ms/pict_11640.jpg
cropped_ms/pict_11641.jpg
cropped_ms/pict_11642.jpg
cropped_ms/pict_11643.jpg
cropped_ms/pict_11644.jpg
cropped_ms/pict_11645.jpg
cropped_ms/pict_11646.jpg
cropped_ms/pict_11647.jpg
cropped_ms/pict_11648.jpg
cropped_ms/pict_11649.jpg
cropped_ms/pict_1165.jpg
cropped_ms/pict_11650.jpg
cropped_ms/pict_11651.jpg
cropped_ms/pict_11652.jpg
cropped_ms/pict_11653.jpg
cropped_ms/pict_11654.jpg
cropped_ms/pict_11655.jpg
cropped_ms/pict_11656.jpg
cropped_ms/pict_11657.jpg
cropped_ms/pict_11658.jpg
cropped_ms/pict_11659.jpg
cropped_ms/pict_1166.jpg
cropped_ms/pict_11660.jpg
cropped_ms/pict_11661.jpg
cropped_ms/pict_11662.jpg
cropped_ms/pict_11663.jpg
cropped_ms/pict_11664.jpg
cropped_ms/pict_11665.jpg
cropped_ms/pict_11666.jpg
cropped_ms/pict_11667.jpg
cropped_ms/pict_11668.jpg
cropped_ms/pict_11669.jpg
cropped_ms/pict_1167.jpg
cropped_ms/pict_11670.jpg
cropped_ms/pict_11671.jpg
cropped_ms/pict_11672.jpg
cropped_ms/pict_11673.jpg
cropped_ms/pict_11674.jpg
cropped_ms/pict_11675.jpg
cropped_ms/pict_11676.jpg
cropped_ms/pict_11677.jpg
cropped_ms/pict_11678.jpg
cropped_ms/pict_11679.jpg
cropped_ms/pict_1168.jpg
cropped_ms/pict_11680.jpg
cropped_ms/pict_11681.jpg
cropped_ms/pict_11682.jpg
cropped_ms/pict_11683.jpg
cropped_ms/pict_11684.jpg
cropped_ms/pict_11685.jpg
cropped_ms/pict_11686.jpg
cropped_ms/pict_11687.jpg
cropped_ms/pict_11688.jpg
cropped_ms/pict_11689.jpg
cropped_ms/pict_1169.jpg
cropped_ms/pict_11690.jpg
cropped_ms/pict_11691.jpg
cropped_ms/pict_11692.jpg
cropped_ms/pict_11693.jpg
cropped_ms/pict_11694.jpg
cropped_ms/pict_11695.jpg
cropped_ms/pict_11696.jpg
cropped_ms/pict_11697.jpg
cropped_ms/pict_11698.jpg
cropped_ms/pict_11699.jpg
cropped_ms/pict_117.jpg
cropped_ms/pict_1170.jpg
cropped_ms/pict_11700.jpg
cropped_ms/pict_11701.jpg
cropped_ms/pict_11702.jpg
cropped_ms/pict_11703.jpg
cropped_ms/pict_11704.jpg
cropped_ms/pict_11705.jpg
cropped_ms/pict_11706.jpg
cropped_ms/pict_11707.jpg
cropped_ms/pict_11708.jpg
cropped_ms/pict_11709.jpg
cropped_ms/pict_1171.jpg
cropped_ms/pict_11710.jpg
cropped_ms/pict_11711.jpg
cropped_ms/pict_11712.jpg
cropped_ms/pict_11713.jpg
cropped_ms/pict_11714.jpg
cropped_ms/pict_11715.jpg
cropped_ms/pict_11716.jpg
cropped_ms/pict_11717.jpg
cropped_ms/pict_11718.jpg
cropped_ms/pict_11719.jpg
cropped_ms/pict_1172.jpg
cropped_ms/pict_11720.jpg
cropped_ms/pict_11721.jpg
cropped_ms/pict_11722.jpg
cropped_ms/pict_11723.jpg
cropped_ms/pict_11724.jpg
cropped_ms/pict_11725.jpg
cropped_ms/pict_11726.jpg
cropped_ms/pict_11727.jpg
cropped_ms/pict_11728.jpg
cropped_ms/pict_11729.jpg
cropped_ms/pict_1173.jpg
cropped_ms/pict_11730.jpg
cropped_ms/pict_11731.jpg
cropped_ms/pict_11732.jpg
cropped_ms/pict_11733.jpg
cropped_ms/pict_11734.jpg
cropped_ms/pict_11735.jpg
cropped_ms/pict_11736.jpg
cropped_ms/pict_11737.jpg
cropped_ms/pict_11738.jpg
cropped_ms/pict_11739.jpg
cropped_ms/pict_1174.jpg
cropped_ms/pict_11740.jpg
cropped_ms/pict_11741.jpg
cropped_ms/pict_11742.jpg
cropped_ms/pict_11743.jpg
cropped_ms/pict_11744.jpg
cropped_ms/pict_11745.jpg
cropped_ms/pict_11746.jpg
cropped_ms/pict_11747.jpg
cropped_ms/pict_11748.jpg
cropped_ms/pict_11749.jpg
cropped_ms/pict_1175.jpg
cropped_ms/pict_11750.jpg
cropped_ms/pict_11751.jpg
cropped_ms/pict_11752.jpg
cropped_ms/pict_11753.jpg
cropped_ms/pict_11754.jpg
cropped_ms/pict_11755.jpg
cropped_ms/pict_11756.jpg
cropped_ms/pict_11757.jpg
cropped_ms/pict_11758.jpg
cropped_ms/pict_11759.jpg
cropped_ms/pict_1176.jpg
cropped_ms/pict_11760.jpg
cropped_ms/pict_11761.jpg
cropped_ms/pict_11762.jpg
cropped_ms/pict_11763.jpg
cropped_ms/pict_11764.jpg
cropped_ms/pict_11765.jpg
cropped_ms/pict_11766.jpg
cropped_ms/pict_11767.jpg
cropped_ms/pict_11768.jpg
cropped_ms/pict_11769.jpg
cropped_ms/pict_1177.jpg
cropped_ms/pict_11770.jpg
cropped_ms/pict_11771.jpg
cropped_ms/pict_11772.jpg
cropped_ms/pict_11773.jpg
cropped_ms/pict_11774.jpg
cropped_ms/pict_11775.jpg
cropped_ms/pict_11776.jpg
cropped_ms/pict_11777.jpg
cropped_ms/pict_11778.jpg
cropped_ms/pict_11779.jpg
cropped_ms/pict_1178.jpg
cropped_ms/pict_11780.jpg
cropped_ms/pict_11781.jpg
cropped_ms/pict_11782.jpg
cropped_ms/pict_11783.jpg
cropped_ms/pict_11784.jpg
cropped_ms/pict_11785.jpg
cropped_ms/pict_11786.jpg
cropped_ms/pict_11787.jpg
cropped_ms/pict_11788.jpg
cropped_ms/pict_11789.jpg
cropped_ms/pict_1179.jpg
cropped_ms/pict_11790.jpg
cropped_ms/pict_11791.jpg
cropped_ms/pict_11792.jpg
cropped_ms/pict_11793.jpg
cropped_ms/pict_11794.jpg
cropped_ms/pict_11795.jpg
cropped_ms/pict_11796.jpg
cropped_ms/pict_11797.jpg
cropped_ms/pict_11798.jpg
cropped_ms/pict_11799.jpg
cropped_ms/pict_118.jpg
cropped_ms/pict_1180.jpg
cropped_ms/pict_11800.jpg
cropped_ms/pict_11801.jpg
cropped_ms/pict_11802.jpg
cropped_ms/pict_11803.jpg
cropped_ms/pict_11804.jpg
cropped_ms/pict_11805.jpg
cropped_ms/pict_11806.jpg
cropped_ms/pict_11807.jpg
cropped_ms/pict_11808.jpg
cropped_ms/pict_11809.jpg
cropped_ms/pict_1181.jpg
cropped_ms/pict_11810.jpg
cropped_ms/pict_11811.jpg
cropped_ms/pict_11812.jpg
cropped_ms/pict_11813.jpg
cropped_ms/pict_11814.jpg
cropped_ms/pict_11815.jpg
cropped_ms/pict_11816.jpg
cropped_ms/pict_11817.jpg
cropped_ms/pict_11818.jpg
cropped_ms/pict_11819.jpg
cropped_ms/pict_1182.jpg
cropped_ms/pict_11820.jpg
cropped_ms/pict_11821.jpg
cropped_ms/pict_11822.jpg
cropped_ms/pict_11823.jpg
cropped_ms/pict_11824.jpg
cropped_ms/pict_11825.jpg
cropped_ms/pict_11826.jpg
cropped_ms/pict_11827.jpg
cropped_ms/pict_11828.jpg
cropped_ms/pict_11829.jpg
cropped_ms/pict_1183.jpg
cropped_ms/pict_11830.jpg
cropped_ms/pict_11831.jpg
cropped_ms/pict_11832.jpg
cropped_ms/pict_11833.jpg
cropped_ms/pict_11834.jpg
cropped_ms/pict_11835.jpg
cropped_ms/pict_11836.jpg
cropped_ms/pict_11837.jpg
cropped_ms/pict_11838.jpg
cropped_ms/pict_11839.jpg
cropped_ms/pict_1184.jpg
cropped_ms/pict_11840.jpg
cropped_ms/pict_11841.jpg
cropped_ms/pict_11842.jpg
cropped_ms/pict_11843.jpg
cropped_ms/pict_11844.jpg
cropped_ms/pict_11845.jpg
cropped_ms/pict_11846.jpg
cropped_ms/pict_11847.jpg
cropped_ms/pict_11848.jpg
cropped_ms/pict_11849.jpg
cropped_ms/pict_1185.jpg
cropped_ms/pict_11850.jpg
cropped_ms/pict_11851.jpg
cropped_ms/pict_11852.jpg
cropped_ms/pict_11853.jpg
cropped_ms/pict_11854.jpg
cropped_ms/pict_11855.jpg
cropped_ms/pict_11856.jpg
cropped_ms/pict_11857.jpg
cropped_ms/pict_11858.jpg
cropped_ms/pict_11859.jpg
cropped_ms/pict_1186.jpg
cropped_ms/pict_11860.jpg
cropped_ms/pict_11861.jpg
cropped_ms/pict_11862.jpg
cropped_ms/pict_11863.jpg
cropped_ms/pict_11864.jpg
cropped_ms/pict_11865.jpg
cropped_ms/pict_11866.jpg
cropped_ms/pict_11867.jpg
cropped_ms/pict_11868.jpg
cropped_ms/pict_11869.jpg
cropped_ms/pict_1187.jpg
cropped_ms/pict_11870.jpg
cropped_ms/pict_11871.jpg
cropped_ms/pict_11872.jpg
cropped_ms/pict_11873.jpg
cropped_ms/pict_11874.jpg
cropped_ms/pict_11875.jpg
cropped_ms/pict_11876.jpg
cropped_ms/pict_11877.jpg
cropped_ms/pict_11878.jpg
cropped_ms/pict_11879.jpg
cropped_ms/pict_1188.jpg
cropped_ms/pict_11880.jpg
cropped_ms/pict_11881.jpg
cropped_ms/pict_11882.jpg
cropped_ms/pict_11883.jpg
cropped_ms/pict_11884.jpg
cropped_ms/pict_11885.jpg
cropped_ms/pict_11886.jpg
cropped_ms/pict_11887.jpg
cropped_ms/pict_11888.jpg
cropped_ms/pict_11889.jpg
cropped_ms/pict_1189.jpg
cropped_ms/pict_11890.jpg
cropped_ms/pict_11891.jpg
cropped_ms/pict_11892.jpg
cropped_ms/pict_11893.jpg
cropped_ms/pict_11894.jpg
cropped_ms/pict_11895.jpg
cropped_ms/pict_11896.jpg
cropped_ms/pict_11897.jpg
cropped_ms/pict_11898.jpg
cropped_ms/pict_11899.jpg
cropped_ms/pict_119.jpg
cropped_ms/pict_1190.jpg
cropped_ms/pict_11900.jpg
cropped_ms/pict_11901.jpg
cropped_ms/pict_11902.jpg
cropped_ms/pict_11903.jpg
cropped_ms/pict_11904.jpg
cropped_ms/pict_11905.jpg
cropped_ms/pict_11906.jpg
cropped_ms/pict_11907.jpg
cropped_ms/pict_11908.jpg
cropped_ms/pict_11909.jpg
cropped_ms/pict_1191.jpg
cropped_ms/pict_11910.jpg
cropped_ms/pict_11911.jpg
cropped_ms/pict_11912.jpg
cropped_ms/pict_11913.jpg
cropped_ms/pict_11914.jpg
cropped_ms/pict_11915.jpg
cropped_ms/pict_11916.jpg
cropped_ms/pict_11917.jpg
cropped_ms/pict_11918.jpg
cropped_ms/pict_11919.jpg
cropped_ms/pict_1192.jpg
cropped_ms/pict_11920.jpg
cropped_ms/pict_11921.jpg
cropped_ms/pict_11922.jpg
cropped_ms/pict_11923.jpg
cropped_ms/pict_11924.jpg
cropped_ms/pict_11925.jpg
cropped_ms/pict_11926.jpg
cropped_ms/pict_11927.jpg
cropped_ms/pict_11928.jpg
cropped_ms/pict_11929.jpg
cropped_ms/pict_1193.jpg
cropped_ms/pict_11930.jpg
cropped_ms/pict_11931.jpg
cropped_ms/pict_11932.jpg
cropped_ms/pict_11933.jpg
cropped_ms/pict_11934.jpg
cropped_ms/pict_11935.jpg
cropped_ms/pict_11936.jpg
cropped_ms/pict_11937.jpg
cropped_ms/pict_11938.jpg
cropped_ms/pict_11939.jpg
cropped_ms/pict_1194.jpg
cropped_ms/pict_11940.jpg
cropped_ms/pict_11941.jpg
cropped_ms/pict_11942.jpg
cropped_ms/pict_11943.jpg
cropped_ms/pict_11944.jpg
cropped_ms/pict_11945.jpg
cropped_ms/pict_11946.jpg
cropped_ms/pict_11947.jpg
cropped_ms/pict_11948.jpg
cropped_ms/pict_11949.jpg
cropped_ms/pict_1195.jpg
cropped_ms/pict_11950.jpg
cropped_ms/pict_11951.jpg
cropped_ms/pict_11952.jpg
cropped_ms/pict_11953.jpg
cropped_ms/pict_11954.jpg
cropped_ms/pict_11955.jpg
cropped_ms/pict_11956.jpg
cropped_ms/pict_11957.jpg
cropped_ms/pict_11958.jpg
cropped_ms/pict_11959.jpg
cropped_ms/pict_1196.jpg
cropped_ms/pict_11960.jpg
cropped_ms/pict_11961.jpg
cropped_ms/pict_11962.jpg
cropped_ms/pict_11963.jpg
cropped_ms/pict_11964.jpg
cropped_ms/pict_11965.jpg
cropped_ms/pict_11966.jpg
cropped_ms/pict_11967.jpg
cropped_ms/pict_11968.jpg
cropped_ms/pict_11969.jpg
cropped_ms/pict_1197.jpg
cropped_ms/pict_11970.jpg
cropped_ms/pict_11971.jpg
cropped_ms/pict_11972.jpg
cropped_ms/pict_11973.jpg
cropped_ms/pict_11974.jpg
cropped_ms/pict_11975.jpg
cropped_ms/pict_11976.jpg
cropped_ms/pict_11977.jpg
cropped_ms/pict_11978.jpg
cropped_ms/pict_11979.jpg
cropped_ms/pict_1198.jpg
cropped_ms/pict_11980.jpg
cropped_ms/pict_11981.jpg
cropped_ms/pict_11982.jpg
cropped_ms/pict_11983.jpg
cropped_ms/pict_11984.jpg
cropped_ms/pict_11985.jpg
cropped_ms/pict_11986.jpg
cropped_ms/pict_11987.jpg
cropped_ms/pict_11988.jpg
cropped_ms/pict_11989.jpg
cropped_ms/pict_1199.jpg
cropped_ms/pict_11990.jpg
cropped_ms/pict_11991.jpg
cropped_ms/pict_11992.jpg
cropped_ms/pict_11993.jpg
cropped_ms/pict_11994.jpg
cropped_ms/pict_11995.jpg
cropped_ms/pict_11996.jpg
cropped_ms/pict_11997.jpg
cropped_ms/pict_11998.jpg
cropped_ms/pict_11999.jpg
cropped_ms/pict_12.jpg
cropped_ms/pict_120.jpg
cropped_ms/pict_1200.jpg
cropped_ms/pict_12000.jpg
cropped_ms/pict_12001.jpg
cropped_ms/pict_12002.jpg
cropped_ms/pict_12003.jpg
cropped_ms/pict_12004.jpg
cropped_ms/pict_12005.jpg
cropped_ms/pict_12006.jpg
cropped_ms/pict_12007.jpg
cropped_ms/pict_12008.jpg
cropped_ms/pict_12009.jpg
cropped_ms/pict_1201.jpg
cropped_ms/pict_12010.jpg
cropped_ms/pict_12011.jpg
cropped_ms/pict_12012.jpg
cropped_ms/pict_12013.jpg
cropped_ms/pict_12014.jpg
cropped_ms/pict_12015.jpg
cropped_ms/pict_12016.jpg
cropped_ms/pict_12017.jpg
cropped_ms/pict_12018.jpg
cropped_ms/pict_12019.jpg
cropped_ms/pict_1202.jpg
cropped_ms/pict_12020.jpg
cropped_ms/pict_12021.jpg
cropped_ms/pict_12022.jpg
cropped_ms/pict_12023.jpg
cropped_ms/pict_12024.jpg
cropped_ms/pict_12025.jpg
cropped_ms/pict_12026.jpg
cropped_ms/pict_12027.jpg
cropped_ms/pict_12028.jpg
cropped_ms/pict_12029.jpg
cropped_ms/pict_1203.jpg
cropped_ms/pict_12030.jpg
cropped_ms/pict_12031.jpg
cropped_ms/pict_12032.jpg
cropped_ms/pict_12033.jpg
cropped_ms/pict_12034.jpg
cropped_ms/pict_12035.jpg
cropped_ms/pict_12036.jpg
cropped_ms/pict_12037.jpg
cropped_ms/pict_12038.jpg
cropped_ms/pict_12039.jpg
cropped_ms/pict_1204.jpg
cropped_ms/pict_12040.jpg
cropped_ms/pict_12041.jpg
cropped_ms/pict_12042.jpg
cropped_ms/pict_12043.jpg
cropped_ms/pict_12044.jpg
cropped_ms/pict_12045.jpg
cropped_ms/pict_12046.jpg
cropped_ms/pict_12047.jpg
cropped_ms/pict_12048.jpg
cropped_ms/pict_12049.jpg
cropped_ms/pict_1205.jpg
cropped_ms/pict_12050.jpg
cropped_ms/pict_12051.jpg
cropped_ms/pict_12052.jpg
cropped_ms/pict_12053.jpg
cropped_ms/pict_12054.jpg
cropped_ms/pict_12055.jpg
cropped_ms/pict_12056.jpg
cropped_ms/pict_12057.jpg
cropped_ms/pict_12058.jpg
cropped_ms/pict_12059.jpg
cropped_ms/pict_1206.jpg
cropped_ms/pict_12060.jpg
cropped_ms/pict_12061.jpg
cropped_ms/pict_12062.jpg
cropped_ms/pict_12063.jpg
cropped_ms/pict_12064.jpg
cropped_ms/pict_12065.jpg
cropped_ms/pict_12066.jpg
cropped_ms/pict_12067.jpg
cropped_ms/pict_12068.jpg
cropped_ms/pict_12069.jpg
cropped_ms/pict_1207.jpg
cropped_ms/pict_12070.jpg
cropped_ms/pict_12071.jpg
cropped_ms/pict_12072.jpg
cropped_ms/pict_12073.jpg
cropped_ms/pict_12074.jpg
cropped_ms/pict_12075.jpg
cropped_ms/pict_12076.jpg
cropped_ms/pict_12077.jpg
cropped_ms/pict_12078.jpg
cropped_ms/pict_12079.jpg
cropped_ms/pict_1208.jpg
cropped_ms/pict_12080.jpg
cropped_ms/pict_12081.jpg
cropped_ms/pict_12082.jpg
cropped_ms/pict_12083.jpg
cropped_ms/pict_12084.jpg
cropped_ms/pict_12085.jpg
cropped_ms/pict_12086.jpg
cropped_ms/pict_12087.jpg
cropped_ms/pict_12088.jpg
cropped_ms/pict_12089.jpg
cropped_ms/pict_1209.jpg
cropped_ms/pict_12090.jpg
cropped_ms/pict_12091.jpg
cropped_ms/pict_12092.jpg
cropped_ms/pict_12093.jpg
cropped_ms/pict_12094.jpg
cropped_ms/pict_12095.jpg
cropped_ms/pict_12096.jpg
cropped_ms/pict_12097.jpg
cropped_ms/pict_12098.jpg
cropped_ms/pict_12099.jpg
cropped_ms/pict_121.jpg
cropped_ms/pict_1210.jpg
cropped_ms/pict_12100.jpg
cropped_ms/pict_12101.jpg
cropped_ms/pict_12102.jpg
cropped_ms/pict_12103.jpg
cropped_ms/pict_12104.jpg
cropped_ms/pict_12105.jpg
cropped_ms/pict_12106.jpg
cropped_ms/pict_12107.jpg
cropped_ms/pict_12108.jpg
cropped_ms/pict_12109.jpg
cropped_ms/pict_1211.jpg
cropped_ms/pict_12110.jpg
cropped_ms/pict_12111.jpg
cropped_ms/pict_12112.jpg
cropped_ms/pict_12113.jpg
cropped_ms/pict_12114.jpg
cropped_ms/pict_12115.jpg
cropped_ms/pict_12116.jpg
cropped_ms/pict_12117.jpg
cropped_ms/pict_12118.jpg
cropped_ms/pict_12119.jpg
cropped_ms/pict_1212.jpg
cropped_ms/pict_12120.jpg
cropped_ms/pict_12121.jpg
cropped_ms/pict_12122.jpg
cropped_ms/pict_12123.jpg
cropped_ms/pict_12124.jpg
cropped_ms/pict_12125.jpg
cropped_ms/pict_12126.jpg
cropped_ms/pict_12127.jpg
cropped_ms/pict_12128.jpg
cropped_ms/pict_12129.jpg
cropped_ms/pict_1213.jpg
cropped_ms/pict_12130.jpg
cropped_ms/pict_12131.jpg
cropped_ms/pict_12132.jpg
cropped_ms/pict_12133.jpg
cropped_ms/pict_12134.jpg
cropped_ms/pict_12135.jpg
cropped_ms/pict_12136.jpg
cropped_ms/pict_12137.jpg
cropped_ms/pict_12138.jpg
cropped_ms/pict_12139.jpg
cropped_ms/pict_1214.jpg
cropped_ms/pict_12140.jpg
cropped_ms/pict_12141.jpg
cropped_ms/pict_12142.jpg
cropped_ms/pict_12143.jpg
cropped_ms/pict_12144.jpg
cropped_ms/pict_12145.jpg
cropped_ms/pict_12146.jpg
cropped_ms/pict_12147.jpg
cropped_ms/pict_12148.jpg
cropped_ms/pict_12149.jpg
cropped_ms/pict_1215.jpg
cropped_ms/pict_12150.jpg
cropped_ms/pict_12151.jpg
cropped_ms/pict_12152.jpg
cropped_ms/pict_12153.jpg
cropped_ms/pict_12154.jpg
cropped_ms/pict_12155.jpg
cropped_ms/pict_12156.jpg
cropped_ms/pict_12157.jpg
cropped_ms/pict_12158.jpg
cropped_ms/pict_12159.jpg
cropped_ms/pict_1216.jpg
cropped_ms/pict_12160.jpg
cropped_ms/pict_12161.jpg
cropped_ms/pict_12162.jpg
cropped_ms/pict_12163.jpg
cropped_ms/pict_12164.jpg
cropped_ms/pict_12165.jpg
cropped_ms/pict_12166.jpg
cropped_ms/pict_12167.jpg
cropped_ms/pict_12168.jpg
cropped_ms/pict_12169.jpg
cropped_ms/pict_1217.jpg
cropped_ms/pict_12170.jpg
cropped_ms/pict_12171.jpg
cropped_ms/pict_12172.jpg
cropped_ms/pict_12173.jpg
cropped_ms/pict_12174.jpg
cropped_ms/pict_12175.jpg
cropped_ms/pict_12176.jpg
cropped_ms/pict_12177.jpg
cropped_ms/pict_12178.jpg
cropped_ms/pict_12179.jpg
cropped_ms/pict_1218.jpg
cropped_ms/pict_12180.jpg
cropped_ms/pict_12181.jpg
cropped_ms/pict_12182.jpg
cropped_ms/pict_12183.jpg
cropped_ms/pict_12184.jpg
cropped_ms/pict_12185.jpg
cropped_ms/pict_12186.jpg
cropped_ms/pict_12187.jpg
cropped_ms/pict_12188.jpg
cropped_ms/pict_12189.jpg
cropped_ms/pict_1219.jpg
cropped_ms/pict_12190.jpg
cropped_ms/pict_12191.jpg
cropped_ms/pict_12192.jpg
cropped_ms/pict_12193.jpg
cropped_ms/pict_12194.jpg
cropped_ms/pict_12195.jpg
cropped_ms/pict_12196.jpg
cropped_ms/pict_12197.jpg
cropped_ms/pict_12198.jpg
cropped_ms/pict_12199.jpg
cropped_ms/pict_122.jpg
cropped_ms/pict_1220.jpg
cropped_ms/pict_12200.jpg
cropped_ms/pict_12201.jpg
cropped_ms/pict_12202.jpg
cropped_ms/pict_12203.jpg
cropped_ms/pict_12204.jpg
cropped_ms/pict_12205.jpg
cropped_ms/pict_12206.jpg
cropped_ms/pict_12207.jpg
cropped_ms/pict_12208.jpg
cropped_ms/pict_12209.jpg
cropped_ms/pict_1221.jpg
cropped_ms/pict_12210.jpg
cropped_ms/pict_12211.jpg
cropped_ms/pict_12212.jpg
cropped_ms/pict_12213.jpg
cropped_ms/pict_12214.jpg
cropped_ms/pict_12215.jpg
cropped_ms/pict_12216.jpg
cropped_ms/pict_12217.jpg
cropped_ms/pict_12218.jpg
cropped_ms/pict_12219.jpg
cropped_ms/pict_1222.jpg
cropped_ms/pict_12220.jpg
cropped_ms/pict_12221.jpg
cropped_ms/pict_12222.jpg
cropped_ms/pict_12223.jpg
cropped_ms/pict_12224.jpg
cropped_ms/pict_12225.jpg
cropped_ms/pict_12226.jpg
cropped_ms/pict_12227.jpg
cropped_ms/pict_12228.jpg
cropped_ms/pict_12229.jpg
cropped_ms/pict_1223.jpg
cropped_ms/pict_12230.jpg
cropped_ms/pict_12231.jpg
cropped_ms/pict_12232.jpg
cropped_ms/pict_12233.jpg
cropped_ms/pict_12234.jpg
cropped_ms/pict_12235.jpg
cropped_ms/pict_12236.jpg
cropped_ms/pict_12237.jpg
cropped_ms/pict_12238.jpg
cropped_ms/pict_12239.jpg
cropped_ms/pict_1224.jpg
cropped_ms/pict_12240.jpg
cropped_ms/pict_12241.jpg
cropped_ms/pict_12242.jpg
cropped_ms/pict_12243.jpg
cropped_ms/pict_12244.jpg
cropped_ms/pict_12245.jpg
cropped_ms/pict_12246.jpg
cropped_ms/pict_12247.jpg
cropped_ms/pict_12248.jpg
cropped_ms/pict_12249.jpg
cropped_ms/pict_1225.jpg
cropped_ms/pict_12250.jpg
cropped_ms/pict_12251.jpg
cropped_ms/pict_12252.jpg
cropped_ms/pict_12253.jpg
cropped_ms/pict_12254.jpg
cropped_ms/pict_12255.jpg
cropped_ms/pict_12256.jpg
cropped_ms/pict_12257.jpg
cropped_ms/pict_12258.jpg
cropped_ms/pict_12259.jpg
cropped_ms/pict_1226.jpg
cropped_ms/pict_12260.jpg
cropped_ms/pict_12261.jpg
cropped_ms/pict_12262.jpg
cropped_ms/pict_12263.jpg
cropped_ms/pict_12264.jpg
cropped_ms/pict_12265.jpg
cropped_ms/pict_12266.jpg
cropped_ms/pict_12267.jpg
cropped_ms/pict_12268.jpg
cropped_ms/pict_12269.jpg
cropped_ms/pict_1227.jpg
cropped_ms/pict_12270.jpg
cropped_ms/pict_12271.jpg
cropped_ms/pict_12272.jpg
cropped_ms/pict_12273.jpg
cropped_ms/pict_12274.jpg
cropped_ms/pict_12275.jpg
cropped_ms/pict_12276.jpg
cropped_ms/pict_12277.jpg
cropped_ms/pict_12278.jpg
cropped_ms/pict_12279.jpg
cropped_ms/pict_1228.jpg
cropped_ms/pict_12280.jpg
cropped_ms/pict_12281.jpg
cropped_ms/pict_12282.jpg
cropped_ms/pict_12283.jpg
cropped_ms/pict_12284.jpg
cropped_ms/pict_12285.jpg
cropped_ms/pict_12286.jpg
cropped_ms/pict_12287.jpg
cropped_ms/pict_12288.jpg
cropped_ms/pict_12289.jpg
cropped_ms/pict_1229.jpg
cropped_ms/pict_12290.jpg
cropped_ms/pict_12291.jpg
cropped_ms/pict_12292.jpg
cropped_ms/pict_12293.jpg
cropped_ms/pict_12294.jpg
cropped_ms/pict_12295.jpg
cropped_ms/pict_12296.jpg
cropped_ms/pict_12297.jpg
cropped_ms/pict_12298.jpg
cropped_ms/pict_12299.jpg
cropped_ms/pict_123.jpg
cropped_ms/pict_1230.jpg
cropped_ms/pict_12300.jpg
cropped_ms/pict_12301.jpg
cropped_ms/pict_12302.jpg
cropped_ms/pict_12303.jpg
cropped_ms/pict_12304.jpg
cropped_ms/pict_12305.jpg
cropped_ms/pict_12306.jpg
cropped_ms/pict_12307.jpg
cropped_ms/pict_12308.jpg
cropped_ms/pict_12309.jpg
cropped_ms/pict_1231.jpg
cropped_ms/pict_12310.jpg
cropped_ms/pict_12311.jpg
cropped_ms/pict_12312.jpg
cropped_ms/pict_12313.jpg
cropped_ms/pict_12314.jpg
cropped_ms/pict_12315.jpg
cropped_ms/pict_12316.jpg
cropped_ms/pict_12317.jpg
cropped_ms/pict_12318.jpg
cropped_ms/pict_12319.jpg
cropped_ms/pict_1232.jpg
cropped_ms/pict_12320.jpg
cropped_ms/pict_12321.jpg
cropped_ms/pict_12322.jpg
cropped_ms/pict_12323.jpg
cropped_ms/pict_12324.jpg
cropped_ms/pict_12325.jpg
cropped_ms/pict_12326.jpg
cropped_ms/pict_12327.jpg
cropped_ms/pict_12328.jpg
cropped_ms/pict_12329.jpg
cropped_ms/pict_1233.jpg
cropped_ms/pict_12330.jpg
cropped_ms/pict_12331.jpg
cropped_ms/pict_12332.jpg
cropped_ms/pict_12333.jpg
cropped_ms/pict_12334.jpg
cropped_ms/pict_12335.jpg
cropped_ms/pict_12336.jpg
cropped_ms/pict_12337.jpg
cropped_ms/pict_12338.jpg
cropped_ms/pict_12339.jpg
cropped_ms/pict_1234.jpg
cropped_ms/pict_12340.jpg
cropped_ms/pict_12341.jpg
cropped_ms/pict_12342.jpg
cropped_ms/pict_12343.jpg
cropped_ms/pict_12344.jpg
cropped_ms/pict_12345.jpg
cropped_ms/pict_12346.jpg
cropped_ms/pict_12347.jpg
cropped_ms/pict_12348.jpg
cropped_ms/pict_12349.jpg
cropped_ms/pict_1235.jpg
cropped_ms/pict_12350.jpg
cropped_ms/pict_12351.jpg
cropped_ms/pict_12352.jpg
cropped_ms/pict_12353.jpg
cropped_ms/pict_12354.jpg
cropped_ms/pict_12355.jpg
cropped_ms/pict_12356.jpg
cropped_ms/pict_12357.jpg
cropped_ms/pict_12358.jpg
cropped_ms/pict_12359.jpg
cropped_ms/pict_1236.jpg
cropped_ms/pict_12360.jpg
cropped_ms/pict_12361.jpg
cropped_ms/pict_12362.jpg
cropped_ms/pict_12363.jpg
cropped_ms/pict_12364.jpg
cropped_ms/pict_12365.jpg
cropped_ms/pict_12366.jpg
cropped_ms/pict_12367.jpg
cropped_ms/pict_12368.jpg
cropped_ms/pict_12369.jpg
cropped_ms/pict_1237.jpg
cropped_ms/pict_12370.jpg
cropped_ms/pict_12371.jpg
cropped_ms/pict_12372.jpg
cropped_ms/pict_12373.jpg
cropped_ms/pict_12374.jpg
cropped_ms/pict_12375.jpg
cropped_ms/pict_12376.jpg
cropped_ms/pict_12377.jpg
cropped_ms/pict_12378.jpg
cropped_ms/pict_12379.jpg
cropped_ms/pict_1238.jpg
cropped_ms/pict_12380.jpg
cropped_ms/pict_12381.jpg
cropped_ms/pict_12382.jpg
cropped_ms/pict_12383.jpg
cropped_ms/pict_12384.jpg
cropped_ms/pict_12385.jpg
cropped_ms/pict_12386.jpg
cropped_ms/pict_12387.jpg
cropped_ms/pict_12388.jpg
cropped_ms/pict_12389.jpg
cropped_ms/pict_1239.jpg
cropped_ms/pict_12390.jpg
cropped_ms/pict_12391.jpg
cropped_ms/pict_12392.jpg
cropped_ms/pict_12393.jpg
cropped_ms/pict_12394.jpg
cropped_ms/pict_12395.jpg
cropped_ms/pict_12396.jpg
cropped_ms/pict_12397.jpg
cropped_ms/pict_12398.jpg
cropped_ms/pict_12399.jpg
cropped_ms/pict_124.jpg
cropped_ms/pict_1240.jpg
cropped_ms/pict_12400.jpg
cropped_ms/pict_12401.jpg
cropped_ms/pict_12402.jpg
cropped_ms/pict_12403.jpg
cropped_ms/pict_12404.jpg
cropped_ms/pict_12405.jpg
cropped_ms/pict_12406.jpg
cropped_ms/pict_12407.jpg
cropped_ms/pict_12408.jpg
cropped_ms/pict_12409.jpg
cropped_ms/pict_1241.jpg
cropped_ms/pict_12410.jpg
cropped_ms/pict_12411.jpg
cropped_ms/pict_12412.jpg
cropped_ms/pict_12413.jpg
cropped_ms/pict_12414.jpg
cropped_ms/pict_12415.jpg
cropped_ms/pict_12416.jpg
cropped_ms/pict_12417.jpg
cropped_ms/pict_12418.jpg
cropped_ms/pict_12419.jpg
cropped_ms/pict_1242.jpg
cropped_ms/pict_12420.jpg
cropped_ms/pict_12421.jpg
cropped_ms/pict_12422.jpg
cropped_ms/pict_12423.jpg
cropped_ms/pict_12424.jpg
cropped_ms/pict_12425.jpg
cropped_ms/pict_12426.jpg
cropped_ms/pict_12427.jpg
cropped_ms/pict_12428.jpg
cropped_ms/pict_12429.jpg
cropped_ms/pict_1243.jpg
cropped_ms/pict_12430.jpg
cropped_ms/pict_12431.jpg
cropped_ms/pict_12432.jpg
cropped_ms/pict_12433.jpg
cropped_ms/pict_12434.jpg
cropped_ms/pict_12435.jpg
cropped_ms/pict_12436.jpg
cropped_ms/pict_12437.jpg
cropped_ms/pict_12438.jpg
cropped_ms/pict_12439.jpg
cropped_ms/pict_1244.jpg
cropped_ms/pict_12440.jpg
cropped_ms/pict_12441.jpg
cropped_ms/pict_12442.jpg
cropped_ms/pict_12443.jpg
cropped_ms/pict_12444.jpg
cropped_ms/pict_12445.jpg
cropped_ms/pict_12446.jpg
cropped_ms/pict_12447.jpg
cropped_ms/pict_12448.jpg
cropped_ms/pict_12449.jpg
cropped_ms/pict_1245.jpg
cropped_ms/pict_12450.jpg
cropped_ms/pict_12451.jpg
cropped_ms/pict_12452.jpg
cropped_ms/pict_12453.jpg
cropped_ms/pict_12454.jpg
cropped_ms/pict_12455.jpg
cropped_ms/pict_12456.jpg
cropped_ms/pict_12457.jpg
cropped_ms/pict_12458.jpg
cropped_ms/pict_12459.jpg
cropped_ms/pict_1246.jpg
cropped_ms/pict_12460.jpg
cropped_ms/pict_12461.jpg
cropped_ms/pict_12462.jpg
cropped_ms/pict_12463.jpg
cropped_ms/pict_12464.jpg
cropped_ms/pict_12465.jpg
cropped_ms/pict_12466.jpg
cropped_ms/pict_12467.jpg
cropped_ms/pict_12468.jpg
cropped_ms/pict_12469.jpg
cropped_ms/pict_1247.jpg
cropped_ms/pict_12470.jpg
cropped_ms/pict_12471.jpg
cropped_ms/pict_12472.jpg
cropped_ms/pict_12473.jpg
cropped_ms/pict_12474.jpg
cropped_ms/pict_12475.jpg
cropped_ms/pict_12476.jpg
cropped_ms/pict_12477.jpg
cropped_ms/pict_12478.jpg
cropped_ms/pict_12479.jpg
cropped_ms/pict_1248.jpg
cropped_ms/pict_12480.jpg
cropped_ms/pict_12481.jpg
cropped_ms/pict_12482.jpg
cropped_ms/pict_12483.jpg
cropped_ms/pict_12484.jpg
cropped_ms/pict_12485.jpg
cropped_ms/pict_12486.jpg
cropped_ms/pict_12487.jpg
cropped_ms/pict_12488.jpg
cropped_ms/pict_12489.jpg
cropped_ms/pict_1249.jpg
cropped_ms/pict_12490.jpg
cropped_ms/pict_12491.jpg
cropped_ms/pict_12492.jpg
cropped_ms/pict_12493.jpg
cropped_ms/pict_12494.jpg
cropped_ms/pict_12495.jpg
cropped_ms/pict_12496.jpg
cropped_ms/pict_12497.jpg
cropped_ms/pict_12498.jpg
cropped_ms/pict_12499.jpg
cropped_ms/pict_125.jpg
cropped_ms/pict_1250.jpg
cropped_ms/pict_12500.jpg
cropped_ms/pict_12501.jpg
cropped_ms/pict_12502.jpg
cropped_ms/pict_12503.jpg
cropped_ms/pict_12504.jpg
cropped_ms/pict_12505.jpg
cropped_ms/pict_12506.jpg
cropped_ms/pict_12507.jpg
cropped_ms/pict_12508.jpg
cropped_ms/pict_12509.jpg
cropped_ms/pict_1251.jpg
cropped_ms/pict_12510.jpg
cropped_ms/pict_12511.jpg
cropped_ms/pict_12512.jpg
cropped_ms/pict_12513.jpg
cropped_ms/pict_12514.jpg
cropped_ms/pict_12515.jpg
cropped_ms/pict_12516.jpg
cropped_ms/pict_12517.jpg
cropped_ms/pict_12518.jpg
cropped_ms/pict_12519.jpg
cropped_ms/pict_1252.jpg
cropped_ms/pict_12520.jpg
cropped_ms/pict_12521.jpg
cropped_ms/pict_12522.jpg
cropped_ms/pict_12523.jpg
cropped_ms/pict_12524.jpg
cropped_ms/pict_12525.jpg
cropped_ms/pict_12526.jpg
cropped_ms/pict_12527.jpg
cropped_ms/pict_12528.jpg
cropped_ms/pict_12529.jpg
cropped_ms/pict_1253.jpg
cropped_ms/pict_12530.jpg
cropped_ms/pict_12531.jpg
cropped_ms/pict_12532.jpg
cropped_ms/pict_12533.jpg
cropped_ms/pict_12534.jpg
cropped_ms/pict_12535.jpg
cropped_ms/pict_12536.jpg
cropped_ms/pict_12537.jpg
cropped_ms/pict_12538.jpg
cropped_ms/pict_12539.jpg
cropped_ms/pict_1254.jpg
cropped_ms/pict_12540.jpg
cropped_ms/pict_12541.jpg
cropped_ms/pict_12542.jpg
cropped_ms/pict_12543.jpg
cropped_ms/pict_12544.jpg
cropped_ms/pict_12545.jpg
cropped_ms/pict_12546.jpg
cropped_ms/pict_12547.jpg
cropped_ms/pict_12548.jpg
cropped_ms/pict_12549.jpg
cropped_ms/pict_1255.jpg
cropped_ms/pict_12550.jpg
cropped_ms/pict_12551.jpg
cropped_ms/pict_12552.jpg
cropped_ms/pict_12553.jpg
cropped_ms/pict_12554.jpg
cropped_ms/pict_12555.jpg
cropped_ms/pict_12556.jpg
cropped_ms/pict_12557.jpg
cropped_ms/pict_12558.jpg
cropped_ms/pict_12559.jpg
cropped_ms/pict_1256.jpg
cropped_ms/pict_12560.jpg
cropped_ms/pict_12561.jpg
cropped_ms/pict_12562.jpg
cropped_ms/pict_12563.jpg
cropped_ms/pict_12564.jpg
cropped_ms/pict_12565.jpg
cropped_ms/pict_12566.jpg
cropped_ms/pict_12567.jpg
cropped_ms/pict_12568.jpg
cropped_ms/pict_12569.jpg
cropped_ms/pict_1257.jpg
cropped_ms/pict_12570.jpg
cropped_ms/pict_12571.jpg
cropped_ms/pict_12572.jpg
cropped_ms/pict_12573.jpg
cropped_ms/pict_12574.jpg
cropped_ms/pict_12575.jpg
cropped_ms/pict_12576.jpg
cropped_ms/pict_12577.jpg
cropped_ms/pict_12578.jpg
cropped_ms/pict_12579.jpg
cropped_ms/pict_1258.jpg
cropped_ms/pict_12580.jpg
cropped_ms/pict_12581.jpg
cropped_ms/pict_12582.jpg
cropped_ms/pict_12583.jpg
cropped_ms/pict_12584.jpg
cropped_ms/pict_12585.jpg
cropped_ms/pict_12586.jpg
cropped_ms/pict_12587.jpg
cropped_ms/pict_12588.jpg
cropped_ms/pict_12589.jpg
cropped_ms/pict_1259.jpg
cropped_ms/pict_12590.jpg
cropped_ms/pict_12591.jpg
cropped_ms/pict_12592.jpg
cropped_ms/pict_12593.jpg
cropped_ms/pict_12594.jpg
cropped_ms/pict_12595.jpg
cropped_ms/pict_12596.jpg
cropped_ms/pict_12597.jpg
cropped_ms/pict_12598.jpg
cropped_ms/pict_12599.jpg
cropped_ms/pict_126.jpg
cropped_ms/pict_1260.jpg
cropped_ms/pict_12600.jpg
cropped_ms/pict_12601.jpg
cropped_ms/pict_12602.jpg
cropped_ms/pict_12603.jpg
cropped_ms/pict_12604.jpg
cropped_ms/pict_12605.jpg
cropped_ms/pict_12606.jpg
cropped_ms/pict_12607.jpg
cropped_ms/pict_12608.jpg
cropped_ms/pict_12609.jpg
cropped_ms/pict_1261.jpg
cropped_ms/pict_12610.jpg
cropped_ms/pict_12611.jpg
cropped_ms/pict_12612.jpg
cropped_ms/pict_12613.jpg
cropped_ms/pict_12614.jpg
cropped_ms/pict_12615.jpg
cropped_ms/pict_12616.jpg
cropped_ms/pict_12617.jpg
cropped_ms/pict_12618.jpg
cropped_ms/pict_12619.jpg
cropped_ms/pict_1262.jpg
cropped_ms/pict_12620.jpg
cropped_ms/pict_12621.jpg
cropped_ms/pict_12622.jpg
cropped_ms/pict_12623.jpg
cropped_ms/pict_12624.jpg
cropped_ms/pict_12625.jpg
cropped_ms/pict_12626.jpg
cropped_ms/pict_12627.jpg
cropped_ms/pict_12628.jpg
cropped_ms/pict_12629.jpg
cropped_ms/pict_1263.jpg
cropped_ms/pict_12630.jpg
cropped_ms/pict_12631.jpg
cropped_ms/pict_12632.jpg
cropped_ms/pict_12633.jpg
cropped_ms/pict_12634.jpg
cropped_ms/pict_12635.jpg
cropped_ms/pict_12636.jpg
cropped_ms/pict_12637.jpg
cropped_ms/pict_12638.jpg
cropped_ms/pict_12639.jpg
cropped_ms/pict_1264.jpg
cropped_ms/pict_12640.jpg
cropped_ms/pict_12641.jpg
^CKilled by signal 2.
rsync error: unexplained error (code 255) at /SourceCache/rsync/rsync-45/rsync/rsync.c(244) [sender=2.6.9]
C02MX066FD58:smartCams cusgadmin$ ls
ImageNet		image_dump		processed_imgs
VOC2012			input_output		region_proposals
deep_learning		misc_caffe		segmentation
finetuning		preprocess.ipynb	train_VOC2012
C02MX066FD58:smartCams cusgadmin$ cd image_dump/
C02MX066FD58:image_dump cusgadmin$ ls
cropped			test_list.txt		train_list_ms.txt
cropped_ms		test_list_ms.txt	train_test_list.txt
label_lookup_table.pi	train_list.txt		train_test_list_ms.txt
C02MX066FD58:image_dump cusgadmin$ python
Python 2.7.9 (default, Apr  7 2015, 07:58:25) 
[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)] on darwin
Type "help", "copyright", "credits" or "license" for more information.
>>> import cv2
>>> image_1 = cv2.imread('cropped/pict_0.jpg')
>>> image_2 = cv2.imread('cropped_ms/pict_0.jpg')
>>> mean = cv2.imread('mean_image.jpg')
>>> cv2.imshow('image', mean)
>>> 
>>> cv2.imshow('image', image_1)
>>> cv2.close()
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
AttributeError: 'module' object has no attribute 'close'
>>> cv2.waitKey()

cv50
>>> 
>>> cv2.waitKeys()
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
AttributeError: 'module' object has no attribute 'waitKeys'
>>> cv2.waitKey()
50
>>> 
>>> cv2.imshow('image', image_2 + mean)
>>> cv2.waitKeys()
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
AttributeError: 'module' object has no attribute 'waitKeys'
>>> cv2.waitKey()
46
>>> 
>>> cv2.waitKey(100)
-1
>>> 
C02MX066FD58:image_dump cusgadmin$ ls
cropped			label_lookup_table.pi	train_test_list.txt
C02MX066FD58:image_dump cusgadmin$ cd ..
C02MX066FD58:smartCams cusgadmin$ cd finetuning/
C02MX066FD58:finetuning cusgadmin$ ls
image_dump	rcc_net		test_list.txt	train_list.txt
C02MX066FD58:finetuning cusgadmin$ cd rcc_net/
C02MX066FD58:rcc_net cusgadmin$ caffe train -solver solver.prototxt -weights bvlc_reference_rcnn_ilsvrc13.caffemodel 
I0430 19:24:31.504195 2050462464 caffe.cpp:117] Use CPU.
I0430 19:24:31.506672 2050462464 caffe.cpp:121] Starting Optimization
I0430 19:24:31.506950 2050462464 solver.cpp:32] Initializing solver from parameters: 
test_iter: 1000
test_interval: 1000
base_lr: 0.001
display: 20
max_iter: 10000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 100000
snapshot: 500
snapshot_prefix: "caffenet_train"
solver_mode: CPU
net: "train_val.prototxt"
I0430 19:24:31.507076 2050462464 solver.cpp:70] Creating training net from net file: train_val.prototxt
I0430 19:24:31.507621 2050462464 net.cpp:257] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0430 19:24:31.507649 2050462464 net.cpp:257] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0430 19:24:31.507658 2050462464 net.cpp:42] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "ImageData"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  image_data_param {
    source: "../train_list.txt"
    batch_size: 64
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8_VOC"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8_VOC"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  param {
    lr_mult: 20
    decay_mult: 0
  }
  inner_product_param {
    num_output: 5
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_VOC"
  bottom: "label"
  top: "loss"
}
I0430 19:24:31.508076 2050462464 layer_factory.hpp:74] Creating layer data
I0430 19:24:31.508102 2050462464 net.cpp:84] Creating Layer data
I0430 19:24:31.508111 2050462464 net.cpp:338] data -> data
I0430 19:24:31.508138 2050462464 net.cpp:338] data -> label
I0430 19:24:31.508147 2050462464 net.cpp:113] Setting up data
I0430 19:24:31.508162 2050462464 image_data_layer.cpp:36] Opening file ../train_list.txt
I0430 19:24:31.539923 2050462464 image_data_layer.cpp:51] A total of 19967 images.
I0430 19:24:31.542049 2050462464 image_data_layer.cpp:80] output data size: 64,3,227,227
I0430 19:24:31.570456 2050462464 net.cpp:120] Top shape: 64 3 227 227 (9893568)
I0430 19:24:31.570508 2050462464 net.cpp:120] Top shape: 64 (64)
I0430 19:24:31.570533 2050462464 layer_factory.hpp:74] Creating layer conv1
I0430 19:24:31.570551 2050462464 net.cpp:84] Creating Layer conv1
I0430 19:24:31.570561 2050462464 net.cpp:380] conv1 <- data
I0430 19:24:31.570575 2050462464 net.cpp:338] conv1 -> conv1
I0430 19:24:31.570654 2050462464 net.cpp:113] Setting up conv1
I0430 19:24:31.571465 2050462464 net.cpp:120] Top shape: 64 96 55 55 (18585600)
I0430 19:24:31.571488 2050462464 layer_factory.hpp:74] Creating layer relu1
I0430 19:24:31.571509 2050462464 net.cpp:84] Creating Layer relu1
I0430 19:24:31.571519 2050462464 net.cpp:380] relu1 <- conv1
I0430 19:24:31.571530 2050462464 net.cpp:327] relu1 -> conv1 (in-place)
I0430 19:24:31.571542 2050462464 net.cpp:113] Setting up relu1
I0430 19:24:31.571552 2050462464 net.cpp:120] Top shape: 64 96 55 55 (18585600)
I0430 19:24:31.571563 2050462464 layer_factory.hpp:74] Creating layer pool1
I0430 19:24:31.571576 2050462464 net.cpp:84] Creating Layer pool1
I0430 19:24:31.571584 2050462464 net.cpp:380] pool1 <- conv1
I0430 19:24:31.571596 2050462464 net.cpp:338] pool1 -> pool1
I0430 19:24:31.571615 2050462464 net.cpp:113] Setting up pool1
I0430 19:24:31.571643 2050462464 net.cpp:120] Top shape: 64 96 27 27 (4478976)
I0430 19:24:31.571656 2050462464 layer_factory.hpp:74] Creating layer norm1
I0430 19:24:31.571674 2050462464 net.cpp:84] Creating Layer norm1
I0430 19:24:31.571683 2050462464 net.cpp:380] norm1 <- pool1
I0430 19:24:31.571696 2050462464 net.cpp:338] norm1 -> norm1
I0430 19:24:31.571708 2050462464 net.cpp:113] Setting up norm1
I0430 19:24:31.571727 2050462464 net.cpp:120] Top shape: 64 96 27 27 (4478976)
I0430 19:24:31.571739 2050462464 layer_factory.hpp:74] Creating layer conv2
I0430 19:24:31.571753 2050462464 net.cpp:84] Creating Layer conv2
I0430 19:24:31.571797 2050462464 net.cpp:380] conv2 <- norm1
I0430 19:24:31.571812 2050462464 net.cpp:338] conv2 -> conv2
I0430 19:24:31.571831 2050462464 net.cpp:113] Setting up conv2
I0430 19:24:31.578548 2050462464 net.cpp:120] Top shape: 64 256 27 27 (11943936)
I0430 19:24:31.578577 2050462464 layer_factory.hpp:74] Creating layer relu2
I0430 19:24:31.578588 2050462464 net.cpp:84] Creating Layer relu2
I0430 19:24:31.578594 2050462464 net.cpp:380] relu2 <- conv2
I0430 19:24:31.578601 2050462464 net.cpp:327] relu2 -> conv2 (in-place)
I0430 19:24:31.578609 2050462464 net.cpp:113] Setting up relu2
I0430 19:24:31.578614 2050462464 net.cpp:120] Top shape: 64 256 27 27 (11943936)
I0430 19:24:31.578621 2050462464 layer_factory.hpp:74] Creating layer pool2
I0430 19:24:31.578629 2050462464 net.cpp:84] Creating Layer pool2
I0430 19:24:31.578634 2050462464 net.cpp:380] pool2 <- conv2
I0430 19:24:31.578640 2050462464 net.cpp:338] pool2 -> pool2
I0430 19:24:31.578647 2050462464 net.cpp:113] Setting up pool2
I0430 19:24:31.578655 2050462464 net.cpp:120] Top shape: 64 256 13 13 (2768896)
I0430 19:24:31.578660 2050462464 layer_factory.hpp:74] Creating layer norm2
I0430 19:24:31.578671 2050462464 net.cpp:84] Creating Layer norm2
I0430 19:24:31.578676 2050462464 net.cpp:380] norm2 <- pool2
I0430 19:24:31.578683 2050462464 net.cpp:338] norm2 -> norm2
I0430 19:24:31.578690 2050462464 net.cpp:113] Setting up norm2
I0430 19:24:31.578696 2050462464 net.cpp:120] Top shape: 64 256 13 13 (2768896)
I0430 19:24:31.578702 2050462464 layer_factory.hpp:74] Creating layer conv3
I0430 19:24:31.578711 2050462464 net.cpp:84] Creating Layer conv3
I0430 19:24:31.578716 2050462464 net.cpp:380] conv3 <- norm2
I0430 19:24:31.578722 2050462464 net.cpp:338] conv3 -> conv3
I0430 19:24:31.578734 2050462464 net.cpp:113] Setting up conv3
I0430 19:24:31.599406 2050462464 net.cpp:120] Top shape: 64 384 13 13 (4153344)
I0430 19:24:31.599436 2050462464 layer_factory.hpp:74] Creating layer relu3
I0430 19:24:31.599445 2050462464 net.cpp:84] Creating Layer relu3
I0430 19:24:31.599450 2050462464 net.cpp:380] relu3 <- conv3
I0430 19:24:31.599457 2050462464 net.cpp:327] relu3 -> conv3 (in-place)
I0430 19:24:31.599464 2050462464 net.cpp:113] Setting up relu3
I0430 19:24:31.599469 2050462464 net.cpp:120] Top shape: 64 384 13 13 (4153344)
I0430 19:24:31.599475 2050462464 layer_factory.hpp:74] Creating layer conv4
I0430 19:24:31.599493 2050462464 net.cpp:84] Creating Layer conv4
I0430 19:24:31.599498 2050462464 net.cpp:380] conv4 <- conv3
I0430 19:24:31.599505 2050462464 net.cpp:338] conv4 -> conv4
I0430 19:24:31.599522 2050462464 net.cpp:113] Setting up conv4
I0430 19:24:31.613883 2050462464 net.cpp:120] Top shape: 64 384 13 13 (4153344)
I0430 19:24:31.613908 2050462464 layer_factory.hpp:74] Creating layer relu4
I0430 19:24:31.613916 2050462464 net.cpp:84] Creating Layer relu4
I0430 19:24:31.613922 2050462464 net.cpp:380] relu4 <- conv4
I0430 19:24:31.613929 2050462464 net.cpp:327] relu4 -> conv4 (in-place)
I0430 19:24:31.613936 2050462464 net.cpp:113] Setting up relu4
I0430 19:24:31.613942 2050462464 net.cpp:120] Top shape: 64 384 13 13 (4153344)
I0430 19:24:31.613948 2050462464 layer_factory.hpp:74] Creating layer conv5
I0430 19:24:31.613956 2050462464 net.cpp:84] Creating Layer conv5
I0430 19:24:31.613961 2050462464 net.cpp:380] conv5 <- conv4
I0430 19:24:31.613989 2050462464 net.cpp:338] conv5 -> conv5
I0430 19:24:31.613999 2050462464 net.cpp:113] Setting up conv5
I0430 19:24:31.624213 2050462464 net.cpp:120] Top shape: 64 256 13 13 (2768896)
I0430 19:24:31.624243 2050462464 layer_factory.hpp:74] Creating layer relu5
I0430 19:24:31.624253 2050462464 net.cpp:84] Creating Layer relu5
I0430 19:24:31.624259 2050462464 net.cpp:380] relu5 <- conv5
I0430 19:24:31.624265 2050462464 net.cpp:327] relu5 -> conv5 (in-place)
I0430 19:24:31.624274 2050462464 net.cpp:113] Setting up relu5
I0430 19:24:31.624279 2050462464 net.cpp:120] Top shape: 64 256 13 13 (2768896)
I0430 19:24:31.624285 2050462464 layer_factory.hpp:74] Creating layer pool5
I0430 19:24:31.624316 2050462464 net.cpp:84] Creating Layer pool5
I0430 19:24:31.624321 2050462464 net.cpp:380] pool5 <- conv5
I0430 19:24:31.624328 2050462464 net.cpp:338] pool5 -> pool5
I0430 19:24:31.624336 2050462464 net.cpp:113] Setting up pool5
I0430 19:24:31.624344 2050462464 net.cpp:120] Top shape: 64 256 6 6 (589824)
I0430 19:24:31.624351 2050462464 layer_factory.hpp:74] Creating layer fc6
I0430 19:24:31.624361 2050462464 net.cpp:84] Creating Layer fc6
I0430 19:24:31.624366 2050462464 net.cpp:380] fc6 <- pool5
I0430 19:24:31.624373 2050462464 net.cpp:338] fc6 -> fc6
I0430 19:24:31.624382 2050462464 net.cpp:113] Setting up fc6
I0430 19:24:32.456959 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0430 19:24:32.456997 2050462464 layer_factory.hpp:74] Creating layer relu6
I0430 19:24:32.457015 2050462464 net.cpp:84] Creating Layer relu6
I0430 19:24:32.457022 2050462464 net.cpp:380] relu6 <- fc6
I0430 19:24:32.457033 2050462464 net.cpp:327] relu6 -> fc6 (in-place)
I0430 19:24:32.457046 2050462464 net.cpp:113] Setting up relu6
I0430 19:24:32.457056 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0430 19:24:32.457063 2050462464 layer_factory.hpp:74] Creating layer drop6
I0430 19:24:32.457074 2050462464 net.cpp:84] Creating Layer drop6
I0430 19:24:32.457082 2050462464 net.cpp:380] drop6 <- fc6
I0430 19:24:32.457093 2050462464 net.cpp:327] drop6 -> fc6 (in-place)
I0430 19:24:32.457103 2050462464 net.cpp:113] Setting up drop6
I0430 19:24:32.457120 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0430 19:24:32.457129 2050462464 layer_factory.hpp:74] Creating layer fc7
I0430 19:24:32.457139 2050462464 net.cpp:84] Creating Layer fc7
I0430 19:24:32.457144 2050462464 net.cpp:380] fc7 <- fc6
I0430 19:24:32.457150 2050462464 net.cpp:338] fc7 -> fc7
I0430 19:24:32.457159 2050462464 net.cpp:113] Setting up fc7
I0430 19:24:32.888252 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0430 19:24:32.888298 2050462464 layer_factory.hpp:74] Creating layer relu7
I0430 19:24:32.888317 2050462464 net.cpp:84] Creating Layer relu7
I0430 19:24:32.888325 2050462464 net.cpp:380] relu7 <- fc7
I0430 19:24:32.888336 2050462464 net.cpp:327] relu7 -> fc7 (in-place)
I0430 19:24:32.888347 2050462464 net.cpp:113] Setting up relu7
I0430 19:24:32.888355 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0430 19:24:32.888363 2050462464 layer_factory.hpp:74] Creating layer drop7
I0430 19:24:32.888373 2050462464 net.cpp:84] Creating Layer drop7
I0430 19:24:32.888383 2050462464 net.cpp:380] drop7 <- fc7
I0430 19:24:32.888392 2050462464 net.cpp:327] drop7 -> fc7 (in-place)
I0430 19:24:32.888402 2050462464 net.cpp:113] Setting up drop7
I0430 19:24:32.888411 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0430 19:24:32.888418 2050462464 layer_factory.hpp:74] Creating layer fc8_VOC
I0430 19:24:32.888430 2050462464 net.cpp:84] Creating Layer fc8_VOC
I0430 19:24:32.888437 2050462464 net.cpp:380] fc8_VOC <- fc7
I0430 19:24:32.888447 2050462464 net.cpp:338] fc8_VOC -> fc8_VOC
I0430 19:24:32.888458 2050462464 net.cpp:113] Setting up fc8_VOC
I0430 19:24:32.888885 2050462464 net.cpp:120] Top shape: 64 5 (320)
I0430 19:24:32.888902 2050462464 layer_factory.hpp:74] Creating layer loss
I0430 19:24:32.888917 2050462464 net.cpp:84] Creating Layer loss
I0430 19:24:32.888926 2050462464 net.cpp:380] loss <- fc8_VOC
I0430 19:24:32.888932 2050462464 net.cpp:380] loss <- label
I0430 19:24:32.888944 2050462464 net.cpp:338] loss -> loss
I0430 19:24:32.888957 2050462464 net.cpp:113] Setting up loss
I0430 19:24:32.888967 2050462464 layer_factory.hpp:74] Creating layer loss
I0430 19:24:32.888985 2050462464 net.cpp:120] Top shape: (1)
I0430 19:24:32.888994 2050462464 net.cpp:122]     with loss weight 1
I0430 19:24:32.889014 2050462464 net.cpp:167] loss needs backward computation.
I0430 19:24:32.889022 2050462464 net.cpp:167] fc8_VOC needs backward computation.
I0430 19:24:32.889030 2050462464 net.cpp:167] drop7 needs backward computation.
I0430 19:24:32.889034 2050462464 net.cpp:167] relu7 needs backward computation.
I0430 19:24:32.889039 2050462464 net.cpp:167] fc7 needs backward computation.
I0430 19:24:32.889117 2050462464 net.cpp:167] drop6 needs backward computation.
I0430 19:24:32.889129 2050462464 net.cpp:167] relu6 needs backward computation.
I0430 19:24:32.889137 2050462464 net.cpp:167] fc6 needs backward computation.
I0430 19:24:32.889144 2050462464 net.cpp:167] pool5 needs backward computation.
I0430 19:24:32.889152 2050462464 net.cpp:167] relu5 needs backward computation.
I0430 19:24:32.889158 2050462464 net.cpp:167] conv5 needs backward computation.
I0430 19:24:32.889164 2050462464 net.cpp:167] relu4 needs backward computation.
I0430 19:24:32.889173 2050462464 net.cpp:167] conv4 needs backward computation.
I0430 19:24:32.889181 2050462464 net.cpp:167] relu3 needs backward computation.
I0430 19:24:32.889188 2050462464 net.cpp:167] conv3 needs backward computation.
I0430 19:24:32.889195 2050462464 net.cpp:167] norm2 needs backward computation.
I0430 19:24:32.889202 2050462464 net.cpp:167] pool2 needs backward computation.
I0430 19:24:32.889209 2050462464 net.cpp:167] relu2 needs backward computation.
I0430 19:24:32.889214 2050462464 net.cpp:167] conv2 needs backward computation.
I0430 19:24:32.889221 2050462464 net.cpp:167] norm1 needs backward computation.
I0430 19:24:32.889228 2050462464 net.cpp:167] pool1 needs backward computation.
I0430 19:24:32.889235 2050462464 net.cpp:167] relu1 needs backward computation.
I0430 19:24:32.889241 2050462464 net.cpp:167] conv1 needs backward computation.
I0430 19:24:32.889248 2050462464 net.cpp:169] data does not need backward computation.
I0430 19:24:32.889255 2050462464 net.cpp:205] This network produces output loss
I0430 19:24:32.889279 2050462464 net.cpp:447] Collecting Learning Rate and Weight Decay.
I0430 19:24:32.889292 2050462464 net.cpp:217] Network initialization done.
I0430 19:24:32.889298 2050462464 net.cpp:218] Memory required for data: 439050500
I0430 19:24:32.889757 2050462464 solver.cpp:154] Creating test net (#0) specified by net file: train_val.prototxt
I0430 19:24:32.889832 2050462464 net.cpp:257] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0430 19:24:32.889866 2050462464 net.cpp:42] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "ImageData"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  image_data_param {
    source: "../test_list.txt"
    batch_size: 64
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8_VOC"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8_VOC"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  param {
    lr_mult: 20
    decay_mult: 0
  }
  inner_product_param {
    num_output: 5
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_VOC"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_VOC"
  bottom: "label"
  top: "loss"
}
I0430 19:24:32.890607 2050462464 layer_factory.hpp:74] Creating layer data
I0430 19:24:32.890625 2050462464 net.cpp:84] Creating Layer data
I0430 19:24:32.890633 2050462464 net.cpp:338] data -> data
I0430 19:24:32.890645 2050462464 net.cpp:338] data -> label
I0430 19:24:32.890651 2050462464 net.cpp:113] Setting up data
I0430 19:24:32.890658 2050462464 image_data_layer.cpp:36] Opening file ../test_list.txt
I0430 19:24:32.893170 2050462464 image_data_layer.cpp:51] A total of 2249 images.
I0430 19:24:32.894289 2050462464 image_data_layer.cpp:80] output data size: 64,3,227,227
I0430 19:24:32.925014 2050462464 net.cpp:120] Top shape: 64 3 227 227 (9893568)
I0430 19:24:32.925055 2050462464 net.cpp:120] Top shape: 64 (64)
I0430 19:24:32.925073 2050462464 layer_factory.hpp:74] Creating layer label_data_1_split
I0430 19:24:32.925096 2050462464 net.cpp:84] Creating Layer label_data_1_split
I0430 19:24:32.925106 2050462464 net.cpp:380] label_data_1_split <- label
I0430 19:24:32.925163 2050462464 net.cpp:338] label_data_1_split -> label_data_1_split_0
I0430 19:24:32.925185 2050462464 net.cpp:338] label_data_1_split -> label_data_1_split_1
I0430 19:24:32.925199 2050462464 net.cpp:113] Setting up label_data_1_split
I0430 19:24:32.925211 2050462464 net.cpp:120] Top shape: 64 (64)
I0430 19:24:32.925222 2050462464 net.cpp:120] Top shape: 64 (64)
I0430 19:24:32.925232 2050462464 layer_factory.hpp:74] Creating layer conv1
I0430 19:24:32.925251 2050462464 net.cpp:84] Creating Layer conv1
I0430 19:24:32.925269 2050462464 net.cpp:380] conv1 <- data
I0430 19:24:32.925284 2050462464 net.cpp:338] conv1 -> conv1
I0430 19:24:32.925302 2050462464 net.cpp:113] Setting up conv1
I0430 19:24:32.926206 2050462464 net.cpp:120] Top shape: 64 96 55 55 (18585600)
I0430 19:24:32.926246 2050462464 layer_factory.hpp:74] Creating layer relu1
I0430 19:24:32.926262 2050462464 net.cpp:84] Creating Layer relu1
I0430 19:24:32.926272 2050462464 net.cpp:380] relu1 <- conv1
I0430 19:24:32.926285 2050462464 net.cpp:327] relu1 -> conv1 (in-place)
I0430 19:24:32.926297 2050462464 net.cpp:113] Setting up relu1
I0430 19:24:32.926307 2050462464 net.cpp:120] Top shape: 64 96 55 55 (18585600)
I0430 19:24:32.926319 2050462464 layer_factory.hpp:74] Creating layer pool1
I0430 19:24:32.926343 2050462464 net.cpp:84] Creating Layer pool1
I0430 19:24:32.926353 2050462464 net.cpp:380] pool1 <- conv1
I0430 19:24:32.926365 2050462464 net.cpp:338] pool1 -> pool1
I0430 19:24:32.926379 2050462464 net.cpp:113] Setting up pool1
I0430 19:24:32.926393 2050462464 net.cpp:120] Top shape: 64 96 27 27 (4478976)
I0430 19:24:32.926406 2050462464 layer_factory.hpp:74] Creating layer norm1
I0430 19:24:32.926419 2050462464 net.cpp:84] Creating Layer norm1
I0430 19:24:32.926429 2050462464 net.cpp:380] norm1 <- pool1
I0430 19:24:32.926440 2050462464 net.cpp:338] norm1 -> norm1
I0430 19:24:32.926452 2050462464 net.cpp:113] Setting up norm1
I0430 19:24:32.926465 2050462464 net.cpp:120] Top shape: 64 96 27 27 (4478976)
I0430 19:24:32.926476 2050462464 layer_factory.hpp:74] Creating layer conv2
I0430 19:24:32.926491 2050462464 net.cpp:84] Creating Layer conv2
I0430 19:24:32.926499 2050462464 net.cpp:380] conv2 <- norm1
I0430 19:24:32.926512 2050462464 net.cpp:338] conv2 -> conv2
I0430 19:24:32.926525 2050462464 net.cpp:113] Setting up conv2
I0430 19:24:32.936224 2050462464 net.cpp:120] Top shape: 64 256 27 27 (11943936)
I0430 19:24:32.936300 2050462464 layer_factory.hpp:74] Creating layer relu2
I0430 19:24:32.936316 2050462464 net.cpp:84] Creating Layer relu2
I0430 19:24:32.936326 2050462464 net.cpp:380] relu2 <- conv2
I0430 19:24:32.936338 2050462464 net.cpp:327] relu2 -> conv2 (in-place)
I0430 19:24:32.936352 2050462464 net.cpp:113] Setting up relu2
I0430 19:24:32.936360 2050462464 net.cpp:120] Top shape: 64 256 27 27 (11943936)
I0430 19:24:32.936373 2050462464 layer_factory.hpp:74] Creating layer pool2
I0430 19:24:32.936388 2050462464 net.cpp:84] Creating Layer pool2
I0430 19:24:32.936396 2050462464 net.cpp:380] pool2 <- conv2
I0430 19:24:32.936408 2050462464 net.cpp:338] pool2 -> pool2
I0430 19:24:32.936429 2050462464 net.cpp:113] Setting up pool2
I0430 19:24:32.936444 2050462464 net.cpp:120] Top shape: 64 256 13 13 (2768896)
I0430 19:24:32.936456 2050462464 layer_factory.hpp:74] Creating layer norm2
I0430 19:24:32.936467 2050462464 net.cpp:84] Creating Layer norm2
I0430 19:24:32.936477 2050462464 net.cpp:380] norm2 <- pool2
I0430 19:24:32.936489 2050462464 net.cpp:338] norm2 -> norm2
I0430 19:24:32.936501 2050462464 net.cpp:113] Setting up norm2
I0430 19:24:32.936513 2050462464 net.cpp:120] Top shape: 64 256 13 13 (2768896)
I0430 19:24:32.936524 2050462464 layer_factory.hpp:74] Creating layer conv3
I0430 19:24:32.936538 2050462464 net.cpp:84] Creating Layer conv3
I0430 19:24:32.936547 2050462464 net.cpp:380] conv3 <- norm2
I0430 19:24:32.936558 2050462464 net.cpp:338] conv3 -> conv3
I0430 19:24:32.936573 2050462464 net.cpp:113] Setting up conv3
I0430 19:24:32.961535 2050462464 net.cpp:120] Top shape: 64 384 13 13 (4153344)
I0430 19:24:32.961601 2050462464 layer_factory.hpp:74] Creating layer relu3
I0430 19:24:32.961617 2050462464 net.cpp:84] Creating Layer relu3
I0430 19:24:32.961627 2050462464 net.cpp:380] relu3 <- conv3
I0430 19:24:32.961638 2050462464 net.cpp:327] relu3 -> conv3 (in-place)
I0430 19:24:32.961650 2050462464 net.cpp:113] Setting up relu3
I0430 19:24:32.961660 2050462464 net.cpp:120] Top shape: 64 384 13 13 (4153344)
I0430 19:24:32.961671 2050462464 layer_factory.hpp:74] Creating layer conv4
I0430 19:24:32.961686 2050462464 net.cpp:84] Creating Layer conv4
I0430 19:24:32.961695 2050462464 net.cpp:380] conv4 <- conv3
I0430 19:24:32.961706 2050462464 net.cpp:338] conv4 -> conv4
I0430 19:24:32.961720 2050462464 net.cpp:113] Setting up conv4
I0430 19:24:32.981053 2050462464 net.cpp:120] Top shape: 64 384 13 13 (4153344)
I0430 19:24:32.981092 2050462464 layer_factory.hpp:74] Creating layer relu4
I0430 19:24:32.981111 2050462464 net.cpp:84] Creating Layer relu4
I0430 19:24:32.981122 2050462464 net.cpp:380] relu4 <- conv4
I0430 19:24:32.981135 2050462464 net.cpp:327] relu4 -> conv4 (in-place)
I0430 19:24:32.981148 2050462464 net.cpp:113] Setting up relu4
I0430 19:24:32.981158 2050462464 net.cpp:120] Top shape: 64 384 13 13 (4153344)
I0430 19:24:32.981170 2050462464 layer_factory.hpp:74] Creating layer conv5
I0430 19:24:32.981185 2050462464 net.cpp:84] Creating Layer conv5
I0430 19:24:32.981195 2050462464 net.cpp:380] conv5 <- conv4
I0430 19:24:32.981210 2050462464 net.cpp:338] conv5 -> conv5
I0430 19:24:32.981241 2050462464 net.cpp:113] Setting up conv5
I0430 19:24:32.994616 2050462464 net.cpp:120] Top shape: 64 256 13 13 (2768896)
I0430 19:24:32.994647 2050462464 layer_factory.hpp:74] Creating layer relu5
I0430 19:24:32.994664 2050462464 net.cpp:84] Creating Layer relu5
I0430 19:24:32.994675 2050462464 net.cpp:380] relu5 <- conv5
I0430 19:24:32.994688 2050462464 net.cpp:327] relu5 -> conv5 (in-place)
I0430 19:24:32.995899 2050462464 net.cpp:113] Setting up relu5
I0430 19:24:32.995928 2050462464 net.cpp:120] Top shape: 64 256 13 13 (2768896)
I0430 19:24:32.995942 2050462464 layer_factory.hpp:74] Creating layer pool5
I0430 19:24:32.995961 2050462464 net.cpp:84] Creating Layer pool5
I0430 19:24:32.995972 2050462464 net.cpp:380] pool5 <- conv5
I0430 19:24:32.995985 2050462464 net.cpp:338] pool5 -> pool5
I0430 19:24:32.996000 2050462464 net.cpp:113] Setting up pool5
I0430 19:24:32.996014 2050462464 net.cpp:120] Top shape: 64 256 6 6 (589824)
I0430 19:24:32.996026 2050462464 layer_factory.hpp:74] Creating layer fc6
I0430 19:24:32.996042 2050462464 net.cpp:84] Creating Layer fc6
I0430 19:24:32.996050 2050462464 net.cpp:380] fc6 <- pool5
I0430 19:24:32.996062 2050462464 net.cpp:338] fc6 -> fc6
I0430 19:24:32.996078 2050462464 net.cpp:113] Setting up fc6
I0430 19:24:33.892565 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0430 19:24:33.892601 2050462464 layer_factory.hpp:74] Creating layer relu6
I0430 19:24:33.892613 2050462464 net.cpp:84] Creating Layer relu6
I0430 19:24:33.892619 2050462464 net.cpp:380] relu6 <- fc6
I0430 19:24:33.892627 2050462464 net.cpp:327] relu6 -> fc6 (in-place)
I0430 19:24:33.892635 2050462464 net.cpp:113] Setting up relu6
I0430 19:24:33.892640 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0430 19:24:33.892647 2050462464 layer_factory.hpp:74] Creating layer drop6
I0430 19:24:33.892654 2050462464 net.cpp:84] Creating Layer drop6
I0430 19:24:33.892658 2050462464 net.cpp:380] drop6 <- fc6
I0430 19:24:33.892665 2050462464 net.cpp:327] drop6 -> fc6 (in-place)
I0430 19:24:33.892673 2050462464 net.cpp:113] Setting up drop6
I0430 19:24:33.892681 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0430 19:24:33.892688 2050462464 layer_factory.hpp:74] Creating layer fc7
I0430 19:24:33.892698 2050462464 net.cpp:84] Creating Layer fc7
I0430 19:24:33.892702 2050462464 net.cpp:380] fc7 <- fc6
I0430 19:24:33.892709 2050462464 net.cpp:338] fc7 -> fc7
I0430 19:24:33.892717 2050462464 net.cpp:113] Setting up fc7
I0430 19:24:34.236332 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0430 19:24:34.236377 2050462464 layer_factory.hpp:74] Creating layer relu7
I0430 19:24:34.236438 2050462464 net.cpp:84] Creating Layer relu7
I0430 19:24:34.236451 2050462464 net.cpp:380] relu7 <- fc7
I0430 19:24:34.236464 2050462464 net.cpp:327] relu7 -> fc7 (in-place)
I0430 19:24:34.236477 2050462464 net.cpp:113] Setting up relu7
I0430 19:24:34.236486 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0430 19:24:34.236497 2050462464 layer_factory.hpp:74] Creating layer drop7
I0430 19:24:34.236510 2050462464 net.cpp:84] Creating Layer drop7
I0430 19:24:34.236517 2050462464 net.cpp:380] drop7 <- fc7
I0430 19:24:34.236527 2050462464 net.cpp:327] drop7 -> fc7 (in-place)
I0430 19:24:34.236539 2050462464 net.cpp:113] Setting up drop7
I0430 19:24:34.236551 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0430 19:24:34.236560 2050462464 layer_factory.hpp:74] Creating layer fc8_VOC
I0430 19:24:34.236575 2050462464 net.cpp:84] Creating Layer fc8_VOC
I0430 19:24:34.236583 2050462464 net.cpp:380] fc8_VOC <- fc7
I0430 19:24:34.236596 2050462464 net.cpp:338] fc8_VOC -> fc8_VOC
I0430 19:24:34.236613 2050462464 net.cpp:113] Setting up fc8_VOC
I0430 19:24:34.237076 2050462464 net.cpp:120] Top shape: 64 5 (320)
I0430 19:24:34.237102 2050462464 layer_factory.hpp:74] Creating layer fc8_VOC_fc8_VOC_0_split
I0430 19:24:34.237115 2050462464 net.cpp:84] Creating Layer fc8_VOC_fc8_VOC_0_split
I0430 19:24:34.237124 2050462464 net.cpp:380] fc8_VOC_fc8_VOC_0_split <- fc8_VOC
I0430 19:24:34.237136 2050462464 net.cpp:338] fc8_VOC_fc8_VOC_0_split -> fc8_VOC_fc8_VOC_0_split_0
I0430 19:24:34.237151 2050462464 net.cpp:338] fc8_VOC_fc8_VOC_0_split -> fc8_VOC_fc8_VOC_0_split_1
I0430 19:24:34.237165 2050462464 net.cpp:113] Setting up fc8_VOC_fc8_VOC_0_split
I0430 19:24:34.237175 2050462464 net.cpp:120] Top shape: 64 5 (320)
I0430 19:24:34.237185 2050462464 net.cpp:120] Top shape: 64 5 (320)
I0430 19:24:34.237193 2050462464 layer_factory.hpp:74] Creating layer accuracy
I0430 19:24:34.237211 2050462464 net.cpp:84] Creating Layer accuracy
I0430 19:24:34.237220 2050462464 net.cpp:380] accuracy <- fc8_VOC_fc8_VOC_0_split_0
I0430 19:24:34.237229 2050462464 net.cpp:380] accuracy <- label_data_1_split_0
I0430 19:24:34.237241 2050462464 net.cpp:338] accuracy -> accuracy
I0430 19:24:34.237252 2050462464 net.cpp:113] Setting up accuracy
I0430 19:24:34.237262 2050462464 net.cpp:120] Top shape: (1)
I0430 19:24:34.237270 2050462464 layer_factory.hpp:74] Creating layer loss
I0430 19:24:34.237282 2050462464 net.cpp:84] Creating Layer loss
I0430 19:24:34.237289 2050462464 net.cpp:380] loss <- fc8_VOC_fc8_VOC_0_split_1
I0430 19:24:34.237298 2050462464 net.cpp:380] loss <- label_data_1_split_1
I0430 19:24:34.237308 2050462464 net.cpp:338] loss -> loss
I0430 19:24:34.237318 2050462464 net.cpp:113] Setting up loss
I0430 19:24:34.237329 2050462464 layer_factory.hpp:74] Creating layer loss
I0430 19:24:34.237347 2050462464 net.cpp:120] Top shape: (1)
I0430 19:24:34.237355 2050462464 net.cpp:122]     with loss weight 1
I0430 19:24:34.237367 2050462464 net.cpp:167] loss needs backward computation.
I0430 19:24:34.237375 2050462464 net.cpp:169] accuracy does not need backward computation.
I0430 19:24:34.237383 2050462464 net.cpp:167] fc8_VOC_fc8_VOC_0_split needs backward computation.
I0430 19:24:34.237390 2050462464 net.cpp:167] fc8_VOC needs backward computation.
I0430 19:24:34.237398 2050462464 net.cpp:167] drop7 needs backward computation.
I0430 19:24:34.237406 2050462464 net.cpp:167] relu7 needs backward computation.
I0430 19:24:34.237412 2050462464 net.cpp:167] fc7 needs backward computation.
I0430 19:24:34.237421 2050462464 net.cpp:167] drop6 needs backward computation.
I0430 19:24:34.237427 2050462464 net.cpp:167] relu6 needs backward computation.
I0430 19:24:34.237435 2050462464 net.cpp:167] fc6 needs backward computation.
I0430 19:24:34.237442 2050462464 net.cpp:167] pool5 needs backward computation.
I0430 19:24:34.237452 2050462464 net.cpp:167] relu5 needs backward computation.
I0430 19:24:34.237458 2050462464 net.cpp:167] conv5 needs backward computation.
I0430 19:24:34.237467 2050462464 net.cpp:167] relu4 needs backward computation.
I0430 19:24:34.237503 2050462464 net.cpp:167] conv4 needs backward computation.
I0430 19:24:34.237511 2050462464 net.cpp:167] relu3 needs backward computation.
I0430 19:24:34.237519 2050462464 net.cpp:167] conv3 needs backward computation.
I0430 19:24:34.237527 2050462464 net.cpp:167] norm2 needs backward computation.
I0430 19:24:34.237534 2050462464 net.cpp:167] pool2 needs backward computation.
I0430 19:24:34.237542 2050462464 net.cpp:167] relu2 needs backward computation.
I0430 19:24:34.237550 2050462464 net.cpp:167] conv2 needs backward computation.
I0430 19:24:34.237557 2050462464 net.cpp:167] norm1 needs backward computation.
I0430 19:24:34.237565 2050462464 net.cpp:167] pool1 needs backward computation.
I0430 19:24:34.237572 2050462464 net.cpp:167] relu1 needs backward computation.
I0430 19:24:34.237579 2050462464 net.cpp:167] conv1 needs backward computation.
I0430 19:24:34.237587 2050462464 net.cpp:169] label_data_1_split does not need backward computation.
I0430 19:24:34.237596 2050462464 net.cpp:169] data does not need backward computation.
I0430 19:24:34.237639 2050462464 net.cpp:205] This network produces output accuracy
I0430 19:24:34.237658 2050462464 net.cpp:205] This network produces output loss
I0430 19:24:34.237682 2050462464 net.cpp:447] Collecting Learning Rate and Weight Decay.
I0430 19:24:34.237696 2050462464 net.cpp:217] Network initialization done.
I0430 19:24:34.237704 2050462464 net.cpp:218] Memory required for data: 439053576
I0430 19:24:34.237947 2050462464 solver.cpp:42] Solver scaffolding done.
I0430 19:24:34.238037 2050462464 caffe.cpp:86] Finetuning from bvlc_reference_rcnn_ilsvrc13.caffemodel
E0430 19:24:35.048671 2050462464 upgrade_proto.cpp:618] Attempting to upgrade input file specified using deprecated V1LayerParameter: bvlc_reference_rcnn_ilsvrc13.caffemodel
I0430 19:24:35.559860 2050462464 upgrade_proto.cpp:626] Successfully upgraded file specified using deprecated V1LayerParameter
E0430 19:24:36.289784 2050462464 upgrade_proto.cpp:618] Attempting to upgrade input file specified using deprecated V1LayerParameter: bvlc_reference_rcnn_ilsvrc13.caffemodel
I0430 19:24:36.608316 2050462464 upgrade_proto.cpp:626] Successfully upgraded file specified using deprecated V1LayerParameter
I0430 19:24:36.728780 2050462464 solver.cpp:222] Solving CaffeNet
I0430 19:24:36.728819 2050462464 solver.cpp:223] Learning Rate Policy: step
I0430 19:24:36.728837 2050462464 solver.cpp:266] Iteration 0, Testing net (#0)
lspci -v | lessI0430 21:52:57.538521 2050462464 solver.cpp:315]     Test net output #0: accuracy = 0.223312
I0430 21:52:57.539101 2050462464 solver.cpp:315]     Test net output #1: loss = 1.59832 (* 1 = 1.59832 loss)
I0430 21:53:05.320619 2050462464 solver.cpp:189] Iteration 0, loss = 1.65779
I0430 21:53:05.320657 2050462464 solver.cpp:204]     Train net output #0: loss = 1.65779 (* 1 = 1.65779 loss)
I0430 21:53:05.320674 2050462464 solver.cpp:464] Iteration 0, lr = 0.001
I0430 21:55:24.480161 2050462464 solver.cpp:189] Iteration 20, loss = 0.131255
I0430 21:55:24.480232 2050462464 solver.cpp:204]     Train net output #0: loss = 0.131255 (* 1 = 0.131255 loss)
I0430 21:55:24.480250 2050462464 solver.cpp:464] Iteration 20, lr = 0.001
I0430 21:57:50.540437 2050462464 solver.cpp:189] Iteration 40, loss = 0.159808
I0430 21:57:50.542279 2050462464 solver.cpp:204]     Train net output #0: loss = 0.159808 (* 1 = 0.159808 loss)
I0430 21:57:50.542301 2050462464 solver.cpp:464] Iteration 40, lr = 0.001
I0430 22:00:13.955126 2050462464 solver.cpp:189] Iteration 60, loss = 0.107206
I0430 22:00:13.955404 2050462464 solver.cpp:204]     Train net output #0: loss = 0.107206 (* 1 = 0.107206 loss)
I0430 22:00:13.955420 2050462464 solver.cpp:464] Iteration 60, lr = 0.001
I0430 22:02:37.913825 2050462464 solver.cpp:189] Iteration 80, loss = 0.297882
I0430 22:02:37.914569 2050462464 solver.cpp:204]     Train net output #0: loss = 0.297882 (* 1 = 0.297882 loss)
I0430 22:02:37.914592 2050462464 solver.cpp:464] Iteration 80, lr = 0.001
I0430 22:05:14.626265 2050462464 solver.cpp:189] Iteration 100, loss = 0.133381
I0430 22:05:14.626654 2050462464 solver.cpp:204]     Train net output #0: loss = 0.133381 (* 1 = 0.133381 loss)
I0430 22:05:14.626677 2050462464 solver.cpp:464] Iteration 100, lr = 0.001
I0430 22:07:47.036033 2050462464 solver.cpp:189] Iteration 120, loss = 0.351931
I0430 22:07:47.036813 2050462464 solver.cpp:204]     Train net output #0: loss = 0.351931 (* 1 = 0.351931 loss)
I0430 22:07:47.036828 2050462464 solver.cpp:464] Iteration 120, lr = 0.001
I0430 22:10:19.477383 2050462464 solver.cpp:189] Iteration 140, loss = 0.26142
I0430 22:10:19.477465 2050462464 solver.cpp:204]     Train net output #0: loss = 0.26142 (* 1 = 0.26142 loss)
I0430 22:10:19.477494 2050462464 solver.cpp:464] Iteration 140, lr = 0.001
I0430 22:12:49.683413 2050462464 solver.cpp:189] Iteration 160, loss = 0.309126
I0430 22:12:49.684165 2050462464 solver.cpp:204]     Train net output #0: loss = 0.309126 (* 1 = 0.309126 loss)
I0430 22:12:49.684178 2050462464 solver.cpp:464] Iteration 160, lr = 0.001
I0430 22:15:14.774538 2050462464 solver.cpp:189] Iteration 180, loss = 0.253823
I0430 22:15:14.774597 2050462464 solver.cpp:204]     Train net output #0: loss = 0.253823 (* 1 = 0.253823 loss)
I0430 22:15:14.774612 2050462464 solver.cpp:464] Iteration 180, lr = 0.001
I0430 22:17:39.905665 2050462464 solver.cpp:189] Iteration 200, loss = 0.183982
I0430 22:17:39.905715 2050462464 solver.cpp:204]     Train net output #0: loss = 0.183982 (* 1 = 0.183982 loss)
I0430 22:17:39.905725 2050462464 solver.cpp:464] Iteration 200, lr = 0.001
I0430 22:20:01.100507 2050462464 solver.cpp:189] Iteration 220, loss = 0.231203
I0430 22:20:01.100558 2050462464 solver.cpp:204]     Train net output #0: loss = 0.231203 (* 1 = 0.231203 loss)
I0430 22:20:01.100569 2050462464 solver.cpp:464] Iteration 220, lr = 0.001
I0430 22:22:22.657263 2050462464 solver.cpp:189] Iteration 240, loss = 0.0635056
I0430 22:22:22.657311 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0635057 (* 1 = 0.0635057 loss)
I0430 22:22:22.657321 2050462464 solver.cpp:464] Iteration 240, lr = 0.001
I0430 22:24:45.555094 2050462464 solver.cpp:189] Iteration 260, loss = 4.13173e-05
I0430 22:24:45.555893 2050462464 solver.cpp:204]     Train net output #0: loss = 4.13763e-05 (* 1 = 4.13763e-05 loss)
I0430 22:24:45.555918 2050462464 solver.cpp:464] Iteration 260, lr = 0.001
I0430 22:27:08.525076 2050462464 solver.cpp:189] Iteration 280, loss = 2.16352e-05
I0430 22:27:08.525859 2050462464 solver.cpp:204]     Train net output #0: loss = 2.16943e-05 (* 1 = 2.16943e-05 loss)
I0430 22:27:08.525876 2050462464 solver.cpp:464] Iteration 280, lr = 0.001
I0430 22:29:32.053184 2050462464 solver.cpp:189] Iteration 300, loss = 1.41955e-05
I0430 22:29:32.053473 2050462464 solver.cpp:204]     Train net output #0: loss = 1.42546e-05 (* 1 = 1.42546e-05 loss)
I0430 22:29:32.053494 2050462464 solver.cpp:464] Iteration 300, lr = 0.001
I0430 22:31:55.274032 2050462464 solver.cpp:189] Iteration 320, loss = 0.23258
I0430 22:31:55.274838 2050462464 solver.cpp:204]     Train net output #0: loss = 0.23258 (* 1 = 0.23258 loss)
I0430 22:31:55.274893 2050462464 solver.cpp:464] Iteration 320, lr = 0.001
I0430 22:34:14.992101 2050462464 solver.cpp:189] Iteration 340, loss = 0.075656
I0430 22:34:14.992172 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0756561 (* 1 = 0.0756561 loss)
I0430 22:34:14.992189 2050462464 solver.cpp:464] Iteration 340, lr = 0.001
I0430 22:36:31.047626 2050462464 solver.cpp:189] Iteration 360, loss = 0.093439
I0430 22:36:31.047745 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0934391 (* 1 = 0.0934391 loss)
I0430 22:36:31.047757 2050462464 solver.cpp:464] Iteration 360, lr = 0.001
I0430 22:38:44.671203 2050462464 solver.cpp:189] Iteration 380, loss = 0.157407
I0430 22:38:44.671253 2050462464 solver.cpp:204]     Train net output #0: loss = 0.157407 (* 1 = 0.157407 loss)
I0430 22:38:44.671263 2050462464 solver.cpp:464] Iteration 380, lr = 0.001
I0430 22:41:05.917366 2050462464 solver.cpp:189] Iteration 400, loss = 0.0900971
I0430 22:41:05.918078 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0900972 (* 1 = 0.0900972 loss)
I0430 22:41:05.918102 2050462464 solver.cpp:464] Iteration 400, lr = 0.001
I0430 22:43:39.815892 2050462464 solver.cpp:189] Iteration 420, loss = 0.283395
I0430 22:43:39.816185 2050462464 solver.cpp:204]     Train net output #0: loss = 0.283395 (* 1 = 0.283395 loss)
I0430 22:43:39.816200 2050462464 solver.cpp:464] Iteration 420, lr = 0.001
I0430 22:46:15.234449 2050462464 solver.cpp:189] Iteration 440, loss = 0.217111
I0430 22:46:15.234508 2050462464 solver.cpp:204]     Train net output #0: loss = 0.217111 (* 1 = 0.217111 loss)
I0430 22:46:15.234524 2050462464 solver.cpp:464] Iteration 440, lr = 0.001
I0430 22:48:56.851136 2050462464 solver.cpp:189] Iteration 460, loss = 0.0746787
I0430 22:48:56.851956 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0746787 (* 1 = 0.0746787 loss)
I0430 22:48:56.851984 2050462464 solver.cpp:464] Iteration 460, lr = 0.001
I0430 22:51:21.046813 2050462464 solver.cpp:189] Iteration 480, loss = 0.0643603
I0430 22:51:21.047139 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0643604 (* 1 = 0.0643604 loss)
I0430 22:51:21.047163 2050462464 solver.cpp:464] Iteration 480, lr = 0.001
I0430 22:53:37.407810 2050462464 solver.cpp:334] Snapshotting to caffenet_train_iter_500.caffemodel
I0430 22:53:39.361138 2050462464 solver.cpp:342] Snapshotting solver state to caffenet_train_iter_500.solverstate
I0430 22:53:48.250772 2050462464 solver.cpp:189] Iteration 500, loss = 0.241305
I0430 22:53:48.250810 2050462464 solver.cpp:204]     Train net output #0: loss = 0.241305 (* 1 = 0.241305 loss)
I0430 22:53:48.250819 2050462464 solver.cpp:464] Iteration 500, lr = 0.001
I0430 22:56:34.465498 2050462464 solver.cpp:189] Iteration 520, loss = 0.18167
I0430 22:56:34.465553 2050462464 solver.cpp:204]     Train net output #0: loss = 0.18167 (* 1 = 0.18167 loss)
I0430 22:56:34.465569 2050462464 solver.cpp:464] Iteration 520, lr = 0.001
I0430 22:59:14.457159 2050462464 solver.cpp:189] Iteration 540, loss = 0.18987
I0430 22:59:14.457234 2050462464 solver.cpp:204]     Train net output #0: loss = 0.18987 (* 1 = 0.18987 loss)
I0430 22:59:14.457253 2050462464 solver.cpp:464] Iteration 540, lr = 0.001
I0430 23:02:01.443593 2050462464 solver.cpp:189] Iteration 560, loss = 0.00898604
I0430 23:02:01.443640 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00898608 (* 1 = 0.00898608 loss)
I0430 23:02:01.443651 2050462464 solver.cpp:464] Iteration 560, lr = 0.001
I0430 23:04:26.626873 2050462464 solver.cpp:189] Iteration 580, loss = 2.99895e-05
I0430 23:04:26.626937 2050462464 solver.cpp:204]     Train net output #0: loss = 3.00329e-05 (* 1 = 3.00329e-05 loss)
I0430 23:04:26.626956 2050462464 solver.cpp:464] Iteration 580, lr = 0.001
I0430 23:06:54.154737 2050462464 solver.cpp:189] Iteration 600, loss = 1.14582e-05
I0430 23:06:54.154806 2050462464 solver.cpp:204]     Train net output #0: loss = 1.15017e-05 (* 1 = 1.15017e-05 loss)
I0430 23:06:54.154824 2050462464 solver.cpp:464] Iteration 600, lr = 0.001
I0430 23:09:41.140285 2050462464 solver.cpp:189] Iteration 620, loss = 0.00157129
I0430 23:09:41.140367 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00157134 (* 1 = 0.00157134 loss)
I0430 23:09:41.140383 2050462464 solver.cpp:464] Iteration 620, lr = 0.001
I0430 23:12:32.075311 2050462464 solver.cpp:189] Iteration 640, loss = 0.05265
I0430 23:12:32.075609 2050462464 solver.cpp:204]     Train net output #0: loss = 0.05265 (* 1 = 0.05265 loss)
I0430 23:12:32.075629 2050462464 solver.cpp:464] Iteration 640, lr = 0.001
I0430 23:15:12.291450 2050462464 solver.cpp:189] Iteration 660, loss = 0.109255
I0430 23:15:12.292474 2050462464 solver.cpp:204]     Train net output #0: loss = 0.109255 (* 1 = 0.109255 loss)
I0430 23:15:12.292495 2050462464 solver.cpp:464] Iteration 660, lr = 0.001
I0430 23:17:45.832679 2050462464 solver.cpp:189] Iteration 680, loss = 0.0642779
I0430 23:17:45.832753 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0642779 (* 1 = 0.0642779 loss)
I0430 23:17:45.832764 2050462464 solver.cpp:464] Iteration 680, lr = 0.001
I0430 23:20:19.467574 2050462464 solver.cpp:189] Iteration 700, loss = 0.262385
I0430 23:20:19.467622 2050462464 solver.cpp:204]     Train net output #0: loss = 0.262385 (* 1 = 0.262385 loss)
I0430 23:20:19.467633 2050462464 solver.cpp:464] Iteration 700, lr = 0.001
I0430 23:22:56.021414 2050462464 solver.cpp:189] Iteration 720, loss = 0.0749533
I0430 23:22:56.021705 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0749533 (* 1 = 0.0749533 loss)
I0430 23:22:56.021728 2050462464 solver.cpp:464] Iteration 720, lr = 0.001
I0430 23:25:20.770776 2050462464 solver.cpp:189] Iteration 740, loss = 0.0835169
I0430 23:25:20.770841 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0835169 (* 1 = 0.0835169 loss)
I0430 23:25:20.770861 2050462464 solver.cpp:464] Iteration 740, lr = 0.001
I0430 23:27:53.155658 2050462464 solver.cpp:189] Iteration 760, loss = 0.126894
I0430 23:27:53.155942 2050462464 solver.cpp:204]     Train net output #0: loss = 0.126894 (* 1 = 0.126894 loss)
I0430 23:27:53.155956 2050462464 solver.cpp:464] Iteration 760, lr = 0.001
I0430 23:30:07.895262 2050462464 solver.cpp:189] Iteration 780, loss = 0.11923
I0430 23:30:07.895309 2050462464 solver.cpp:204]     Train net output #0: loss = 0.11923 (* 1 = 0.11923 loss)
I0430 23:30:07.895318 2050462464 solver.cpp:464] Iteration 780, lr = 0.001
I0430 23:31:57.365604 2050462464 solver.cpp:189] Iteration 800, loss = 0.14945
I0430 23:31:57.365656 2050462464 solver.cpp:204]     Train net output #0: loss = 0.14945 (* 1 = 0.14945 loss)
I0430 23:31:57.365666 2050462464 solver.cpp:464] Iteration 800, lr = 0.001
I0430 23:33:40.557071 2050462464 solver.cpp:189] Iteration 820, loss = 0.079453
I0430 23:33:40.557817 2050462464 solver.cpp:204]     Train net output #0: loss = 0.079453 (* 1 = 0.079453 loss)
I0430 23:33:40.557829 2050462464 solver.cpp:464] Iteration 820, lr = 0.001
I0430 23:35:18.883687 2050462464 solver.cpp:189] Iteration 840, loss = 0.0320655
I0430 23:35:18.883750 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0320656 (* 1 = 0.0320656 loss)
I0430 23:35:18.883765 2050462464 solver.cpp:464] Iteration 840, lr = 0.001
I0430 23:37:08.026054 2050462464 solver.cpp:189] Iteration 860, loss = 0.00460368
I0430 23:37:08.027009 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00460372 (* 1 = 0.00460372 loss)
I0430 23:37:08.027024 2050462464 solver.cpp:464] Iteration 860, lr = 0.001
I0430 23:39:10.559917 2050462464 solver.cpp:189] Iteration 880, loss = 1.67354e-05
I0430 23:39:10.559967 2050462464 solver.cpp:204]     Train net output #0: loss = 1.67712e-05 (* 1 = 1.67712e-05 loss)
I0430 23:39:10.559976 2050462464 solver.cpp:464] Iteration 880, lr = 0.001
I0430 23:40:59.053454 2050462464 solver.cpp:189] Iteration 900, loss = 1.26449e-05
I0430 23:40:59.053500 2050462464 solver.cpp:204]     Train net output #0: loss = 1.26808e-05 (* 1 = 1.26808e-05 loss)
I0430 23:40:59.053510 2050462464 solver.cpp:464] Iteration 900, lr = 0.001
I0430 23:42:47.407340 2050462464 solver.cpp:189] Iteration 920, loss = 7.50581e-05
I0430 23:42:47.407388 2050462464 solver.cpp:204]     Train net output #0: loss = 7.5094e-05 (* 1 = 7.5094e-05 loss)
I0430 23:42:47.407399 2050462464 solver.cpp:464] Iteration 920, lr = 0.001
I0430 23:44:35.684612 2050462464 solver.cpp:189] Iteration 940, loss = 0.128119
I0430 23:44:35.684671 2050462464 solver.cpp:204]     Train net output #0: loss = 0.128119 (* 1 = 0.128119 loss)
I0430 23:44:35.684687 2050462464 solver.cpp:464] Iteration 940, lr = 0.001
I0430 23:46:32.854135 2050462464 solver.cpp:189] Iteration 960, loss = 0.116549
I0430 23:46:32.854198 2050462464 solver.cpp:204]     Train net output #0: loss = 0.116549 (* 1 = 0.116549 loss)
I0430 23:46:32.854214 2050462464 solver.cpp:464] Iteration 960, lr = 0.001
I0430 23:48:13.907863 2050462464 solver.cpp:189] Iteration 980, loss = 0.114675
I0430 23:48:13.907909 2050462464 solver.cpp:204]     Train net output #0: loss = 0.114675 (* 1 = 0.114675 loss)
I0430 23:48:13.907918 2050462464 solver.cpp:464] Iteration 980, lr = 0.001
I0430 23:49:53.087286 2050462464 solver.cpp:334] Snapshotting to caffenet_train_iter_1000.caffemodel
I0430 23:49:54.854121 2050462464 solver.cpp:342] Snapshotting solver state to caffenet_train_iter_1000.solverstate
I0430 23:49:56.291128 2050462464 solver.cpp:266] Iteration 1000, Testing net (#0)
I0501 00:24:30.266445 2050462464 solver.cpp:315]     Test net output #0: accuracy = 0.955156
I0501 00:24:30.266752 2050462464 solver.cpp:315]     Test net output #1: loss = 0.136043 (* 1 = 0.136043 loss)
I0501 00:24:34.901490 2050462464 solver.cpp:189] Iteration 1000, loss = 0.0745172
I0501 00:24:34.901542 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0745172 (* 1 = 0.0745172 loss)
I0501 00:24:34.901551 2050462464 solver.cpp:464] Iteration 1000, lr = 0.001
I0501 00:26:11.113633 2050462464 solver.cpp:189] Iteration 1020, loss = 0.00675328
I0501 00:26:11.113679 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00675325 (* 1 = 0.00675325 loss)
I0501 00:26:11.113688 2050462464 solver.cpp:464] Iteration 1020, lr = 0.001
I0501 00:27:49.076522 2050462464 solver.cpp:189] Iteration 1040, loss = 0.153154
I0501 00:27:49.076568 2050462464 solver.cpp:204]     Train net output #0: loss = 0.153154 (* 1 = 0.153154 loss)
I0501 00:27:49.076577 2050462464 solver.cpp:464] Iteration 1040, lr = 0.001
I0501 00:29:25.143520 2050462464 solver.cpp:189] Iteration 1060, loss = 0.180144
I0501 00:29:25.143623 2050462464 solver.cpp:204]     Train net output #0: loss = 0.180143 (* 1 = 0.180143 loss)
I0501 00:29:25.143658 2050462464 solver.cpp:464] Iteration 1060, lr = 0.001
I0501 00:31:01.689189 2050462464 solver.cpp:189] Iteration 1080, loss = 0.16037
I0501 00:31:01.689247 2050462464 solver.cpp:204]     Train net output #0: loss = 0.16037 (* 1 = 0.16037 loss)
I0501 00:31:01.689262 2050462464 solver.cpp:464] Iteration 1080, lr = 0.001
I0501 00:32:38.484503 2050462464 solver.cpp:189] Iteration 1100, loss = 0.0250182
I0501 00:32:38.484556 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0250182 (* 1 = 0.0250182 loss)
I0501 00:32:38.484566 2050462464 solver.cpp:464] Iteration 1100, lr = 0.001
I0501 00:34:15.410382 2050462464 solver.cpp:189] Iteration 1120, loss = 0.112023
I0501 00:34:15.410426 2050462464 solver.cpp:204]     Train net output #0: loss = 0.112023 (* 1 = 0.112023 loss)
I0501 00:34:15.410435 2050462464 solver.cpp:464] Iteration 1120, lr = 0.001
I0501 00:35:51.847929 2050462464 solver.cpp:189] Iteration 1140, loss = 0.102217
I0501 00:35:51.848034 2050462464 solver.cpp:204]     Train net output #0: loss = 0.102217 (* 1 = 0.102217 loss)
I0501 00:35:51.848048 2050462464 solver.cpp:464] Iteration 1140, lr = 0.001
I0501 00:37:38.223527 2050462464 solver.cpp:189] Iteration 1160, loss = 0.0838569
I0501 00:37:38.223587 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0838569 (* 1 = 0.0838569 loss)
I0501 00:37:38.223603 2050462464 solver.cpp:464] Iteration 1160, lr = 0.001
I0501 00:39:17.540206 2050462464 solver.cpp:189] Iteration 1180, loss = 0.00035511
I0501 00:39:17.540257 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000355079 (* 1 = 0.000355079 loss)
I0501 00:39:17.540272 2050462464 solver.cpp:464] Iteration 1180, lr = 0.001
I0501 00:40:53.585292 2050462464 solver.cpp:189] Iteration 1200, loss = 8.45083e-06
I0501 00:40:53.585341 2050462464 solver.cpp:204]     Train net output #0: loss = 8.42148e-06 (* 1 = 8.42148e-06 loss)
I0501 00:40:53.585371 2050462464 solver.cpp:464] Iteration 1200, lr = 0.001
I0501 00:42:29.633330 2050462464 solver.cpp:189] Iteration 1220, loss = 5.5803e-06
I0501 00:42:29.633379 2050462464 solver.cpp:204]     Train net output #0: loss = 5.55092e-06 (* 1 = 5.55092e-06 loss)
I0501 00:42:29.633391 2050462464 solver.cpp:464] Iteration 1220, lr = 0.001
I0501 00:44:05.999444 2050462464 solver.cpp:189] Iteration 1240, loss = 0.000114724
I0501 00:44:05.999490 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000114695 (* 1 = 0.000114695 loss)
I0501 00:44:05.999501 2050462464 solver.cpp:464] Iteration 1240, lr = 0.001
I0501 00:45:42.460147 2050462464 solver.cpp:189] Iteration 1260, loss = 0.068683
I0501 00:45:42.460208 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0686829 (* 1 = 0.0686829 loss)
I0501 00:45:42.460218 2050462464 solver.cpp:464] Iteration 1260, lr = 0.001
I0501 00:47:19.402396 2050462464 solver.cpp:189] Iteration 1280, loss = 0.0136515
I0501 00:47:19.402454 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0136514 (* 1 = 0.0136514 loss)
I0501 00:47:19.402470 2050462464 solver.cpp:464] Iteration 1280, lr = 0.001
I0501 00:48:55.441622 2050462464 solver.cpp:189] Iteration 1300, loss = 0.0904204
I0501 00:48:55.441669 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0904203 (* 1 = 0.0904203 loss)
I0501 00:48:55.441681 2050462464 solver.cpp:464] Iteration 1300, lr = 0.001
I0501 00:50:31.692917 2050462464 solver.cpp:189] Iteration 1320, loss = 0.271489
I0501 00:50:31.692965 2050462464 solver.cpp:204]     Train net output #0: loss = 0.271489 (* 1 = 0.271489 loss)
I0501 00:50:31.692973 2050462464 solver.cpp:464] Iteration 1320, lr = 0.001
I0501 00:52:07.556241 2050462464 solver.cpp:189] Iteration 1340, loss = 0.0598133
I0501 00:52:07.556285 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0598132 (* 1 = 0.0598132 loss)
I0501 00:52:07.556294 2050462464 solver.cpp:464] Iteration 1340, lr = 0.001
I0501 00:53:43.392226 2050462464 solver.cpp:189] Iteration 1360, loss = 0.0870813
I0501 00:53:43.392287 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0870812 (* 1 = 0.0870812 loss)
I0501 00:53:43.392303 2050462464 solver.cpp:464] Iteration 1360, lr = 0.001
I0501 00:55:20.521605 2050462464 solver.cpp:189] Iteration 1380, loss = 0.0706542
I0501 00:55:20.521687 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0706542 (* 1 = 0.0706542 loss)
I0501 00:55:20.521728 2050462464 solver.cpp:464] Iteration 1380, lr = 0.001
I0501 00:56:56.434702 2050462464 solver.cpp:189] Iteration 1400, loss = 0.114245
I0501 00:56:56.434748 2050462464 solver.cpp:204]     Train net output #0: loss = 0.114245 (* 1 = 0.114245 loss)
I0501 00:56:56.434757 2050462464 solver.cpp:464] Iteration 1400, lr = 0.001
I0501 00:58:31.921439 2050462464 solver.cpp:189] Iteration 1420, loss = 0.101583
I0501 00:58:31.921489 2050462464 solver.cpp:204]     Train net output #0: loss = 0.101583 (* 1 = 0.101583 loss)
I0501 00:58:31.921499 2050462464 solver.cpp:464] Iteration 1420, lr = 0.001
I0501 01:00:07.171103 2050462464 solver.cpp:189] Iteration 1440, loss = 0.00169782
I0501 01:00:07.171149 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00169776 (* 1 = 0.00169776 loss)
I0501 01:00:07.171159 2050462464 solver.cpp:464] Iteration 1440, lr = 0.001
I0501 01:01:42.034023 2050462464 solver.cpp:189] Iteration 1460, loss = 0.0154274
I0501 01:01:42.034065 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0154273 (* 1 = 0.0154273 loss)
I0501 01:01:42.034078 2050462464 solver.cpp:464] Iteration 1460, lr = 0.001
I0501 01:03:16.964658 2050462464 solver.cpp:189] Iteration 1480, loss = 0.00110232
I0501 01:03:16.964704 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00110224 (* 1 = 0.00110224 loss)
I0501 01:03:16.964714 2050462464 solver.cpp:464] Iteration 1480, lr = 0.001
I0501 01:04:48.058797 2050462464 solver.cpp:334] Snapshotting to caffenet_train_iter_1500.caffemodel
I0501 01:04:49.706791 2050462464 solver.cpp:342] Snapshotting solver state to caffenet_train_iter_1500.solverstate
I0501 01:04:55.367506 2050462464 solver.cpp:189] Iteration 1500, loss = 0.000317312
I0501 01:04:55.367545 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000317234 (* 1 = 0.000317234 loss)
I0501 01:04:55.367555 2050462464 solver.cpp:464] Iteration 1500, lr = 0.001
I0501 01:06:30.240685 2050462464 solver.cpp:189] Iteration 1520, loss = 2.43616e-05
I0501 01:06:30.240806 2050462464 solver.cpp:204]     Train net output #0: loss = 2.42842e-05 (* 1 = 2.42842e-05 loss)
I0501 01:06:30.240850 2050462464 solver.cpp:464] Iteration 1520, lr = 0.001
I0501 01:08:13.065039 2050462464 solver.cpp:189] Iteration 1540, loss = 7.16934e-05
I0501 01:08:13.065109 2050462464 solver.cpp:204]     Train net output #0: loss = 7.16172e-05 (* 1 = 7.16172e-05 loss)
I0501 01:08:13.065124 2050462464 solver.cpp:464] Iteration 1540, lr = 0.001
I0501 01:09:49.142424 2050462464 solver.cpp:189] Iteration 1560, loss = 0.542019
I0501 01:09:49.142472 2050462464 solver.cpp:204]     Train net output #0: loss = 0.542019 (* 1 = 0.542019 loss)
I0501 01:09:49.142484 2050462464 solver.cpp:464] Iteration 1560, lr = 0.001
I0501 01:11:23.678808 2050462464 solver.cpp:189] Iteration 1580, loss = 0.00958178
I0501 01:11:23.678854 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00958172 (* 1 = 0.00958172 loss)
I0501 01:11:23.678864 2050462464 solver.cpp:464] Iteration 1580, lr = 0.001
I0501 01:12:58.248404 2050462464 solver.cpp:189] Iteration 1600, loss = 0.0239166
I0501 01:12:58.248450 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0239165 (* 1 = 0.0239165 loss)
I0501 01:12:58.248460 2050462464 solver.cpp:464] Iteration 1600, lr = 0.001
I0501 01:14:32.784770 2050462464 solver.cpp:189] Iteration 1620, loss = 0.0243137
I0501 01:14:32.784814 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0243136 (* 1 = 0.0243136 loss)
I0501 01:14:32.784824 2050462464 solver.cpp:464] Iteration 1620, lr = 0.001
I0501 01:16:06.890725 2050462464 solver.cpp:189] Iteration 1640, loss = 0.157315
I0501 01:16:06.890781 2050462464 solver.cpp:204]     Train net output #0: loss = 0.157315 (* 1 = 0.157315 loss)
I0501 01:16:06.890791 2050462464 solver.cpp:464] Iteration 1640, lr = 0.001
I0501 01:17:40.081748 2050462464 solver.cpp:189] Iteration 1660, loss = 0.0285963
I0501 01:17:40.081814 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0285962 (* 1 = 0.0285962 loss)
I0501 01:17:40.081837 2050462464 solver.cpp:464] Iteration 1660, lr = 0.001
I0501 01:19:13.784445 2050462464 solver.cpp:189] Iteration 1680, loss = 0.156406
I0501 01:19:13.784499 2050462464 solver.cpp:204]     Train net output #0: loss = 0.156406 (* 1 = 0.156406 loss)
I0501 01:19:13.784508 2050462464 solver.cpp:464] Iteration 1680, lr = 0.001
I0501 01:20:47.220871 2050462464 solver.cpp:189] Iteration 1700, loss = 0.0865173
I0501 01:20:47.220927 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0865173 (* 1 = 0.0865173 loss)
I0501 01:20:47.220937 2050462464 solver.cpp:464] Iteration 1700, lr = 0.001
I0501 01:22:21.324228 2050462464 solver.cpp:189] Iteration 1720, loss = 0.162854
I0501 01:22:21.324272 2050462464 solver.cpp:204]     Train net output #0: loss = 0.162854 (* 1 = 0.162854 loss)
I0501 01:22:21.324282 2050462464 solver.cpp:464] Iteration 1720, lr = 0.001
I0501 01:23:55.658799 2050462464 solver.cpp:189] Iteration 1740, loss = 0.0599542
I0501 01:23:55.658846 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0599542 (* 1 = 0.0599542 loss)
I0501 01:23:55.658856 2050462464 solver.cpp:464] Iteration 1740, lr = 0.001
I0501 01:25:30.102720 2050462464 solver.cpp:189] Iteration 1760, loss = 0.0553547
I0501 01:25:30.102766 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0553547 (* 1 = 0.0553547 loss)
I0501 01:25:30.102777 2050462464 solver.cpp:464] Iteration 1760, lr = 0.001
I0501 01:27:06.832959 2050462464 solver.cpp:189] Iteration 1780, loss = 0.125914
I0501 01:27:06.833009 2050462464 solver.cpp:204]     Train net output #0: loss = 0.125914 (* 1 = 0.125914 loss)
I0501 01:27:06.833024 2050462464 solver.cpp:464] Iteration 1780, lr = 0.001
I0501 01:28:42.569186 2050462464 solver.cpp:189] Iteration 1800, loss = 0.00579589
I0501 01:28:42.569236 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00579589 (* 1 = 0.00579589 loss)
I0501 01:28:42.569247 2050462464 solver.cpp:464] Iteration 1800, lr = 0.001
I0501 01:30:18.389515 2050462464 solver.cpp:189] Iteration 1820, loss = 0.000149325
I0501 01:30:18.389565 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000149328 (* 1 = 0.000149328 loss)
I0501 01:30:18.389580 2050462464 solver.cpp:464] Iteration 1820, lr = 0.001
I0501 01:31:53.866570 2050462464 solver.cpp:189] Iteration 1840, loss = 8.31174e-06
I0501 01:31:53.866642 2050462464 solver.cpp:204]     Train net output #0: loss = 8.31351e-06 (* 1 = 8.31351e-06 loss)
I0501 01:31:53.866657 2050462464 solver.cpp:464] Iteration 1840, lr = 0.001
I0501 01:33:29.380570 2050462464 solver.cpp:189] Iteration 1860, loss = 5.42558e-06
I0501 01:33:29.380643 2050462464 solver.cpp:204]     Train net output #0: loss = 5.42736e-06 (* 1 = 5.42736e-06 loss)
I0501 01:33:29.380661 2050462464 solver.cpp:464] Iteration 1860, lr = 0.001
I0501 01:35:04.731688 2050462464 solver.cpp:189] Iteration 1880, loss = 0.035174
I0501 01:35:04.731730 2050462464 solver.cpp:204]     Train net output #0: loss = 0.035174 (* 1 = 0.035174 loss)
I0501 01:35:04.731739 2050462464 solver.cpp:464] Iteration 1880, lr = 0.001
I0501 01:36:40.097949 2050462464 solver.cpp:189] Iteration 1900, loss = 0.0213417
I0501 01:36:40.097995 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0213417 (* 1 = 0.0213417 loss)
I0501 01:36:40.098006 2050462464 solver.cpp:464] Iteration 1900, lr = 0.001
I0501 01:38:15.302585 2050462464 solver.cpp:189] Iteration 1920, loss = 0.0153988
I0501 01:38:15.302716 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0153988 (* 1 = 0.0153988 loss)
I0501 01:38:15.302731 2050462464 solver.cpp:464] Iteration 1920, lr = 0.001
I0501 01:39:50.649979 2050462464 solver.cpp:189] Iteration 1940, loss = 0.0208921
I0501 01:39:50.650027 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0208921 (* 1 = 0.0208921 loss)
I0501 01:39:50.650035 2050462464 solver.cpp:464] Iteration 1940, lr = 0.001
I0501 01:41:26.140888 2050462464 solver.cpp:189] Iteration 1960, loss = 0.0266193
I0501 01:41:26.140934 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0266194 (* 1 = 0.0266194 loss)
I0501 01:41:26.140944 2050462464 solver.cpp:464] Iteration 1960, lr = 0.001
I0501 01:43:01.329509 2050462464 solver.cpp:189] Iteration 1980, loss = 0.11372
I0501 01:43:01.329569 2050462464 solver.cpp:204]     Train net output #0: loss = 0.113721 (* 1 = 0.113721 loss)
I0501 01:43:01.329589 2050462464 solver.cpp:464] Iteration 1980, lr = 0.001
I0501 01:44:32.864555 2050462464 solver.cpp:334] Snapshotting to caffenet_train_iter_2000.caffemodel
I0501 01:44:34.439971 2050462464 solver.cpp:342] Snapshotting solver state to caffenet_train_iter_2000.solverstate
I0501 01:44:35.664275 2050462464 solver.cpp:266] Iteration 2000, Testing net (#0)
I0501 02:15:54.891237 2050462464 solver.cpp:315]     Test net output #0: accuracy = 0.954656
I0501 02:15:54.891278 2050462464 solver.cpp:315]     Test net output #1: loss = 0.157058 (* 1 = 0.157058 loss)
I0501 02:15:59.354334 2050462464 solver.cpp:189] Iteration 2000, loss = 0.0274592
I0501 02:15:59.354375 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0274592 (* 1 = 0.0274592 loss)
I0501 02:15:59.354385 2050462464 solver.cpp:464] Iteration 2000, lr = 0.001
I0501 02:17:34.923877 2050462464 solver.cpp:189] Iteration 2020, loss = 0.0322365
I0501 02:17:34.923926 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0322365 (* 1 = 0.0322365 loss)
I0501 02:17:34.923943 2050462464 solver.cpp:464] Iteration 2020, lr = 0.001
I0501 02:19:10.898892 2050462464 solver.cpp:189] Iteration 2040, loss = 0.0495903
I0501 02:19:10.898947 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0495904 (* 1 = 0.0495904 loss)
I0501 02:19:10.898960 2050462464 solver.cpp:464] Iteration 2040, lr = 0.001
I0501 02:20:46.262634 2050462464 solver.cpp:189] Iteration 2060, loss = 0.0346905
I0501 02:20:46.262683 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0346906 (* 1 = 0.0346906 loss)
I0501 02:20:46.262693 2050462464 solver.cpp:464] Iteration 2060, lr = 0.001
I0501 02:22:20.788601 2050462464 solver.cpp:189] Iteration 2080, loss = 0.065557
I0501 02:22:20.788646 2050462464 solver.cpp:204]     Train net output #0: loss = 0.065557 (* 1 = 0.065557 loss)
I0501 02:22:20.788656 2050462464 solver.cpp:464] Iteration 2080, lr = 0.001
I0501 02:23:56.051524 2050462464 solver.cpp:189] Iteration 2100, loss = 0.0482361
I0501 02:23:56.051589 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0482362 (* 1 = 0.0482362 loss)
I0501 02:23:56.051600 2050462464 solver.cpp:464] Iteration 2100, lr = 0.001
I0501 02:25:30.747719 2050462464 solver.cpp:189] Iteration 2120, loss = 0.000190898
I0501 02:25:30.747763 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000190955 (* 1 = 0.000190955 loss)
I0501 02:25:30.747773 2050462464 solver.cpp:464] Iteration 2120, lr = 0.001
I0501 02:27:06.473384 2050462464 solver.cpp:189] Iteration 2140, loss = 0.000915008
I0501 02:27:06.473433 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000915065 (* 1 = 0.000915065 loss)
I0501 02:27:06.473441 2050462464 solver.cpp:464] Iteration 2140, lr = 0.001
I0501 02:28:42.215270 2050462464 solver.cpp:189] Iteration 2160, loss = 5.91716e-06
I0501 02:28:42.215322 2050462464 solver.cpp:204]     Train net output #0: loss = 5.97394e-06 (* 1 = 5.97394e-06 loss)
I0501 02:28:42.215337 2050462464 solver.cpp:464] Iteration 2160, lr = 0.001
I0501 02:30:18.067003 2050462464 solver.cpp:189] Iteration 2180, loss = 0.0678476
I0501 02:30:18.067061 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0678477 (* 1 = 0.0678477 loss)
I0501 02:30:18.067077 2050462464 solver.cpp:464] Iteration 2180, lr = 0.001
I0501 02:31:53.205004 2050462464 solver.cpp:189] Iteration 2200, loss = 0.00649634
I0501 02:31:53.205065 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0064964 (* 1 = 0.0064964 loss)
I0501 02:31:53.205080 2050462464 solver.cpp:464] Iteration 2200, lr = 0.001
I0501 02:33:27.840474 2050462464 solver.cpp:189] Iteration 2220, loss = 0.0600946
I0501 02:33:27.840519 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0600946 (* 1 = 0.0600946 loss)
I0501 02:33:27.840529 2050462464 solver.cpp:464] Iteration 2220, lr = 0.001
I0501 02:35:03.166560 2050462464 solver.cpp:189] Iteration 2240, loss = 0.0269582
I0501 02:35:03.166610 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0269582 (* 1 = 0.0269582 loss)
I0501 02:35:03.166618 2050462464 solver.cpp:464] Iteration 2240, lr = 0.001
I0501 02:36:38.986459 2050462464 solver.cpp:189] Iteration 2260, loss = 0.0460133
I0501 02:36:38.986503 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0460133 (* 1 = 0.0460133 loss)
I0501 02:36:38.986512 2050462464 solver.cpp:464] Iteration 2260, lr = 0.001
I0501 02:38:13.107650 2050462464 solver.cpp:189] Iteration 2280, loss = 0.00969402
I0501 02:38:13.107697 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00969404 (* 1 = 0.00969404 loss)
I0501 02:38:13.107707 2050462464 solver.cpp:464] Iteration 2280, lr = 0.001
I0501 02:39:49.539014 2050462464 solver.cpp:189] Iteration 2300, loss = 0.0400417
I0501 02:39:49.539064 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0400417 (* 1 = 0.0400417 loss)
I0501 02:39:49.539074 2050462464 solver.cpp:464] Iteration 2300, lr = 0.001
I0501 02:41:24.997344 2050462464 solver.cpp:189] Iteration 2320, loss = 0.0275847
I0501 02:41:24.997397 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0275847 (* 1 = 0.0275847 loss)
I0501 02:41:24.997407 2050462464 solver.cpp:464] Iteration 2320, lr = 0.001
I0501 02:43:00.247030 2050462464 solver.cpp:189] Iteration 2340, loss = 0.089143
I0501 02:43:00.247083 2050462464 solver.cpp:204]     Train net output #0: loss = 0.089143 (* 1 = 0.089143 loss)
I0501 02:43:00.247092 2050462464 solver.cpp:464] Iteration 2340, lr = 0.001
I0501 02:44:36.274418 2050462464 solver.cpp:189] Iteration 2360, loss = 0.161138
I0501 02:44:36.274466 2050462464 solver.cpp:204]     Train net output #0: loss = 0.161138 (* 1 = 0.161138 loss)
I0501 02:44:36.274476 2050462464 solver.cpp:464] Iteration 2360, lr = 0.001
I0501 02:46:11.715222 2050462464 solver.cpp:189] Iteration 2380, loss = 0.00178201
I0501 02:46:11.715268 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00178199 (* 1 = 0.00178199 loss)
I0501 02:46:11.715278 2050462464 solver.cpp:464] Iteration 2380, lr = 0.001
I0501 02:47:46.787227 2050462464 solver.cpp:189] Iteration 2400, loss = 0.00874927
I0501 02:47:46.787287 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00874926 (* 1 = 0.00874926 loss)
I0501 02:47:46.787297 2050462464 solver.cpp:464] Iteration 2400, lr = 0.001
I0501 02:49:21.879938 2050462464 solver.cpp:189] Iteration 2420, loss = 3.11164e-05
I0501 02:49:21.879986 2050462464 solver.cpp:204]     Train net output #0: loss = 3.11166e-05 (* 1 = 3.11166e-05 loss)
I0501 02:49:21.879995 2050462464 solver.cpp:464] Iteration 2420, lr = 0.001
I0501 02:50:56.990645 2050462464 solver.cpp:189] Iteration 2440, loss = 2.53092e-05
I0501 02:50:56.990700 2050462464 solver.cpp:204]     Train net output #0: loss = 2.53093e-05 (* 1 = 2.53093e-05 loss)
I0501 02:50:56.990710 2050462464 solver.cpp:464] Iteration 2440, lr = 0.001
I0501 02:52:32.657199 2050462464 solver.cpp:189] Iteration 2460, loss = 1.83524e-05
I0501 02:52:32.657258 2050462464 solver.cpp:204]     Train net output #0: loss = 1.83525e-05 (* 1 = 1.83525e-05 loss)
I0501 02:52:32.657274 2050462464 solver.cpp:464] Iteration 2460, lr = 0.001
I0501 02:54:07.375974 2050462464 solver.cpp:189] Iteration 2480, loss = 2.81725e-06
I0501 02:54:07.376034 2050462464 solver.cpp:204]     Train net output #0: loss = 2.81639e-06 (* 1 = 2.81639e-06 loss)
I0501 02:54:07.376045 2050462464 solver.cpp:464] Iteration 2480, lr = 0.001
I0501 02:55:38.188160 2050462464 solver.cpp:334] Snapshotting to caffenet_train_iter_2500.caffemodel
I0501 02:55:39.892346 2050462464 solver.cpp:342] Snapshotting solver state to caffenet_train_iter_2500.solverstate
I0501 02:55:45.647356 2050462464 solver.cpp:189] Iteration 2500, loss = 0.00243551
I0501 02:55:45.647387 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00243551 (* 1 = 0.00243551 loss)
I0501 02:55:45.647395 2050462464 solver.cpp:464] Iteration 2500, lr = 0.001
I0501 02:57:20.494417 2050462464 solver.cpp:189] Iteration 2520, loss = 0.0681473
I0501 02:57:20.494463 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0681473 (* 1 = 0.0681473 loss)
I0501 02:57:20.494472 2050462464 solver.cpp:464] Iteration 2520, lr = 0.001
I0501 02:58:55.724424 2050462464 solver.cpp:189] Iteration 2540, loss = 0.0116125
I0501 02:58:55.724467 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0116125 (* 1 = 0.0116125 loss)
I0501 02:58:55.724478 2050462464 solver.cpp:464] Iteration 2540, lr = 0.001
I0501 03:00:31.431576 2050462464 solver.cpp:189] Iteration 2560, loss = 0.0062839
I0501 03:00:31.431622 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00628391 (* 1 = 0.00628391 loss)
I0501 03:00:31.431637 2050462464 solver.cpp:464] Iteration 2560, lr = 0.001
I0501 03:02:06.948429 2050462464 solver.cpp:189] Iteration 2580, loss = 0.00889759
I0501 03:02:06.948475 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00889758 (* 1 = 0.00889758 loss)
I0501 03:02:06.948485 2050462464 solver.cpp:464] Iteration 2580, lr = 0.001
I0501 03:03:42.217725 2050462464 solver.cpp:189] Iteration 2600, loss = 0.0070056
I0501 03:03:42.217774 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00700559 (* 1 = 0.00700559 loss)
I0501 03:03:42.217784 2050462464 solver.cpp:464] Iteration 2600, lr = 0.001
I0501 03:05:17.424520 2050462464 solver.cpp:189] Iteration 2620, loss = 0.0774957
I0501 03:05:17.424566 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0774957 (* 1 = 0.0774957 loss)
I0501 03:05:17.424581 2050462464 solver.cpp:464] Iteration 2620, lr = 0.001
I0501 03:06:52.407910 2050462464 solver.cpp:189] Iteration 2640, loss = 0.0754019
I0501 03:06:52.407961 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0754019 (* 1 = 0.0754019 loss)
I0501 03:06:52.407973 2050462464 solver.cpp:464] Iteration 2640, lr = 0.001
I0501 03:08:27.260534 2050462464 solver.cpp:189] Iteration 2660, loss = 0.0595944
I0501 03:08:27.260579 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0595944 (* 1 = 0.0595944 loss)
I0501 03:08:27.260589 2050462464 solver.cpp:464] Iteration 2660, lr = 0.001
I0501 03:10:02.744598 2050462464 solver.cpp:189] Iteration 2680, loss = 0.051659
I0501 03:10:02.744675 2050462464 solver.cpp:204]     Train net output #0: loss = 0.051659 (* 1 = 0.051659 loss)
I0501 03:10:02.744690 2050462464 solver.cpp:464] Iteration 2680, lr = 0.001
I0501 03:11:37.581544 2050462464 solver.cpp:189] Iteration 2700, loss = 0.0416601
I0501 03:11:37.581593 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0416601 (* 1 = 0.0416601 loss)
I0501 03:11:37.581603 2050462464 solver.cpp:464] Iteration 2700, lr = 0.001
I0501 03:13:12.360942 2050462464 solver.cpp:189] Iteration 2720, loss = 0.0358025
I0501 03:13:12.360996 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0358025 (* 1 = 0.0358025 loss)
I0501 03:13:12.361006 2050462464 solver.cpp:464] Iteration 2720, lr = 0.001
I0501 03:14:47.310178 2050462464 solver.cpp:189] Iteration 2740, loss = 1.82531e-05
I0501 03:14:47.310221 2050462464 solver.cpp:204]     Train net output #0: loss = 1.82398e-05 (* 1 = 1.82398e-05 loss)
I0501 03:14:47.310231 2050462464 solver.cpp:464] Iteration 2740, lr = 0.001
I0501 03:16:21.890213 2050462464 solver.cpp:189] Iteration 2760, loss = 1.28871e-05
I0501 03:16:21.890260 2050462464 solver.cpp:204]     Train net output #0: loss = 1.28735e-05 (* 1 = 1.28735e-05 loss)
I0501 03:16:21.890269 2050462464 solver.cpp:464] Iteration 2760, lr = 0.001
I0501 03:17:56.552129 2050462464 solver.cpp:189] Iteration 2780, loss = 7.36247e-07
I0501 03:17:56.552186 2050462464 solver.cpp:204]     Train net output #0: loss = 7.2271e-07 (* 1 = 7.2271e-07 loss)
I0501 03:17:56.552196 2050462464 solver.cpp:464] Iteration 2780, lr = 0.001
I0501 03:19:31.347230 2050462464 solver.cpp:189] Iteration 2800, loss = 7.08168e-05
I0501 03:19:31.347275 2050462464 solver.cpp:204]     Train net output #0: loss = 7.08032e-05 (* 1 = 7.08032e-05 loss)
I0501 03:19:31.347285 2050462464 solver.cpp:464] Iteration 2800, lr = 0.001
I0501 03:21:06.173446 2050462464 solver.cpp:189] Iteration 2820, loss = 0.0446472
I0501 03:21:06.173486 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0446472 (* 1 = 0.0446472 loss)
I0501 03:21:06.173496 2050462464 solver.cpp:464] Iteration 2820, lr = 0.001
I0501 03:22:41.459451 2050462464 solver.cpp:189] Iteration 2840, loss = 0.00122924
I0501 03:22:41.459506 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00122923 (* 1 = 0.00122923 loss)
I0501 03:22:41.459519 2050462464 solver.cpp:464] Iteration 2840, lr = 0.001
I0501 03:24:16.152928 2050462464 solver.cpp:189] Iteration 2860, loss = 0.0112919
I0501 03:24:16.152981 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0112919 (* 1 = 0.0112919 loss)
I0501 03:24:16.152992 2050462464 solver.cpp:464] Iteration 2860, lr = 0.001
I0501 03:25:49.904230 2050462464 solver.cpp:189] Iteration 2880, loss = 0.175472
I0501 03:25:49.904276 2050462464 solver.cpp:204]     Train net output #0: loss = 0.175472 (* 1 = 0.175472 loss)
I0501 03:25:49.904285 2050462464 solver.cpp:464] Iteration 2880, lr = 0.001
I0501 03:27:24.866768 2050462464 solver.cpp:189] Iteration 2900, loss = 0.0349744
I0501 03:27:24.866827 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0349744 (* 1 = 0.0349744 loss)
I0501 03:27:24.866837 2050462464 solver.cpp:464] Iteration 2900, lr = 0.001
I0501 03:28:59.715512 2050462464 solver.cpp:189] Iteration 2920, loss = 0.000969997
I0501 03:28:59.715564 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00096998 (* 1 = 0.00096998 loss)
I0501 03:28:59.715575 2050462464 solver.cpp:464] Iteration 2920, lr = 0.001
I0501 03:30:34.334758 2050462464 solver.cpp:189] Iteration 2940, loss = 0.0337864
I0501 03:30:34.334805 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0337864 (* 1 = 0.0337864 loss)
I0501 03:30:34.334815 2050462464 solver.cpp:464] Iteration 2940, lr = 0.001
I0501 03:32:09.204658 2050462464 solver.cpp:189] Iteration 2960, loss = 0.0486995
I0501 03:32:09.204699 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0486995 (* 1 = 0.0486995 loss)
I0501 03:32:09.204712 2050462464 solver.cpp:464] Iteration 2960, lr = 0.001
I0501 03:33:44.077499 2050462464 solver.cpp:189] Iteration 2980, loss = 0.00219633
I0501 03:33:44.077568 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00219632 (* 1 = 0.00219632 loss)
I0501 03:33:44.077579 2050462464 solver.cpp:464] Iteration 2980, lr = 0.001
I0501 03:35:15.643800 2050462464 solver.cpp:334] Snapshotting to caffenet_train_iter_3000.caffemodel
I0501 03:35:17.233629 2050462464 solver.cpp:342] Snapshotting solver state to caffenet_train_iter_3000.solverstate
I0501 03:35:18.484275 2050462464 solver.cpp:266] Iteration 3000, Testing net (#0)
I0501 04:06:25.804906 2050462464 solver.cpp:315]     Test net output #0: accuracy = 0.963453
I0501 04:06:25.805042 2050462464 solver.cpp:315]     Test net output #1: loss = 0.12976 (* 1 = 0.12976 loss)
I0501 04:06:30.262891 2050462464 solver.cpp:189] Iteration 3000, loss = 0.000410378
I0501 04:06:30.262931 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000410364 (* 1 = 0.000410364 loss)
I0501 04:06:30.262941 2050462464 solver.cpp:464] Iteration 3000, lr = 0.001
I0501 04:08:04.623181 2050462464 solver.cpp:189] Iteration 3020, loss = 0.00700407
I0501 04:08:04.623227 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00700407 (* 1 = 0.00700407 loss)
I0501 04:08:04.623236 2050462464 solver.cpp:464] Iteration 3020, lr = 0.001
I0501 04:09:39.095513 2050462464 solver.cpp:189] Iteration 3040, loss = 0.0012026
I0501 04:09:39.095558 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00120262 (* 1 = 0.00120262 loss)
I0501 04:09:39.095568 2050462464 solver.cpp:464] Iteration 3040, lr = 0.001
I0501 04:11:13.921811 2050462464 solver.cpp:189] Iteration 3060, loss = 0.00132799
I0501 04:11:13.921857 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00132801 (* 1 = 0.00132801 loss)
I0501 04:11:13.921867 2050462464 solver.cpp:464] Iteration 3060, lr = 0.001
I0501 04:12:48.689127 2050462464 solver.cpp:189] Iteration 3080, loss = 3.64696e-06
I0501 04:12:48.689172 2050462464 solver.cpp:204]     Train net output #0: loss = 3.66398e-06 (* 1 = 3.66398e-06 loss)
I0501 04:12:48.689182 2050462464 solver.cpp:464] Iteration 3080, lr = 0.001
I0501 04:14:23.575280 2050462464 solver.cpp:189] Iteration 3100, loss = 2.29292e-06
I0501 04:14:23.575325 2050462464 solver.cpp:204]     Train net output #0: loss = 2.30973e-06 (* 1 = 2.30973e-06 loss)
I0501 04:14:23.575335 2050462464 solver.cpp:464] Iteration 3100, lr = 0.001
I0501 04:15:58.147524 2050462464 solver.cpp:189] Iteration 3120, loss = 0.0578922
I0501 04:15:58.147583 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0578922 (* 1 = 0.0578922 loss)
I0501 04:15:58.147601 2050462464 solver.cpp:464] Iteration 3120, lr = 0.001
I0501 04:17:33.531657 2050462464 solver.cpp:189] Iteration 3140, loss = 0.0114199
I0501 04:17:33.531703 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0114199 (* 1 = 0.0114199 loss)
I0501 04:17:33.531713 2050462464 solver.cpp:464] Iteration 3140, lr = 0.001
I0501 04:19:08.323647 2050462464 solver.cpp:189] Iteration 3160, loss = 0.0662711
I0501 04:19:08.323700 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0662711 (* 1 = 0.0662711 loss)
I0501 04:19:08.323710 2050462464 solver.cpp:464] Iteration 3160, lr = 0.001
I0501 04:20:43.164692 2050462464 solver.cpp:189] Iteration 3180, loss = 0.0229072
I0501 04:20:43.164739 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0229073 (* 1 = 0.0229073 loss)
I0501 04:20:43.164748 2050462464 solver.cpp:464] Iteration 3180, lr = 0.001
I0501 04:22:17.650359 2050462464 solver.cpp:189] Iteration 3200, loss = 0.0510988
I0501 04:22:17.650405 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0510988 (* 1 = 0.0510988 loss)
I0501 04:22:17.650415 2050462464 solver.cpp:464] Iteration 3200, lr = 0.001
I0501 04:23:52.548879 2050462464 solver.cpp:189] Iteration 3220, loss = 0.0257768
I0501 04:23:52.548926 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0257768 (* 1 = 0.0257768 loss)
I0501 04:23:52.548936 2050462464 solver.cpp:464] Iteration 3220, lr = 0.001
I0501 04:25:27.447285 2050462464 solver.cpp:189] Iteration 3240, loss = 0.0581891
I0501 04:25:27.447347 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0581892 (* 1 = 0.0581892 loss)
I0501 04:25:27.447358 2050462464 solver.cpp:464] Iteration 3240, lr = 0.001
I0501 04:27:01.455991 2050462464 solver.cpp:189] Iteration 3260, loss = 0.148908
I0501 04:27:01.456038 2050462464 solver.cpp:204]     Train net output #0: loss = 0.148908 (* 1 = 0.148908 loss)
I0501 04:27:01.456048 2050462464 solver.cpp:464] Iteration 3260, lr = 0.001
I0501 04:28:36.462698 2050462464 solver.cpp:189] Iteration 3280, loss = 0.0352188
I0501 04:28:36.462746 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0352188 (* 1 = 0.0352188 loss)
I0501 04:28:36.462760 2050462464 solver.cpp:464] Iteration 3280, lr = 0.001
I0501 04:30:10.700274 2050462464 solver.cpp:189] Iteration 3300, loss = 0.0132139
I0501 04:30:10.700325 2050462464 solver.cpp:204]     Train net output #0: loss = 0.013214 (* 1 = 0.013214 loss)
I0501 04:30:10.700335 2050462464 solver.cpp:464] Iteration 3300, lr = 0.001
I0501 04:31:45.381085 2050462464 solver.cpp:189] Iteration 3320, loss = 0.058867
I0501 04:31:45.381150 2050462464 solver.cpp:204]     Train net output #0: loss = 0.058867 (* 1 = 0.058867 loss)
I0501 04:31:45.381160 2050462464 solver.cpp:464] Iteration 3320, lr = 0.001
I0501 04:33:19.886420 2050462464 solver.cpp:189] Iteration 3340, loss = 0.0343942
I0501 04:33:19.886467 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0343943 (* 1 = 0.0343943 loss)
I0501 04:33:19.886477 2050462464 solver.cpp:464] Iteration 3340, lr = 0.001
I0501 04:34:53.935178 2050462464 solver.cpp:189] Iteration 3360, loss = 0.000151195
I0501 04:34:53.935227 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000151227 (* 1 = 0.000151227 loss)
I0501 04:34:53.935236 2050462464 solver.cpp:464] Iteration 3360, lr = 0.001
I0501 04:36:26.056097 2050462464 solver.cpp:189] Iteration 3380, loss = 1.21363e-06
I0501 04:36:26.056149 2050462464 solver.cpp:204]     Train net output #0: loss = 1.248e-06 (* 1 = 1.248e-06 loss)
I0501 04:36:26.056159 2050462464 solver.cpp:464] Iteration 3380, lr = 0.001
I0501 04:37:57.722422 2050462464 solver.cpp:189] Iteration 3400, loss = 1.58433e-06
I0501 04:37:57.722522 2050462464 solver.cpp:204]     Train net output #0: loss = 1.61869e-06 (* 1 = 1.61869e-06 loss)
I0501 04:37:57.722537 2050462464 solver.cpp:464] Iteration 3400, lr = 0.001
I0501 04:39:30.024951 2050462464 solver.cpp:189] Iteration 3420, loss = 2.91069e-06
I0501 04:39:30.025005 2050462464 solver.cpp:204]     Train net output #0: loss = 2.94503e-06 (* 1 = 2.94503e-06 loss)
I0501 04:39:30.025015 2050462464 solver.cpp:464] Iteration 3420, lr = 0.001
I0501 04:41:01.672960 2050462464 solver.cpp:189] Iteration 3440, loss = 0.00134761
I0501 04:41:01.673081 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00134764 (* 1 = 0.00134764 loss)
I0501 04:41:01.673094 2050462464 solver.cpp:464] Iteration 3440, lr = 0.001
I0501 04:42:32.410318 2050462464 solver.cpp:189] Iteration 3460, loss = 0.00952293
I0501 04:42:32.410454 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00952296 (* 1 = 0.00952296 loss)
I0501 04:42:32.410503 2050462464 solver.cpp:464] Iteration 3460, lr = 0.001
I0501 04:44:03.958492 2050462464 solver.cpp:189] Iteration 3480, loss = 0.0288233
I0501 04:44:03.958544 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0288234 (* 1 = 0.0288234 loss)
I0501 04:44:03.958554 2050462464 solver.cpp:464] Iteration 3480, lr = 0.001
I0501 04:45:30.889173 2050462464 solver.cpp:334] Snapshotting to caffenet_train_iter_3500.caffemodel
I0501 04:45:32.532930 2050462464 solver.cpp:342] Snapshotting solver state to caffenet_train_iter_3500.solverstate
I0501 04:45:38.104624 2050462464 solver.cpp:189] Iteration 3500, loss = 0.0114846
I0501 04:45:38.104661 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0114846 (* 1 = 0.0114846 loss)
I0501 04:45:38.104671 2050462464 solver.cpp:464] Iteration 3500, lr = 0.001
I0501 04:47:08.939138 2050462464 solver.cpp:189] Iteration 3520, loss = 0.0201424
I0501 04:47:08.939188 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0201424 (* 1 = 0.0201424 loss)
I0501 04:47:08.939198 2050462464 solver.cpp:464] Iteration 3520, lr = 0.001
I0501 04:48:39.042848 2050462464 solver.cpp:189] Iteration 3540, loss = 0.0189122
I0501 04:48:39.042906 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0189123 (* 1 = 0.0189123 loss)
I0501 04:48:39.042917 2050462464 solver.cpp:464] Iteration 3540, lr = 0.001
I0501 04:50:10.327606 2050462464 solver.cpp:189] Iteration 3560, loss = 0.0286205
I0501 04:50:10.327684 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0286205 (* 1 = 0.0286205 loss)
I0501 04:50:10.327697 2050462464 solver.cpp:464] Iteration 3560, lr = 0.001
I0501 04:51:39.499284 2050462464 solver.cpp:189] Iteration 3580, loss = 0.00551271
I0501 04:51:39.499337 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00551273 (* 1 = 0.00551273 loss)
I0501 04:51:39.499347 2050462464 solver.cpp:464] Iteration 3580, lr = 0.001
I0501 04:53:10.870051 2050462464 solver.cpp:189] Iteration 3600, loss = 0.00561404
I0501 04:53:10.870101 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00561406 (* 1 = 0.00561406 loss)
I0501 04:53:10.870111 2050462464 solver.cpp:464] Iteration 3600, lr = 0.001
I0501 04:54:40.779780 2050462464 solver.cpp:189] Iteration 3620, loss = 0.0659971
I0501 04:54:40.779824 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0659971 (* 1 = 0.0659971 loss)
I0501 04:54:40.779834 2050462464 solver.cpp:464] Iteration 3620, lr = 0.001
I0501 04:56:10.986716 2050462464 solver.cpp:189] Iteration 3640, loss = 0.0220978
I0501 04:56:10.986763 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0220978 (* 1 = 0.0220978 loss)
I0501 04:56:10.986773 2050462464 solver.cpp:464] Iteration 3640, lr = 0.001
I0501 04:57:40.619577 2050462464 solver.cpp:189] Iteration 3660, loss = 0.145789
I0501 04:57:40.619627 2050462464 solver.cpp:204]     Train net output #0: loss = 0.145789 (* 1 = 0.145789 loss)
I0501 04:57:40.619637 2050462464 solver.cpp:464] Iteration 3660, lr = 0.001
I0501 04:59:10.490161 2050462464 solver.cpp:189] Iteration 3680, loss = 5.17009e-05
I0501 04:59:10.490209 2050462464 solver.cpp:204]     Train net output #0: loss = 5.1731e-05 (* 1 = 5.1731e-05 loss)
I0501 04:59:10.490219 2050462464 solver.cpp:464] Iteration 3680, lr = 0.001
I0501 05:00:40.299399 2050462464 solver.cpp:189] Iteration 3700, loss = 0.000219348
I0501 05:00:40.299453 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000219378 (* 1 = 0.000219378 loss)
I0501 05:00:40.299463 2050462464 solver.cpp:464] Iteration 3700, lr = 0.001
I0501 05:02:11.359597 2050462464 solver.cpp:189] Iteration 3720, loss = 0.000524173
I0501 05:02:11.359647 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000524203 (* 1 = 0.000524203 loss)
I0501 05:02:11.359657 2050462464 solver.cpp:464] Iteration 3720, lr = 0.001
I0501 05:03:37.468286 2050462464 solver.cpp:189] Iteration 3740, loss = 0.00167857
I0501 05:03:37.468339 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0016786 (* 1 = 0.0016786 loss)
I0501 05:03:37.468349 2050462464 solver.cpp:464] Iteration 3740, lr = 0.001
I0501 05:05:01.205781 2050462464 solver.cpp:189] Iteration 3760, loss = 0.000260338
I0501 05:05:01.205834 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000260358 (* 1 = 0.000260358 loss)
I0501 05:05:01.205844 2050462464 solver.cpp:464] Iteration 3760, lr = 0.001
I0501 05:06:24.439107 2050462464 solver.cpp:189] Iteration 3780, loss = 0.000820207
I0501 05:06:24.439157 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000820231 (* 1 = 0.000820231 loss)
I0501 05:06:24.439167 2050462464 solver.cpp:464] Iteration 3780, lr = 0.001
I0501 05:07:47.743209 2050462464 solver.cpp:189] Iteration 3800, loss = 0.0271804
I0501 05:07:47.743263 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0271804 (* 1 = 0.0271804 loss)
I0501 05:07:47.743273 2050462464 solver.cpp:464] Iteration 3800, lr = 0.001
I0501 05:09:11.250520 2050462464 solver.cpp:189] Iteration 3820, loss = 0.00611028
I0501 05:09:11.250568 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00611029 (* 1 = 0.00611029 loss)
I0501 05:09:11.250578 2050462464 solver.cpp:464] Iteration 3820, lr = 0.001
I0501 05:10:34.931120 2050462464 solver.cpp:189] Iteration 3840, loss = 0.00405058
I0501 05:10:34.931185 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0040506 (* 1 = 0.0040506 loss)
I0501 05:10:34.931195 2050462464 solver.cpp:464] Iteration 3840, lr = 0.001
I0501 05:11:58.132349 2050462464 solver.cpp:189] Iteration 3860, loss = 0.0127074
I0501 05:11:58.132402 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0127074 (* 1 = 0.0127074 loss)
I0501 05:11:58.132412 2050462464 solver.cpp:464] Iteration 3860, lr = 0.001
I0501 05:13:21.439447 2050462464 solver.cpp:189] Iteration 3880, loss = 0.00234082
I0501 05:13:21.439498 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00234084 (* 1 = 0.00234084 loss)
I0501 05:13:21.439507 2050462464 solver.cpp:464] Iteration 3880, lr = 0.001
I0501 05:14:44.774453 2050462464 solver.cpp:189] Iteration 3900, loss = 0.0703764
I0501 05:14:44.774507 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0703764 (* 1 = 0.0703764 loss)
I0501 05:14:44.774518 2050462464 solver.cpp:464] Iteration 3900, lr = 0.001
I0501 05:16:07.921139 2050462464 solver.cpp:189] Iteration 3920, loss = 0.0241887
I0501 05:16:07.921192 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0241887 (* 1 = 0.0241887 loss)
I0501 05:16:07.921202 2050462464 solver.cpp:464] Iteration 3920, lr = 0.001
I0501 05:17:31.399106 2050462464 solver.cpp:189] Iteration 3940, loss = 0.000203637
I0501 05:17:31.399157 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000203668 (* 1 = 0.000203668 loss)
I0501 05:17:31.399166 2050462464 solver.cpp:464] Iteration 3940, lr = 0.001
I0501 05:18:54.817090 2050462464 solver.cpp:189] Iteration 3960, loss = 0.000690818
I0501 05:18:54.817147 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000690853 (* 1 = 0.000690853 loss)
I0501 05:18:54.817160 2050462464 solver.cpp:464] Iteration 3960, lr = 0.001
I0501 05:20:18.206954 2050462464 solver.cpp:189] Iteration 3980, loss = 0.000381492
I0501 05:20:18.207002 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000381525 (* 1 = 0.000381525 loss)
I0501 05:20:18.207012 2050462464 solver.cpp:464] Iteration 3980, lr = 0.001
I0501 05:21:38.087322 2050462464 solver.cpp:334] Snapshotting to caffenet_train_iter_4000.caffemodel
I0501 05:21:39.617956 2050462464 solver.cpp:342] Snapshotting solver state to caffenet_train_iter_4000.solverstate
I0501 05:21:40.873666 2050462464 solver.cpp:266] Iteration 4000, Testing net (#0)
I0501 05:49:14.018064 2050462464 solver.cpp:315]     Test net output #0: accuracy = 0.955328
I0501 05:49:14.018228 2050462464 solver.cpp:315]     Test net output #1: loss = 0.196321 (* 1 = 0.196321 loss)
I0501 05:49:18.019675 2050462464 solver.cpp:189] Iteration 4000, loss = 8.974e-05
I0501 05:49:18.019711 2050462464 solver.cpp:204]     Train net output #0: loss = 8.97726e-05 (* 1 = 8.97726e-05 loss)
I0501 05:49:18.019721 2050462464 solver.cpp:464] Iteration 4000, lr = 0.001
I0501 05:50:41.678978 2050462464 solver.cpp:189] Iteration 4020, loss = 1.00106e-06
I0501 05:50:41.679026 2050462464 solver.cpp:204]     Train net output #0: loss = 1.03377e-06 (* 1 = 1.03377e-06 loss)
I0501 05:50:41.679036 2050462464 solver.cpp:464] Iteration 4020, lr = 0.001
I0501 05:52:05.233767 2050462464 solver.cpp:189] Iteration 4040, loss = 1.2339e-06
I0501 05:52:05.233820 2050462464 solver.cpp:204]     Train net output #0: loss = 1.26664e-06 (* 1 = 1.26664e-06 loss)
I0501 05:52:05.233830 2050462464 solver.cpp:464] Iteration 4040, lr = 0.001
I0501 05:53:28.625455 2050462464 solver.cpp:189] Iteration 4060, loss = 0.00290572
I0501 05:53:28.625500 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00290575 (* 1 = 0.00290575 loss)
I0501 05:53:28.625510 2050462464 solver.cpp:464] Iteration 4060, lr = 0.001
I0501 05:54:52.184787 2050462464 solver.cpp:189] Iteration 4080, loss = 0.000879571
I0501 05:54:52.184839 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000879601 (* 1 = 0.000879601 loss)
I0501 05:54:52.184849 2050462464 solver.cpp:464] Iteration 4080, lr = 0.001
I0501 05:56:15.693573 2050462464 solver.cpp:189] Iteration 4100, loss = 0.00998941
I0501 05:56:15.693649 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00998945 (* 1 = 0.00998945 loss)
I0501 05:56:15.693660 2050462464 solver.cpp:464] Iteration 4100, lr = 0.001
I0501 05:57:38.985957 2050462464 solver.cpp:189] Iteration 4120, loss = 0.00144769
I0501 05:57:38.986011 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00144772 (* 1 = 0.00144772 loss)
I0501 05:57:38.986021 2050462464 solver.cpp:464] Iteration 4120, lr = 0.001
I0501 05:59:02.396541 2050462464 solver.cpp:189] Iteration 4140, loss = 0.000453029
I0501 05:59:02.396596 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000453065 (* 1 = 0.000453065 loss)
I0501 05:59:02.396606 2050462464 solver.cpp:464] Iteration 4140, lr = 0.001
I0501 06:00:26.030336 2050462464 solver.cpp:189] Iteration 4160, loss = 0.0141891
I0501 06:00:26.030388 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0141891 (* 1 = 0.0141891 loss)
I0501 06:00:26.030398 2050462464 solver.cpp:464] Iteration 4160, lr = 0.001
I0501 06:01:49.811966 2050462464 solver.cpp:189] Iteration 4180, loss = 0.0581928
I0501 06:01:49.812013 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0581928 (* 1 = 0.0581928 loss)
I0501 06:01:49.812023 2050462464 solver.cpp:464] Iteration 4180, lr = 0.001
I0501 06:03:13.251438 2050462464 solver.cpp:189] Iteration 4200, loss = 0.0155607
I0501 06:03:13.251492 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0155607 (* 1 = 0.0155607 loss)
I0501 06:03:13.251502 2050462464 solver.cpp:464] Iteration 4200, lr = 0.001
I0501 06:04:37.221112 2050462464 solver.cpp:189] Iteration 4220, loss = 0.00219958
I0501 06:04:37.221160 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00219962 (* 1 = 0.00219962 loss)
I0501 06:04:37.221170 2050462464 solver.cpp:464] Iteration 4220, lr = 0.001
I0501 06:06:00.930678 2050462464 solver.cpp:189] Iteration 4240, loss = 0.0238049
I0501 06:06:00.930734 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0238049 (* 1 = 0.0238049 loss)
I0501 06:06:00.930743 2050462464 solver.cpp:464] Iteration 4240, lr = 0.001
I0501 06:07:24.677392 2050462464 solver.cpp:189] Iteration 4260, loss = 0.0101535
I0501 06:07:24.677445 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0101535 (* 1 = 0.0101535 loss)
I0501 06:07:24.677454 2050462464 solver.cpp:464] Iteration 4260, lr = 0.001
I0501 06:08:48.082226 2050462464 solver.cpp:189] Iteration 4280, loss = 0.00278476
I0501 06:08:48.082279 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0027848 (* 1 = 0.0027848 loss)
I0501 06:08:48.082289 2050462464 solver.cpp:464] Iteration 4280, lr = 0.001
I0501 06:10:11.793220 2050462464 solver.cpp:189] Iteration 4300, loss = 6.39147e-05
I0501 06:10:11.793272 2050462464 solver.cpp:204]     Train net output #0: loss = 6.39541e-05 (* 1 = 6.39541e-05 loss)
I0501 06:10:11.793282 2050462464 solver.cpp:464] Iteration 4300, lr = 0.001
I0501 06:11:35.285639 2050462464 solver.cpp:189] Iteration 4320, loss = 8.62298e-07
I0501 06:11:35.285691 2050462464 solver.cpp:204]     Train net output #0: loss = 9.01532e-07 (* 1 = 9.01532e-07 loss)
I0501 06:11:35.285701 2050462464 solver.cpp:464] Iteration 4320, lr = 0.001
I0501 06:12:58.869076 2050462464 solver.cpp:189] Iteration 4340, loss = 1.25717e-06
I0501 06:12:58.869125 2050462464 solver.cpp:204]     Train net output #0: loss = 1.29642e-06 (* 1 = 1.29642e-06 loss)
I0501 06:12:58.869135 2050462464 solver.cpp:464] Iteration 4340, lr = 0.001
I0501 06:14:22.917829 2050462464 solver.cpp:189] Iteration 4360, loss = 0.00346855
I0501 06:14:22.917877 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00346859 (* 1 = 0.00346859 loss)
I0501 06:14:22.917886 2050462464 solver.cpp:464] Iteration 4360, lr = 0.001
I0501 06:15:46.819078 2050462464 solver.cpp:189] Iteration 4380, loss = 0.00548418
I0501 06:15:46.819126 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00548421 (* 1 = 0.00548421 loss)
I0501 06:15:46.819136 2050462464 solver.cpp:464] Iteration 4380, lr = 0.001
I0501 06:17:10.623590 2050462464 solver.cpp:189] Iteration 4400, loss = 0.000862502
I0501 06:17:10.623664 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000862545 (* 1 = 0.000862545 loss)
I0501 06:17:10.623675 2050462464 solver.cpp:464] Iteration 4400, lr = 0.001
I0501 06:18:34.022089 2050462464 solver.cpp:189] Iteration 4420, loss = 0.0203411
I0501 06:18:34.022137 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0203412 (* 1 = 0.0203412 loss)
I0501 06:18:34.022147 2050462464 solver.cpp:464] Iteration 4420, lr = 0.001
I0501 06:19:57.655886 2050462464 solver.cpp:189] Iteration 4440, loss = 0.0397054
I0501 06:19:57.655939 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0397054 (* 1 = 0.0397054 loss)
I0501 06:19:57.655948 2050462464 solver.cpp:464] Iteration 4440, lr = 0.001
I0501 06:21:21.437753 2050462464 solver.cpp:189] Iteration 4460, loss = 0.00211471
I0501 06:21:21.437803 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00211475 (* 1 = 0.00211475 loss)
I0501 06:21:21.437813 2050462464 solver.cpp:464] Iteration 4460, lr = 0.001
I0501 06:22:44.781433 2050462464 solver.cpp:189] Iteration 4480, loss = 0.00428446
I0501 06:22:44.781487 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0042845 (* 1 = 0.0042845 loss)
I0501 06:22:44.781497 2050462464 solver.cpp:464] Iteration 4480, lr = 0.001
I0501 06:24:04.427760 2050462464 solver.cpp:334] Snapshotting to caffenet_train_iter_4500.caffemodel
I0501 06:24:05.911996 2050462464 solver.cpp:342] Snapshotting solver state to caffenet_train_iter_4500.solverstate
I0501 06:24:11.064734 2050462464 solver.cpp:189] Iteration 4500, loss = 0.00101011
I0501 06:24:11.064774 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00101016 (* 1 = 0.00101016 loss)
I0501 06:24:11.064784 2050462464 solver.cpp:464] Iteration 4500, lr = 0.001
I0501 06:25:34.745800 2050462464 solver.cpp:189] Iteration 4520, loss = 0.00253329
I0501 06:25:34.745852 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00253333 (* 1 = 0.00253333 loss)
I0501 06:25:34.745862 2050462464 solver.cpp:464] Iteration 4520, lr = 0.001
I0501 06:26:58.367086 2050462464 solver.cpp:189] Iteration 4540, loss = 0.00588252
I0501 06:26:58.367136 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00588257 (* 1 = 0.00588257 loss)
I0501 06:26:58.367146 2050462464 solver.cpp:464] Iteration 4540, lr = 0.001
I0501 06:28:21.764952 2050462464 solver.cpp:189] Iteration 4560, loss = 2.07415e-05
I0501 06:28:21.765010 2050462464 solver.cpp:204]     Train net output #0: loss = 2.07859e-05 (* 1 = 2.07859e-05 loss)
I0501 06:28:21.765020 2050462464 solver.cpp:464] Iteration 4560, lr = 0.001
I0501 06:29:45.161022 2050462464 solver.cpp:189] Iteration 4580, loss = 0.000722608
I0501 06:29:45.161079 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000722653 (* 1 = 0.000722653 loss)
I0501 06:29:45.161089 2050462464 solver.cpp:464] Iteration 4580, lr = 0.001
I0501 06:31:08.451665 2050462464 solver.cpp:189] Iteration 4600, loss = 0.000256242
I0501 06:31:08.451716 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000256283 (* 1 = 0.000256283 loss)
I0501 06:31:08.451726 2050462464 solver.cpp:464] Iteration 4600, lr = 0.001
I0501 06:32:32.209161 2050462464 solver.cpp:189] Iteration 4620, loss = 2.77766e-05
I0501 06:32:32.209215 2050462464 solver.cpp:204]     Train net output #0: loss = 2.78184e-05 (* 1 = 2.78184e-05 loss)
I0501 06:32:32.209225 2050462464 solver.cpp:464] Iteration 4620, lr = 0.001
I0501 06:33:55.945673 2050462464 solver.cpp:189] Iteration 4640, loss = 0.00377383
I0501 06:33:55.945721 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00377387 (* 1 = 0.00377387 loss)
I0501 06:33:55.945730 2050462464 solver.cpp:464] Iteration 4640, lr = 0.001
I0501 06:35:19.845455 2050462464 solver.cpp:189] Iteration 4660, loss = 2.18236e-05
I0501 06:35:19.845510 2050462464 solver.cpp:204]     Train net output #0: loss = 2.18655e-05 (* 1 = 2.18655e-05 loss)
I0501 06:35:19.845520 2050462464 solver.cpp:464] Iteration 4660, lr = 0.001
I0501 06:36:43.325597 2050462464 solver.cpp:189] Iteration 4680, loss = 0.0414276
I0501 06:36:43.325669 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0414276 (* 1 = 0.0414276 loss)
I0501 06:36:43.325678 2050462464 solver.cpp:464] Iteration 4680, lr = 0.001
I0501 06:38:06.671345 2050462464 solver.cpp:189] Iteration 4700, loss = 0.000752208
I0501 06:38:06.671401 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000752252 (* 1 = 0.000752252 loss)
I0501 06:38:06.671411 2050462464 solver.cpp:464] Iteration 4700, lr = 0.001
I0501 06:39:30.447286 2050462464 solver.cpp:189] Iteration 4720, loss = 3.69698e-05
I0501 06:39:30.447340 2050462464 solver.cpp:204]     Train net output #0: loss = 3.70165e-05 (* 1 = 3.70165e-05 loss)
I0501 06:39:30.447350 2050462464 solver.cpp:464] Iteration 4720, lr = 0.001
I0501 06:40:54.173094 2050462464 solver.cpp:189] Iteration 4740, loss = 0.0129501
I0501 06:40:54.173148 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0129502 (* 1 = 0.0129502 loss)
I0501 06:40:54.173158 2050462464 solver.cpp:464] Iteration 4740, lr = 0.001
I0501 06:42:17.605149 2050462464 solver.cpp:189] Iteration 4760, loss = 0.0824935
I0501 06:42:17.605288 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0824935 (* 1 = 0.0824935 loss)
I0501 06:42:17.605299 2050462464 solver.cpp:464] Iteration 4760, lr = 0.001
I0501 06:43:41.234426 2050462464 solver.cpp:189] Iteration 4780, loss = 0.000478929
I0501 06:43:41.234477 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000478974 (* 1 = 0.000478974 loss)
I0501 06:43:41.234488 2050462464 solver.cpp:464] Iteration 4780, lr = 0.001
I0501 06:45:04.447866 2050462464 solver.cpp:189] Iteration 4800, loss = 0.0142926
I0501 06:45:04.447921 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0142927 (* 1 = 0.0142927 loss)
I0501 06:45:04.447931 2050462464 solver.cpp:464] Iteration 4800, lr = 0.001
I0501 06:46:27.841172 2050462464 solver.cpp:189] Iteration 4820, loss = 0.000890985
I0501 06:46:27.841228 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000891028 (* 1 = 0.000891028 loss)
I0501 06:46:27.841238 2050462464 solver.cpp:464] Iteration 4820, lr = 0.001
I0501 06:47:51.335069 2050462464 solver.cpp:189] Iteration 4840, loss = 0.00185413
I0501 06:47:51.335125 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00185417 (* 1 = 0.00185417 loss)
I0501 06:47:51.335135 2050462464 solver.cpp:464] Iteration 4840, lr = 0.001
I0501 06:49:15.716794 2050462464 solver.cpp:189] Iteration 4860, loss = 0.0031549
I0501 06:49:15.716840 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00315495 (* 1 = 0.00315495 loss)
I0501 06:49:15.716850 2050462464 solver.cpp:464] Iteration 4860, lr = 0.001
I0501 06:50:39.398203 2050462464 solver.cpp:189] Iteration 4880, loss = 0.0342165
I0501 06:50:39.398252 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0342166 (* 1 = 0.0342166 loss)
I0501 06:50:39.398262 2050462464 solver.cpp:464] Iteration 4880, lr = 0.001
I0501 06:52:02.972317 2050462464 solver.cpp:189] Iteration 4900, loss = 0.00701001
I0501 06:52:02.972371 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00701006 (* 1 = 0.00701006 loss)
I0501 06:52:02.972381 2050462464 solver.cpp:464] Iteration 4900, lr = 0.001
I0501 06:53:26.405683 2050462464 solver.cpp:189] Iteration 4920, loss = 0.00036029
I0501 06:53:26.405728 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000360339 (* 1 = 0.000360339 loss)
I0501 06:53:26.405738 2050462464 solver.cpp:464] Iteration 4920, lr = 0.001
I0501 06:54:50.141790 2050462464 solver.cpp:189] Iteration 4940, loss = 0.00206976
I0501 06:54:50.141845 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0020698 (* 1 = 0.0020698 loss)
I0501 06:54:50.141855 2050462464 solver.cpp:464] Iteration 4940, lr = 0.001
I0501 06:56:13.604179 2050462464 solver.cpp:189] Iteration 4960, loss = 5.12019e-06
I0501 06:56:13.604255 2050462464 solver.cpp:204]     Train net output #0: loss = 5.16827e-06 (* 1 = 5.16827e-06 loss)
I0501 06:56:13.604265 2050462464 solver.cpp:464] Iteration 4960, lr = 0.001
I0501 06:57:37.064443 2050462464 solver.cpp:189] Iteration 4980, loss = 0.0473041
I0501 06:57:37.064489 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0473041 (* 1 = 0.0473041 loss)
I0501 06:57:37.064499 2050462464 solver.cpp:464] Iteration 4980, lr = 0.001
I0501 06:58:56.790278 2050462464 solver.cpp:334] Snapshotting to caffenet_train_iter_5000.caffemodel
I0501 06:58:58.283336 2050462464 solver.cpp:342] Snapshotting solver state to caffenet_train_iter_5000.solverstate
I0501 06:58:59.544586 2050462464 solver.cpp:266] Iteration 5000, Testing net (#0)
I0501 07:26:35.443742 2050462464 solver.cpp:315]     Test net output #0: accuracy = 0.952422
I0501 07:26:35.443783 2050462464 solver.cpp:315]     Test net output #1: loss = 0.279536 (* 1 = 0.279536 loss)
I0501 07:26:39.412400 2050462464 solver.cpp:189] Iteration 5000, loss = 0.000169626
I0501 07:26:39.412439 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000169674 (* 1 = 0.000169674 loss)
I0501 07:26:39.412449 2050462464 solver.cpp:464] Iteration 5000, lr = 0.001
I0501 07:28:02.640496 2050462464 solver.cpp:189] Iteration 5020, loss = 0.000824102
I0501 07:28:02.640545 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000824152 (* 1 = 0.000824152 loss)
I0501 07:28:02.640555 2050462464 solver.cpp:464] Iteration 5020, lr = 0.001
I0501 07:29:26.052085 2050462464 solver.cpp:189] Iteration 5040, loss = 0.00296158
I0501 07:29:26.052141 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00296163 (* 1 = 0.00296163 loss)
I0501 07:29:26.052151 2050462464 solver.cpp:464] Iteration 5040, lr = 0.001
I0501 07:30:49.368449 2050462464 solver.cpp:189] Iteration 5060, loss = 0.00202454
I0501 07:30:49.368501 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00202458 (* 1 = 0.00202458 loss)
I0501 07:30:49.368512 2050462464 solver.cpp:464] Iteration 5060, lr = 0.001
I0501 07:32:12.461558 2050462464 solver.cpp:189] Iteration 5080, loss = 0.0584399
I0501 07:32:12.461612 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0584399 (* 1 = 0.0584399 loss)
I0501 07:32:12.461622 2050462464 solver.cpp:464] Iteration 5080, lr = 0.001
I0501 07:33:36.666543 2050462464 solver.cpp:189] Iteration 5100, loss = 0.0624717
I0501 07:33:36.666591 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0624718 (* 1 = 0.0624718 loss)
I0501 07:33:36.666601 2050462464 solver.cpp:464] Iteration 5100, lr = 0.001
I0501 07:35:00.143077 2050462464 solver.cpp:189] Iteration 5120, loss = 0.000797136
I0501 07:35:00.143123 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000797179 (* 1 = 0.000797179 loss)
I0501 07:35:00.143133 2050462464 solver.cpp:464] Iteration 5120, lr = 0.001
I0501 07:36:23.532703 2050462464 solver.cpp:189] Iteration 5140, loss = 0.00450305
I0501 07:36:23.532754 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00450309 (* 1 = 0.00450309 loss)
I0501 07:36:23.532764 2050462464 solver.cpp:464] Iteration 5140, lr = 0.001
I0501 07:37:47.172976 2050462464 solver.cpp:189] Iteration 5160, loss = 0.0555159
I0501 07:37:47.173023 2050462464 solver.cpp:204]     Train net output #0: loss = 0.055516 (* 1 = 0.055516 loss)
I0501 07:37:47.173032 2050462464 solver.cpp:464] Iteration 5160, lr = 0.001
I0501 07:39:10.445549 2050462464 solver.cpp:189] Iteration 5180, loss = 0.0224369
I0501 07:39:10.445602 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0224369 (* 1 = 0.0224369 loss)
I0501 07:39:10.445611 2050462464 solver.cpp:464] Iteration 5180, lr = 0.001
I0501 07:40:33.686008 2050462464 solver.cpp:189] Iteration 5200, loss = 0.0349848
I0501 07:40:33.686058 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0349848 (* 1 = 0.0349848 loss)
I0501 07:40:33.686069 2050462464 solver.cpp:464] Iteration 5200, lr = 0.001
I0501 07:41:56.920743 2050462464 solver.cpp:189] Iteration 5220, loss = 0.00336942
I0501 07:41:56.920796 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00336946 (* 1 = 0.00336946 loss)
I0501 07:41:56.920806 2050462464 solver.cpp:464] Iteration 5220, lr = 0.001
I0501 07:43:19.885110 2050462464 solver.cpp:189] Iteration 5240, loss = 0.000186316
I0501 07:43:19.886016 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000186361 (* 1 = 0.000186361 loss)
I0501 07:43:19.886029 2050462464 solver.cpp:464] Iteration 5240, lr = 0.001
I0501 07:44:43.604539 2050462464 solver.cpp:189] Iteration 5260, loss = 2.24001e-05
I0501 07:44:43.604584 2050462464 solver.cpp:204]     Train net output #0: loss = 2.24458e-05 (* 1 = 2.24458e-05 loss)
I0501 07:44:43.604594 2050462464 solver.cpp:464] Iteration 5260, lr = 0.001
I0501 07:46:07.087308 2050462464 solver.cpp:189] Iteration 5280, loss = 9.20368e-05
I0501 07:46:07.087359 2050462464 solver.cpp:204]     Train net output #0: loss = 9.20824e-05 (* 1 = 9.20824e-05 loss)
I0501 07:46:07.087368 2050462464 solver.cpp:464] Iteration 5280, lr = 0.001
I0501 07:47:31.022984 2050462464 solver.cpp:189] Iteration 5300, loss = 0.00829853
I0501 07:47:31.023031 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00829857 (* 1 = 0.00829857 loss)
I0501 07:47:31.023041 2050462464 solver.cpp:464] Iteration 5300, lr = 0.001
I0501 07:48:54.073930 2050462464 solver.cpp:189] Iteration 5320, loss = 0.000745712
I0501 07:48:54.073977 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000745762 (* 1 = 0.000745762 loss)
I0501 07:48:54.073987 2050462464 solver.cpp:464] Iteration 5320, lr = 0.001
I0501 07:50:17.820899 2050462464 solver.cpp:189] Iteration 5340, loss = 0.00798576
I0501 07:50:17.820950 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00798581 (* 1 = 0.00798581 loss)
I0501 07:50:17.820960 2050462464 solver.cpp:464] Iteration 5340, lr = 0.001
I0501 07:51:41.255949 2050462464 solver.cpp:189] Iteration 5360, loss = 0.00445784
I0501 07:51:41.256003 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00445789 (* 1 = 0.00445789 loss)
I0501 07:51:41.256014 2050462464 solver.cpp:464] Iteration 5360, lr = 0.001
I0501 07:53:04.649082 2050462464 solver.cpp:189] Iteration 5380, loss = 0.0192802
I0501 07:53:04.649129 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0192803 (* 1 = 0.0192803 loss)
I0501 07:53:04.649138 2050462464 solver.cpp:464] Iteration 5380, lr = 0.001
I0501 07:54:28.119784 2050462464 solver.cpp:189] Iteration 5400, loss = 0.00868445
I0501 07:54:28.119837 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00868451 (* 1 = 0.00868451 loss)
I0501 07:54:28.119846 2050462464 solver.cpp:464] Iteration 5400, lr = 0.001
I0501 07:55:51.756110 2050462464 solver.cpp:189] Iteration 5420, loss = 0.0769128
I0501 07:55:51.756162 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0769129 (* 1 = 0.0769129 loss)
I0501 07:55:51.756172 2050462464 solver.cpp:464] Iteration 5420, lr = 0.001
I0501 07:57:15.364420 2050462464 solver.cpp:189] Iteration 5440, loss = 0.0019399
I0501 07:57:15.364473 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00193997 (* 1 = 0.00193997 loss)
I0501 07:57:15.364483 2050462464 solver.cpp:464] Iteration 5440, lr = 0.001
I0501 07:58:38.929738 2050462464 solver.cpp:189] Iteration 5460, loss = 0.000493857
I0501 07:58:38.929785 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000493931 (* 1 = 0.000493931 loss)
I0501 07:58:38.929793 2050462464 solver.cpp:464] Iteration 5460, lr = 0.001
I0501 08:00:02.471426 2050462464 solver.cpp:189] Iteration 5480, loss = 0.00816155
I0501 08:00:02.471482 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00816163 (* 1 = 0.00816163 loss)
I0501 08:00:02.471492 2050462464 solver.cpp:464] Iteration 5480, lr = 0.001
I0501 08:01:22.306586 2050462464 solver.cpp:334] Snapshotting to caffenet_train_iter_5500.caffemodel
I0501 08:01:23.766690 2050462464 solver.cpp:342] Snapshotting solver state to caffenet_train_iter_5500.solverstate
I0501 08:01:28.994133 2050462464 solver.cpp:189] Iteration 5500, loss = 0.000255724
I0501 08:01:28.994171 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000255805 (* 1 = 0.000255805 loss)
I0501 08:01:28.994181 2050462464 solver.cpp:464] Iteration 5500, lr = 0.001
I0501 08:02:52.543298 2050462464 solver.cpp:189] Iteration 5520, loss = 0.0519186
I0501 08:02:52.543372 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0519186 (* 1 = 0.0519186 loss)
I0501 08:02:52.543382 2050462464 solver.cpp:464] Iteration 5520, lr = 0.001
I0501 08:04:16.496887 2050462464 solver.cpp:189] Iteration 5540, loss = 0.000231545
I0501 08:04:16.496932 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000231632 (* 1 = 0.000231632 loss)
I0501 08:04:16.496942 2050462464 solver.cpp:464] Iteration 5540, lr = 0.001
I0501 08:05:40.173184 2050462464 solver.cpp:189] Iteration 5560, loss = 1.33827e-06
I0501 08:05:40.173238 2050462464 solver.cpp:204]     Train net output #0: loss = 1.42494e-06 (* 1 = 1.42494e-06 loss)
I0501 08:05:40.173249 2050462464 solver.cpp:464] Iteration 5560, lr = 0.001
I0501 08:07:03.793361 2050462464 solver.cpp:189] Iteration 5580, loss = 4.38697e-07
I0501 08:07:03.793406 2050462464 solver.cpp:204]     Train net output #0: loss = 5.2527e-07 (* 1 = 5.2527e-07 loss)
I0501 08:07:03.793416 2050462464 solver.cpp:464] Iteration 5580, lr = 0.001
I0501 08:08:27.395889 2050462464 solver.cpp:189] Iteration 5600, loss = -1.43973e-08
I0501 08:08:27.395941 2050462464 solver.cpp:204]     Train net output #0: loss = 7.26432e-08 (* 1 = 7.26432e-08 loss)
I0501 08:08:27.395951 2050462464 solver.cpp:464] Iteration 5600, lr = 0.001
I0501 08:09:51.016126 2050462464 solver.cpp:189] Iteration 5620, loss = 0.00226308
I0501 08:09:51.016175 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00226316 (* 1 = 0.00226316 loss)
I0501 08:09:51.016185 2050462464 solver.cpp:464] Iteration 5620, lr = 0.001
I0501 08:11:14.876894 2050462464 solver.cpp:189] Iteration 5640, loss = 4.82042e-05
I0501 08:11:14.876942 2050462464 solver.cpp:204]     Train net output #0: loss = 4.82901e-05 (* 1 = 4.82901e-05 loss)
I0501 08:11:14.876951 2050462464 solver.cpp:464] Iteration 5640, lr = 0.001
I0501 08:12:38.397742 2050462464 solver.cpp:189] Iteration 5660, loss = 0.000291656
I0501 08:12:38.397790 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000291745 (* 1 = 0.000291745 loss)
I0501 08:12:38.397800 2050462464 solver.cpp:464] Iteration 5660, lr = 0.001
I0501 08:14:01.897958 2050462464 solver.cpp:189] Iteration 5680, loss = 0.00252247
I0501 08:14:01.898007 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00252256 (* 1 = 0.00252256 loss)
I0501 08:14:01.898016 2050462464 solver.cpp:464] Iteration 5680, lr = 0.001
I0501 08:15:25.108084 2050462464 solver.cpp:189] Iteration 5700, loss = 0.00286566
I0501 08:15:25.108131 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00286575 (* 1 = 0.00286575 loss)
I0501 08:15:25.108140 2050462464 solver.cpp:464] Iteration 5700, lr = 0.001
I0501 08:16:48.555050 2050462464 solver.cpp:189] Iteration 5720, loss = 0.00318409
I0501 08:16:48.555099 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00318418 (* 1 = 0.00318418 loss)
I0501 08:16:48.555109 2050462464 solver.cpp:464] Iteration 5720, lr = 0.001
I0501 08:18:12.006731 2050462464 solver.cpp:189] Iteration 5740, loss = 0.0162111
I0501 08:18:12.006786 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0162112 (* 1 = 0.0162112 loss)
I0501 08:18:12.006795 2050462464 solver.cpp:464] Iteration 5740, lr = 0.001
I0501 08:19:35.553908 2050462464 solver.cpp:189] Iteration 5760, loss = 0.00625836
I0501 08:19:35.553961 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00625845 (* 1 = 0.00625845 loss)
I0501 08:19:35.553971 2050462464 solver.cpp:464] Iteration 5760, lr = 0.001
I0501 08:20:59.210415 2050462464 solver.cpp:189] Iteration 5780, loss = 0.00381213
I0501 08:20:59.210471 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00381223 (* 1 = 0.00381223 loss)
I0501 08:20:59.210485 2050462464 solver.cpp:464] Iteration 5780, lr = 0.001
I0501 08:22:22.979470 2050462464 solver.cpp:189] Iteration 5800, loss = 0.000131424
I0501 08:22:22.979523 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000131518 (* 1 = 0.000131518 loss)
I0501 08:22:22.979533 2050462464 solver.cpp:464] Iteration 5800, lr = 0.001
I0501 08:23:46.695196 2050462464 solver.cpp:189] Iteration 5820, loss = 0.00128976
I0501 08:23:46.695269 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00128985 (* 1 = 0.00128985 loss)
I0501 08:23:46.695279 2050462464 solver.cpp:464] Iteration 5820, lr = 0.001
I0501 08:25:10.331027 2050462464 solver.cpp:189] Iteration 5840, loss = 0.00319939
I0501 08:25:10.331074 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00319948 (* 1 = 0.00319948 loss)
I0501 08:25:10.331084 2050462464 solver.cpp:464] Iteration 5840, lr = 0.001
I0501 08:26:33.627432 2050462464 solver.cpp:189] Iteration 5860, loss = 0.00228503
I0501 08:26:33.627481 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00228512 (* 1 = 0.00228512 loss)
I0501 08:26:33.627491 2050462464 solver.cpp:464] Iteration 5860, lr = 0.001
I0501 08:27:57.054669 2050462464 solver.cpp:189] Iteration 5880, loss = 8.74681e-05
I0501 08:27:57.054716 2050462464 solver.cpp:204]     Train net output #0: loss = 8.75594e-05 (* 1 = 8.75594e-05 loss)
I0501 08:27:57.054726 2050462464 solver.cpp:464] Iteration 5880, lr = 0.001
I0501 08:29:20.888808 2050462464 solver.cpp:189] Iteration 5900, loss = 2.17585e-05
I0501 08:29:20.888857 2050462464 solver.cpp:204]     Train net output #0: loss = 2.18506e-05 (* 1 = 2.18506e-05 loss)
I0501 08:29:20.888867 2050462464 solver.cpp:464] Iteration 5900, lr = 0.001
I0501 08:30:44.869246 2050462464 solver.cpp:189] Iteration 5920, loss = 0.000274607
I0501 08:30:44.869294 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000274699 (* 1 = 0.000274699 loss)
I0501 08:30:44.869304 2050462464 solver.cpp:464] Iteration 5920, lr = 0.001
I0501 08:32:08.347491 2050462464 solver.cpp:189] Iteration 5940, loss = 0.0607436
I0501 08:32:08.347548 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0607436 (* 1 = 0.0607436 loss)
I0501 08:32:08.347558 2050462464 solver.cpp:464] Iteration 5940, lr = 0.001
I0501 08:33:32.128422 2050462464 solver.cpp:189] Iteration 5960, loss = 3.15476e-05
I0501 08:33:32.128476 2050462464 solver.cpp:204]     Train net output #0: loss = 3.16385e-05 (* 1 = 3.16385e-05 loss)
I0501 08:33:32.128486 2050462464 solver.cpp:464] Iteration 5960, lr = 0.001
I0501 08:34:55.479909 2050462464 solver.cpp:189] Iteration 5980, loss = 0.000588256
I0501 08:34:55.479964 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000588347 (* 1 = 0.000588347 loss)
I0501 08:34:55.479972 2050462464 solver.cpp:464] Iteration 5980, lr = 0.001
I0501 08:36:15.283880 2050462464 solver.cpp:334] Snapshotting to caffenet_train_iter_6000.caffemodel
I0501 08:36:16.745239 2050462464 solver.cpp:342] Snapshotting solver state to caffenet_train_iter_6000.solverstate
I0501 08:36:17.943753 2050462464 solver.cpp:266] Iteration 6000, Testing net (#0)
I0501 09:33:34.527951 2050462464 solver.cpp:315]     Test net output #0: accuracy = 0.970891
I0501 09:33:34.528923 2050462464 solver.cpp:315]     Test net output #1: loss = 0.132656 (* 1 = 0.132656 loss)
I0501 09:33:38.572357 2050462464 solver.cpp:189] Iteration 6000, loss = 0.00169877
I0501 09:33:38.572399 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00169886 (* 1 = 0.00169886 loss)
I0501 09:33:38.572408 2050462464 solver.cpp:464] Iteration 6000, lr = 0.001
I0501 09:35:07.684965 2050462464 solver.cpp:189] Iteration 6020, loss = 0.00482749
I0501 09:35:07.685015 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00482758 (* 1 = 0.00482758 loss)
I0501 09:35:07.685024 2050462464 solver.cpp:464] Iteration 6020, lr = 0.001
I0501 09:36:38.248214 2050462464 solver.cpp:189] Iteration 6040, loss = 0.011378
I0501 09:36:38.248268 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0113781 (* 1 = 0.0113781 loss)
I0501 09:36:38.248278 2050462464 solver.cpp:464] Iteration 6040, lr = 0.001
I0501 09:38:11.236233 2050462464 solver.cpp:189] Iteration 6060, loss = 0.00105941
I0501 09:38:11.236284 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0010595 (* 1 = 0.0010595 loss)
I0501 09:38:11.236294 2050462464 solver.cpp:464] Iteration 6060, lr = 0.001
I0501 09:39:44.562477 2050462464 solver.cpp:189] Iteration 6080, loss = 0.000836371
I0501 09:39:44.563251 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000836465 (* 1 = 0.000836465 loss)
I0501 09:39:44.563266 2050462464 solver.cpp:464] Iteration 6080, lr = 0.001
I0501 09:41:22.905475 2050462464 solver.cpp:189] Iteration 6100, loss = 0.00103834
I0501 09:41:22.906255 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00103843 (* 1 = 0.00103843 loss)
I0501 09:41:22.906271 2050462464 solver.cpp:464] Iteration 6100, lr = 0.001
I0501 09:42:55.780680 2050462464 solver.cpp:189] Iteration 6120, loss = 0.00142438
I0501 09:42:55.781000 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00142447 (* 1 = 0.00142447 loss)
I0501 09:42:55.781013 2050462464 solver.cpp:464] Iteration 6120, lr = 0.001
I0501 09:44:27.028329 2050462464 solver.cpp:189] Iteration 6140, loss = 0.000154917
I0501 09:44:27.028383 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000155008 (* 1 = 0.000155008 loss)
I0501 09:44:27.028393 2050462464 solver.cpp:464] Iteration 6140, lr = 0.001
I0501 09:46:00.182939 2050462464 solver.cpp:189] Iteration 6160, loss = 8.33657e-06
I0501 09:46:00.182996 2050462464 solver.cpp:204]     Train net output #0: loss = 8.42652e-06 (* 1 = 8.42652e-06 loss)
I0501 09:46:00.183006 2050462464 solver.cpp:464] Iteration 6160, lr = 0.001
I0501 09:47:28.632431 2050462464 solver.cpp:189] Iteration 6180, loss = 0.000254232
I0501 09:47:28.632480 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000254322 (* 1 = 0.000254322 loss)
I0501 09:47:28.632490 2050462464 solver.cpp:464] Iteration 6180, lr = 0.001
I0501 09:49:02.933003 2050462464 solver.cpp:189] Iteration 6200, loss = 0.000200712
I0501 09:49:02.933357 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000200803 (* 1 = 0.000200803 loss)
I0501 09:49:02.933372 2050462464 solver.cpp:464] Iteration 6200, lr = 0.001
I0501 09:50:30.883600 2050462464 solver.cpp:189] Iteration 6220, loss = 2.41495e-06
I0501 09:50:30.883656 2050462464 solver.cpp:204]     Train net output #0: loss = 2.50539e-06 (* 1 = 2.50539e-06 loss)
I0501 09:50:30.883671 2050462464 solver.cpp:464] Iteration 6220, lr = 0.001
I0501 09:51:58.273500 2050462464 solver.cpp:189] Iteration 6240, loss = 0.0061674
I0501 09:51:58.273553 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00616749 (* 1 = 0.00616749 loss)
I0501 09:51:58.273568 2050462464 solver.cpp:464] Iteration 6240, lr = 0.001
I0501 09:53:26.111259 2050462464 solver.cpp:189] Iteration 6260, loss = 1.50731e-05
I0501 09:53:26.111313 2050462464 solver.cpp:204]     Train net output #0: loss = 1.51667e-05 (* 1 = 1.51667e-05 loss)
I0501 09:53:26.111323 2050462464 solver.cpp:464] Iteration 6260, lr = 0.001
I0501 09:54:57.674751 2050462464 solver.cpp:189] Iteration 6280, loss = 0.00239532
I0501 09:54:57.674819 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00239541 (* 1 = 0.00239541 loss)
I0501 09:54:57.674835 2050462464 solver.cpp:464] Iteration 6280, lr = 0.001
I0501 09:56:35.704936 2050462464 solver.cpp:189] Iteration 6300, loss = 0.0106843
I0501 09:56:35.705663 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0106844 (* 1 = 0.0106844 loss)
I0501 09:56:35.705687 2050462464 solver.cpp:464] Iteration 6300, lr = 0.001
I0501 09:58:07.609457 2050462464 solver.cpp:189] Iteration 6320, loss = 0.0443112
I0501 09:58:07.610252 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0443113 (* 1 = 0.0443113 loss)
I0501 09:58:07.610265 2050462464 solver.cpp:464] Iteration 6320, lr = 0.001
I0501 09:59:40.841548 2050462464 solver.cpp:189] Iteration 6340, loss = 0.0172207
I0501 09:59:40.842438 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0172208 (* 1 = 0.0172208 loss)
I0501 09:59:40.842452 2050462464 solver.cpp:464] Iteration 6340, lr = 0.001
I0501 10:01:08.077014 2050462464 solver.cpp:189] Iteration 6360, loss = 0.00887517
I0501 10:01:08.077082 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00887525 (* 1 = 0.00887525 loss)
I0501 10:01:08.077093 2050462464 solver.cpp:464] Iteration 6360, lr = 0.001
I0501 10:02:38.261705 2050462464 solver.cpp:189] Iteration 6380, loss = 0.00942235
I0501 10:02:38.262663 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00942244 (* 1 = 0.00942244 loss)
I0501 10:02:38.262692 2050462464 solver.cpp:464] Iteration 6380, lr = 0.001
I0501 10:04:04.015584 2050462464 solver.cpp:189] Iteration 6400, loss = 0.000407776
I0501 10:04:04.015641 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000407867 (* 1 = 0.000407867 loss)
I0501 10:04:04.015651 2050462464 solver.cpp:464] Iteration 6400, lr = 0.001
I0501 10:05:32.968737 2050462464 solver.cpp:189] Iteration 6420, loss = 0.00301199
I0501 10:05:32.968787 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00301208 (* 1 = 0.00301208 loss)
I0501 10:05:32.968797 2050462464 solver.cpp:464] Iteration 6420, lr = 0.001
I0501 10:07:00.620985 2050462464 solver.cpp:189] Iteration 6440, loss = 0.0383639
I0501 10:07:00.621034 2050462464 solver.cpp:204]     Train net output #0: loss = 0.038364 (* 1 = 0.038364 loss)
I0501 10:07:00.621043 2050462464 solver.cpp:464] Iteration 6440, lr = 0.001
I0501 10:08:26.304764 2050462464 solver.cpp:189] Iteration 6460, loss = 0.000615544
I0501 10:08:26.304821 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000615637 (* 1 = 0.000615637 loss)
I0501 10:08:26.304831 2050462464 solver.cpp:464] Iteration 6460, lr = 0.001
I0501 10:09:55.570247 2050462464 solver.cpp:189] Iteration 6480, loss = 0.00212673
I0501 10:09:55.570641 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00212682 (* 1 = 0.00212682 loss)
I0501 10:09:55.570657 2050462464 solver.cpp:464] Iteration 6480, lr = 0.001
I0501 10:11:17.653904 2050462464 solver.cpp:334] Snapshotting to caffenet_train_iter_6500.caffemodel
I0501 10:11:19.311717 2050462464 solver.cpp:342] Snapshotting solver state to caffenet_train_iter_6500.solverstate
I0501 10:11:24.511180 2050462464 solver.cpp:189] Iteration 6500, loss = 0.000133875
I0501 10:11:24.511219 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000133973 (* 1 = 0.000133973 loss)
I0501 10:11:24.511227 2050462464 solver.cpp:464] Iteration 6500, lr = 0.001
I0501 10:12:48.353819 2050462464 solver.cpp:189] Iteration 6520, loss = 1.03121e-05
I0501 10:12:48.353868 2050462464 solver.cpp:204]     Train net output #0: loss = 1.04089e-05 (* 1 = 1.04089e-05 loss)
I0501 10:12:48.353878 2050462464 solver.cpp:464] Iteration 6520, lr = 0.001
I0501 10:14:12.294983 2050462464 solver.cpp:189] Iteration 6540, loss = 1.48779e-05
I0501 10:14:12.295130 2050462464 solver.cpp:204]     Train net output #0: loss = 1.49748e-05 (* 1 = 1.49748e-05 loss)
I0501 10:14:12.295141 2050462464 solver.cpp:464] Iteration 6540, lr = 0.001
I0501 10:15:36.781584 2050462464 solver.cpp:189] Iteration 6560, loss = 0.0344589
I0501 10:15:36.781640 2050462464 solver.cpp:204]     Train net output #0: loss = 0.034459 (* 1 = 0.034459 loss)
I0501 10:15:36.781648 2050462464 solver.cpp:464] Iteration 6560, lr = 0.001
I0501 10:17:03.044631 2050462464 solver.cpp:189] Iteration 6580, loss = 3.32312e-05
I0501 10:17:03.044687 2050462464 solver.cpp:204]     Train net output #0: loss = 3.33289e-05 (* 1 = 3.33289e-05 loss)
I0501 10:17:03.044697 2050462464 solver.cpp:464] Iteration 6580, lr = 0.001
I0501 10:18:27.705025 2050462464 solver.cpp:189] Iteration 6600, loss = 0.00335719
I0501 10:18:27.705082 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00335729 (* 1 = 0.00335729 loss)
I0501 10:18:27.705092 2050462464 solver.cpp:464] Iteration 6600, lr = 0.001
I0501 10:19:52.680003 2050462464 solver.cpp:189] Iteration 6620, loss = 0.0165843
I0501 10:19:52.680058 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0165844 (* 1 = 0.0165844 loss)
I0501 10:19:52.680068 2050462464 solver.cpp:464] Iteration 6620, lr = 0.001
I0501 10:21:18.024818 2050462464 solver.cpp:189] Iteration 6640, loss = 0.000750582
I0501 10:21:18.024895 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000750674 (* 1 = 0.000750674 loss)
I0501 10:21:18.024905 2050462464 solver.cpp:464] Iteration 6640, lr = 0.001
I0501 10:22:43.014164 2050462464 solver.cpp:189] Iteration 6660, loss = 0.00413408
I0501 10:22:43.014219 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00413417 (* 1 = 0.00413417 loss)
I0501 10:22:43.014228 2050462464 solver.cpp:464] Iteration 6660, lr = 0.001
I0501 10:24:08.618716 2050462464 solver.cpp:189] Iteration 6680, loss = 0.000808025
I0501 10:24:08.618770 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000808122 (* 1 = 0.000808122 loss)
I0501 10:24:08.618779 2050462464 solver.cpp:464] Iteration 6680, lr = 0.001
I0501 10:25:34.083252 2050462464 solver.cpp:189] Iteration 6700, loss = 0.00380511
I0501 10:25:34.083302 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00380522 (* 1 = 0.00380522 loss)
I0501 10:25:34.083317 2050462464 solver.cpp:464] Iteration 6700, lr = 0.001
I0501 10:27:03.214767 2050462464 solver.cpp:189] Iteration 6720, loss = 3.25902e-05
I0501 10:27:03.214818 2050462464 solver.cpp:204]     Train net output #0: loss = 3.26908e-05 (* 1 = 3.26908e-05 loss)
I0501 10:27:03.214828 2050462464 solver.cpp:464] Iteration 6720, lr = 0.001
I0501 10:28:32.518683 2050462464 solver.cpp:189] Iteration 6740, loss = 0.00138324
I0501 10:28:32.518738 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00138334 (* 1 = 0.00138334 loss)
I0501 10:28:32.518748 2050462464 solver.cpp:464] Iteration 6740, lr = 0.001
I0501 10:30:00.414844 2050462464 solver.cpp:189] Iteration 6760, loss = 0.00191608
I0501 10:30:00.414901 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00191618 (* 1 = 0.00191618 loss)
I0501 10:30:00.414911 2050462464 solver.cpp:464] Iteration 6760, lr = 0.001
I0501 10:31:27.477895 2050462464 solver.cpp:189] Iteration 6780, loss = 0.018247
I0501 10:31:27.477941 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0182471 (* 1 = 0.0182471 loss)
I0501 10:31:27.477949 2050462464 solver.cpp:464] Iteration 6780, lr = 0.001
I0501 10:32:56.167289 2050462464 solver.cpp:189] Iteration 6800, loss = 0.000416839
I0501 10:32:56.167357 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000416938 (* 1 = 0.000416938 loss)
I0501 10:32:56.167371 2050462464 solver.cpp:464] Iteration 6800, lr = 0.001
I0501 10:34:24.337474 2050462464 solver.cpp:189] Iteration 6820, loss = 3.37069e-05
I0501 10:34:24.337528 2050462464 solver.cpp:204]     Train net output #0: loss = 3.38057e-05 (* 1 = 3.38057e-05 loss)
I0501 10:34:24.337538 2050462464 solver.cpp:464] Iteration 6820, lr = 0.001
I0501 10:35:49.243527 2050462464 solver.cpp:189] Iteration 6840, loss = 0.00373477
I0501 10:35:49.243579 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00373487 (* 1 = 0.00373487 loss)
I0501 10:35:49.243589 2050462464 solver.cpp:464] Iteration 6840, lr = 0.001
I0501 10:37:15.184943 2050462464 solver.cpp:189] Iteration 6860, loss = 0.0114337
I0501 10:37:15.184999 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0114338 (* 1 = 0.0114338 loss)
I0501 10:37:15.185009 2050462464 solver.cpp:464] Iteration 6860, lr = 0.001
I0501 10:38:41.554961 2050462464 solver.cpp:189] Iteration 6880, loss = 0.000843976
I0501 10:38:41.555006 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000844077 (* 1 = 0.000844077 loss)
I0501 10:38:41.555016 2050462464 solver.cpp:464] Iteration 6880, lr = 0.001
I0501 10:40:07.130542 2050462464 solver.cpp:189] Iteration 6900, loss = 0.000725321
I0501 10:40:07.130596 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000725417 (* 1 = 0.000725417 loss)
I0501 10:40:07.130605 2050462464 solver.cpp:464] Iteration 6900, lr = 0.001
I0501 10:41:31.760290 2050462464 solver.cpp:189] Iteration 6920, loss = 0.0020415
I0501 10:41:31.760339 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0020416 (* 1 = 0.0020416 loss)
I0501 10:41:31.760347 2050462464 solver.cpp:464] Iteration 6920, lr = 0.001
I0501 10:42:56.633760 2050462464 solver.cpp:189] Iteration 6940, loss = 0.010001
I0501 10:42:56.633823 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0100011 (* 1 = 0.0100011 loss)
I0501 10:42:56.633833 2050462464 solver.cpp:464] Iteration 6940, lr = 0.001
I0501 10:44:21.566069 2050462464 solver.cpp:189] Iteration 6960, loss = 0.00245011
I0501 10:44:21.566117 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00245021 (* 1 = 0.00245021 loss)
I0501 10:44:21.566126 2050462464 solver.cpp:464] Iteration 6960, lr = 0.001
I0501 10:45:47.455718 2050462464 solver.cpp:189] Iteration 6980, loss = 0.00132401
I0501 10:45:47.455775 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00132412 (* 1 = 0.00132412 loss)
I0501 10:45:47.455785 2050462464 solver.cpp:464] Iteration 6980, lr = 0.001
I0501 10:47:09.647495 2050462464 solver.cpp:334] Snapshotting to caffenet_train_iter_7000.caffemodel
I0501 10:47:11.177889 2050462464 solver.cpp:342] Snapshotting solver state to caffenet_train_iter_7000.solverstate
I0501 10:47:12.395102 2050462464 solver.cpp:266] Iteration 7000, Testing net (#0)
I0501 11:19:15.122220 2050462464 solver.cpp:315]     Test net output #0: accuracy = 0.965109
I0501 11:19:15.123080 2050462464 solver.cpp:315]     Test net output #1: loss = 0.147491 (* 1 = 0.147491 loss)
I0501 11:19:19.572190 2050462464 solver.cpp:189] Iteration 7000, loss = 0.00616911
I0501 11:19:19.572226 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00616922 (* 1 = 0.00616922 loss)
I0501 11:19:19.572235 2050462464 solver.cpp:464] Iteration 7000, lr = 0.001
I0501 11:20:52.602614 2050462464 solver.cpp:189] Iteration 7020, loss = 0.0303551
I0501 11:20:52.602659 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0303552 (* 1 = 0.0303552 loss)
I0501 11:20:52.602669 2050462464 solver.cpp:464] Iteration 7020, lr = 0.001
I0501 11:22:24.330627 2050462464 solver.cpp:189] Iteration 7040, loss = 0.0241421
I0501 11:22:24.330720 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0241422 (* 1 = 0.0241422 loss)
I0501 11:22:24.330734 2050462464 solver.cpp:464] Iteration 7040, lr = 0.001
I0501 11:23:56.665482 2050462464 solver.cpp:189] Iteration 7060, loss = 7.69474e-05
I0501 11:23:56.666265 2050462464 solver.cpp:204]     Train net output #0: loss = 7.70543e-05 (* 1 = 7.70543e-05 loss)
I0501 11:23:56.666285 2050462464 solver.cpp:464] Iteration 7060, lr = 0.001
I0501 11:25:27.388923 2050462464 solver.cpp:189] Iteration 7080, loss = 0.000158304
I0501 11:25:27.388978 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000158409 (* 1 = 0.000158409 loss)
I0501 11:25:27.388988 2050462464 solver.cpp:464] Iteration 7080, lr = 0.001
I0501 11:26:57.824913 2050462464 solver.cpp:189] Iteration 7100, loss = 2.91681e-05
I0501 11:26:57.824962 2050462464 solver.cpp:204]     Train net output #0: loss = 2.9272e-05 (* 1 = 2.9272e-05 loss)
I0501 11:26:57.824976 2050462464 solver.cpp:464] Iteration 7100, lr = 0.001
I0501 11:28:28.702499 2050462464 solver.cpp:189] Iteration 7120, loss = 7.79211e-06
I0501 11:28:28.702541 2050462464 solver.cpp:204]     Train net output #0: loss = 7.89674e-06 (* 1 = 7.89674e-06 loss)
I0501 11:28:28.702551 2050462464 solver.cpp:464] Iteration 7120, lr = 0.001
I0501 11:29:59.693191 2050462464 solver.cpp:189] Iteration 7140, loss = 2.51137e-07
I0501 11:29:59.693243 2050462464 solver.cpp:204]     Train net output #0: loss = 3.55766e-07 (* 1 = 3.55766e-07 loss)
I0501 11:29:59.693254 2050462464 solver.cpp:464] Iteration 7140, lr = 0.001
I0501 11:31:30.596976 2050462464 solver.cpp:189] Iteration 7160, loss = 3.50678e-08
I0501 11:31:30.597026 2050462464 solver.cpp:204]     Train net output #0: loss = 1.39699e-07 (* 1 = 1.39699e-07 loss)
I0501 11:31:30.597053 2050462464 solver.cpp:464] Iteration 7160, lr = 0.001
I0501 11:33:03.490346 2050462464 solver.cpp:189] Iteration 7180, loss = 0.00189808
I0501 11:33:03.490401 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00189818 (* 1 = 0.00189818 loss)
I0501 11:33:03.490411 2050462464 solver.cpp:464] Iteration 7180, lr = 0.001
I0501 11:34:36.269147 2050462464 solver.cpp:189] Iteration 7200, loss = 0.00438077
I0501 11:34:36.269418 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00438088 (* 1 = 0.00438088 loss)
I0501 11:34:36.269433 2050462464 solver.cpp:464] Iteration 7200, lr = 0.001
I0501 11:36:17.069171 2050462464 solver.cpp:189] Iteration 7220, loss = 0.00418375
I0501 11:36:17.069460 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00418385 (* 1 = 0.00418385 loss)
I0501 11:36:17.069473 2050462464 solver.cpp:464] Iteration 7220, lr = 0.001
I0501 11:37:49.810451 2050462464 solver.cpp:189] Iteration 7240, loss = 5.30079e-05
I0501 11:37:49.811206 2050462464 solver.cpp:204]     Train net output #0: loss = 5.31129e-05 (* 1 = 5.31129e-05 loss)
I0501 11:37:49.811219 2050462464 solver.cpp:464] Iteration 7240, lr = 0.001
I0501 11:39:21.232633 2050462464 solver.cpp:189] Iteration 7260, loss = 0.000252002
I0501 11:39:21.233414 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000252109 (* 1 = 0.000252109 loss)
I0501 11:39:21.233428 2050462464 solver.cpp:464] Iteration 7260, lr = 0.001
I0501 11:40:51.543154 2050462464 solver.cpp:189] Iteration 7280, loss = 0.00281386
I0501 11:40:51.543207 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00281397 (* 1 = 0.00281397 loss)
I0501 11:40:51.543217 2050462464 solver.cpp:464] Iteration 7280, lr = 0.001
I0501 11:42:20.776420 2050462464 solver.cpp:189] Iteration 7300, loss = 0.00174891
I0501 11:42:20.776469 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00174901 (* 1 = 0.00174901 loss)
I0501 11:42:20.776479 2050462464 solver.cpp:464] Iteration 7300, lr = 0.001
I0501 11:43:53.793838 2050462464 solver.cpp:189] Iteration 7320, loss = 0.0215204
I0501 11:43:53.794600 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0215205 (* 1 = 0.0215205 loss)
I0501 11:43:53.794612 2050462464 solver.cpp:464] Iteration 7320, lr = 0.001
I0501 11:45:24.985118 2050462464 solver.cpp:189] Iteration 7340, loss = 0.00257312
I0501 11:45:24.985368 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00257322 (* 1 = 0.00257322 loss)
I0501 11:45:24.985383 2050462464 solver.cpp:464] Iteration 7340, lr = 0.001
I0501 11:46:49.631616 2050462464 solver.cpp:189] Iteration 7360, loss = 0.000500912
I0501 11:46:49.632499 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000501021 (* 1 = 0.000501021 loss)
I0501 11:46:49.632513 2050462464 solver.cpp:464] Iteration 7360, lr = 0.001
I0501 11:48:12.384158 2050462464 solver.cpp:189] Iteration 7380, loss = 0.00205946
I0501 11:48:12.384210 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00205957 (* 1 = 0.00205957 loss)
I0501 11:48:12.384220 2050462464 solver.cpp:464] Iteration 7380, lr = 0.001
I0501 11:49:36.336721 2050462464 solver.cpp:189] Iteration 7400, loss = 0.00031021
I0501 11:49:36.336768 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000310318 (* 1 = 0.000310318 loss)
I0501 11:49:36.336778 2050462464 solver.cpp:464] Iteration 7400, lr = 0.001
I0501 11:51:02.387626 2050462464 solver.cpp:189] Iteration 7420, loss = 2.90693e-05
I0501 11:51:02.387677 2050462464 solver.cpp:204]     Train net output #0: loss = 2.91776e-05 (* 1 = 2.91776e-05 loss)
I0501 11:51:02.387687 2050462464 solver.cpp:464] Iteration 7420, lr = 0.001
I0501 11:52:28.525051 2050462464 solver.cpp:189] Iteration 7440, loss = -4.08691e-08
I0501 11:52:28.525104 2050462464 solver.cpp:204]     Train net output #0: loss = 6.70553e-08 (* 1 = 6.70553e-08 loss)
I0501 11:52:28.525115 2050462464 solver.cpp:464] Iteration 7440, lr = 0.001
I0501 11:53:53.249395 2050462464 solver.cpp:189] Iteration 7460, loss = 0.000250691
I0501 11:53:53.249449 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000250799 (* 1 = 0.000250799 loss)
I0501 11:53:53.249459 2050462464 solver.cpp:464] Iteration 7460, lr = 0.001
I0501 11:55:18.279880 2050462464 solver.cpp:189] Iteration 7480, loss = 6.70651e-06
I0501 11:55:18.279933 2050462464 solver.cpp:204]     Train net output #0: loss = 6.81434e-06 (* 1 = 6.81434e-06 loss)
I0501 11:55:18.279944 2050462464 solver.cpp:464] Iteration 7480, lr = 0.001
I0501 11:56:39.559504 2050462464 solver.cpp:334] Snapshotting to caffenet_train_iter_7500.caffemodel
I0501 11:56:41.299422 2050462464 solver.cpp:342] Snapshotting solver state to caffenet_train_iter_7500.solverstate
I0501 11:56:46.789883 2050462464 solver.cpp:189] Iteration 7500, loss = 0.00015475
I0501 11:56:46.789918 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000154858 (* 1 = 0.000154858 loss)
I0501 11:56:46.789928 2050462464 solver.cpp:464] Iteration 7500, lr = 0.001
I0501 11:58:10.777956 2050462464 solver.cpp:189] Iteration 7520, loss = 2.54298e-06
I0501 11:58:10.778857 2050462464 solver.cpp:204]     Train net output #0: loss = 2.64875e-06 (* 1 = 2.64875e-06 loss)
I0501 11:58:10.778870 2050462464 solver.cpp:464] Iteration 7520, lr = 0.001
I0501 11:59:36.182847 2050462464 solver.cpp:189] Iteration 7540, loss = 0.000676456
I0501 11:59:36.182901 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00067656 (* 1 = 0.00067656 loss)
I0501 11:59:36.182910 2050462464 solver.cpp:464] Iteration 7540, lr = 0.001
I0501 12:00:59.115152 2050462464 solver.cpp:189] Iteration 7560, loss = 0.00340094
I0501 12:00:59.115203 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00340104 (* 1 = 0.00340104 loss)
I0501 12:00:59.115213 2050462464 solver.cpp:464] Iteration 7560, lr = 0.001
I0501 12:02:24.876674 2050462464 solver.cpp:189] Iteration 7580, loss = 0.056771
I0501 12:02:24.876729 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0567711 (* 1 = 0.0567711 loss)
I0501 12:02:24.876739 2050462464 solver.cpp:464] Iteration 7580, lr = 0.001
I0501 12:03:49.291048 2050462464 solver.cpp:189] Iteration 7600, loss = 0.000301749
I0501 12:03:49.291095 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000301849 (* 1 = 0.000301849 loss)
I0501 12:03:49.291105 2050462464 solver.cpp:464] Iteration 7600, lr = 0.001
I0501 12:05:14.893215 2050462464 solver.cpp:189] Iteration 7620, loss = 0.00126067
I0501 12:05:14.893271 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00126077 (* 1 = 0.00126077 loss)
I0501 12:05:14.893281 2050462464 solver.cpp:464] Iteration 7620, lr = 0.001
I0501 12:06:39.331351 2050462464 solver.cpp:189] Iteration 7640, loss = 0.000139574
I0501 12:06:39.331403 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000139679 (* 1 = 0.000139679 loss)
I0501 12:06:39.331413 2050462464 solver.cpp:464] Iteration 7640, lr = 0.001
I0501 12:08:02.742022 2050462464 solver.cpp:189] Iteration 7660, loss = 0.000825475
I0501 12:08:02.742077 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000825578 (* 1 = 0.000825578 loss)
I0501 12:08:02.742087 2050462464 solver.cpp:464] Iteration 7660, lr = 0.001
I0501 12:09:26.145644 2050462464 solver.cpp:189] Iteration 7680, loss = 2.39487e-07
I0501 12:09:26.145699 2050462464 solver.cpp:204]     Train net output #0: loss = 3.44593e-07 (* 1 = 3.44593e-07 loss)
I0501 12:09:26.145709 2050462464 solver.cpp:464] Iteration 7680, lr = 0.001
I0501 12:10:49.256716 2050462464 solver.cpp:189] Iteration 7700, loss = 4.64809e-05
I0501 12:10:49.256758 2050462464 solver.cpp:204]     Train net output #0: loss = 4.65867e-05 (* 1 = 4.65867e-05 loss)
I0501 12:10:49.256768 2050462464 solver.cpp:464] Iteration 7700, lr = 0.001
I0501 12:12:12.187547 2050462464 solver.cpp:189] Iteration 7720, loss = 3.67686e-06
I0501 12:12:12.187602 2050462464 solver.cpp:204]     Train net output #0: loss = 3.78323e-06 (* 1 = 3.78323e-06 loss)
I0501 12:12:12.187611 2050462464 solver.cpp:464] Iteration 7720, lr = 0.001
I0501 12:13:35.606928 2050462464 solver.cpp:189] Iteration 7740, loss = 0.000253866
I0501 12:13:35.606982 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000253972 (* 1 = 0.000253972 loss)
I0501 12:13:35.606992 2050462464 solver.cpp:464] Iteration 7740, lr = 0.001
I0501 12:14:59.660681 2050462464 solver.cpp:189] Iteration 7760, loss = 2.67645e-06
I0501 12:14:59.660727 2050462464 solver.cpp:204]     Train net output #0: loss = 2.78286e-06 (* 1 = 2.78286e-06 loss)
I0501 12:14:59.660737 2050462464 solver.cpp:464] Iteration 7760, lr = 0.001
I0501 12:16:23.391774 2050462464 solver.cpp:189] Iteration 7780, loss = 8.80309e-06
I0501 12:16:23.391850 2050462464 solver.cpp:204]     Train net output #0: loss = 8.90922e-06 (* 1 = 8.90922e-06 loss)
I0501 12:16:23.391861 2050462464 solver.cpp:464] Iteration 7780, lr = 0.001
I0501 12:17:46.858014 2050462464 solver.cpp:189] Iteration 7800, loss = 0.00261988
I0501 12:17:46.858055 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00261999 (* 1 = 0.00261999 loss)
I0501 12:17:46.858065 2050462464 solver.cpp:464] Iteration 7800, lr = 0.001
I0501 12:19:09.754673 2050462464 solver.cpp:189] Iteration 7820, loss = 0.000152442
I0501 12:19:09.754727 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000152548 (* 1 = 0.000152548 loss)
I0501 12:19:09.754737 2050462464 solver.cpp:464] Iteration 7820, lr = 0.001
I0501 12:20:33.255692 2050462464 solver.cpp:189] Iteration 7840, loss = 0.00132291
I0501 12:20:33.255873 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00132302 (* 1 = 0.00132302 loss)
I0501 12:20:33.255885 2050462464 solver.cpp:464] Iteration 7840, lr = 0.001
I0501 12:21:59.728409 2050462464 solver.cpp:189] Iteration 7860, loss = 0.00248431
I0501 12:21:59.728788 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00248441 (* 1 = 0.00248441 loss)
I0501 12:21:59.728811 2050462464 solver.cpp:464] Iteration 7860, lr = 0.001
I0501 12:41:12.272792 2050462464 solver.cpp:189] Iteration 7880, loss = 0.0039741
I0501 12:41:12.273147 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0039742 (* 1 = 0.0039742 loss)
I0501 12:41:12.273170 2050462464 solver.cpp:464] Iteration 7880, lr = 0.001
I0501 12:42:44.018741 2050462464 solver.cpp:189] Iteration 7900, loss = 0.00139827
I0501 12:42:44.019521 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00139837 (* 1 = 0.00139837 loss)
I0501 12:42:44.019533 2050462464 solver.cpp:464] Iteration 7900, lr = 0.001
I0501 12:44:19.229682 2050462464 solver.cpp:189] Iteration 7920, loss = 0.00149556
I0501 12:44:19.229740 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00149566 (* 1 = 0.00149566 loss)
I0501 12:44:19.229756 2050462464 solver.cpp:464] Iteration 7920, lr = 0.001
I0501 12:46:13.450389 2050462464 solver.cpp:189] Iteration 7940, loss = 0.000430386
I0501 12:46:13.450795 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000430491 (* 1 = 0.000430491 loss)
I0501 12:46:13.450809 2050462464 solver.cpp:464] Iteration 7940, lr = 0.001
I0501 12:47:55.344691 2050462464 solver.cpp:189] Iteration 7960, loss = 0.0110233
I0501 12:47:55.344740 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0110234 (* 1 = 0.0110234 loss)
I0501 12:47:55.344750 2050462464 solver.cpp:464] Iteration 7960, lr = 0.001
I0501 12:49:25.498436 2050462464 solver.cpp:189] Iteration 7980, loss = 0.00293265
I0501 12:49:25.498483 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00293275 (* 1 = 0.00293275 loss)
I0501 12:49:25.498492 2050462464 solver.cpp:464] Iteration 7980, lr = 0.001
I0501 12:50:47.272570 2050462464 solver.cpp:334] Snapshotting to caffenet_train_iter_8000.caffemodel
I0501 12:50:49.008038 2050462464 solver.cpp:342] Snapshotting solver state to caffenet_train_iter_8000.solverstate
I0501 12:50:50.288343 2050462464 solver.cpp:266] Iteration 8000, Testing net (#0)
^C
C02MX066FD58:rcc_net cusgadmin$ 
