C02MX066FD58:~ cusgadmin$ ls
Applications			beats
Box Sync			caffe
Collect_tweets			data_e-mission
Creative Cloud Files		duckcurve
Creative Cloud Files (unknown)	e-mission
Desktop				eclipse_kepler
Documents			eclipse_luna
Downloads			matlab_crash_dump.16825-1
Dropbox				matlab_crash_dump.46603-1
Google Drive			mongo_log.s
Google Earth.app		nano.save
Grammar_TS			new_collect.py
Library				nohup.out
Movies				null.txt
MultimodePIF			pf_project
Music				projectARZ
Objective-C			root
Pictures			scripts_e-mission
Public				smartCams
Qt				temp.txt
Untitled0.ipynb
C02MX066FD58:~ cusgadmin$ ls
Applications			beats
Box Sync			caffe
Collect_tweets			data_e-mission
Creative Cloud Files		duckcurve
Creative Cloud Files (unknown)	e-mission
Desktop				eclipse_kepler
Documents			eclipse_luna
Downloads			matlab_crash_dump.16825-1
Dropbox				matlab_crash_dump.46603-1
Google Drive			mongo_log.s
Google Earth.app		nano.save
Grammar_TS			new_collect.py
Library				nohup.out
Movies				null.txt
MultimodePIF			pf_project
Music				projectARZ
Objective-C			root
Pictures			scripts_e-mission
Public				smartCams
Qt				temp.txt
Untitled0.ipynb
C02MX066FD58:~ cusgadmin$ cd smartCams/
.DS_Store               SmartTrafficCams alias  
Data                    Wksp/                   
C02MX066FD58:~ cusgadmin$ cd smartCams/Wksp/smartCams/
C02MX066FD58:smartCams cusgadmin$ ls
ImageNet			misc_caffe
VOC2012				preprocess.ipynb
background			processed_imgs
deep_learning			rcnn_features_ml
finetuning			region_proposals
image_dump			segmentation
imagenet_mean.binaryproto	train_VOC2012
input_output
C02MX066FD58:smartCams cusgadmin$ cd sm
-bash: cd: sm: No such file or directory
C02MX066FD58:smartCams cusgadmin$ ls
ImageNet			misc_caffe
VOC2012				preprocess.ipynb
background			processed_imgs
deep_learning			rcnn_features_ml
finetuning			region_proposals
image_dump			segmentation
imagenet_mean.binaryproto	train_VOC2012
input_output
C02MX066FD58:smartCams cusgadmin$ cd finetuning/
C02MX066FD58:finetuning cusgadmin$ ls
image_dump	output.txt	rcc_net		test_list.txt	train_list.txt
C02MX066FD58:finetuning cusgadmin$ cd rcc_net/
C02MX066FD58:rcc_net cusgadmin$ ls
bvlc_reference_rcnn_ilsvrc13.caffemodel
caffenet_finetuned_no_background.caffemodel
caffenet_train_iter_1000.caffemodel
caffenet_train_iter_1000.solverstate
caffenet_train_iter_1500.caffemodel
caffenet_train_iter_1500.solverstate
caffenet_train_iter_2000.caffemodel
caffenet_train_iter_2000.solverstate
caffenet_train_iter_2500.caffemodel
caffenet_train_iter_2500.solverstate
caffenet_train_iter_3000.caffemodel
caffenet_train_iter_3000.solverstate
caffenet_train_iter_3500.caffemodel
caffenet_train_iter_3500.solverstate
caffenet_train_iter_4000.caffemodel
caffenet_train_iter_4000.solverstate
caffenet_train_iter_4500.caffemodel
caffenet_train_iter_4500.solverstate
caffenet_train_iter_500.caffemodel
caffenet_train_iter_500.solverstate
caffenet_train_iter_5000.caffemodel
caffenet_train_iter_5000.solverstate
caffenet_train_iter_5500.caffemodel
caffenet_train_iter_5500.solverstate
caffenet_train_iter_6000.caffemodel
caffenet_train_iter_6000.solverstate
caffenet_train_iter_6500.caffemodel
caffenet_train_iter_6500.solverstate
caffenet_train_iter_7000.caffemodel
caffenet_train_iter_7000.solverstate
caffenet_train_iter_7500.caffemodel
caffenet_train_iter_7500.solverstate
caffenet_train_iter_8000.caffemodel
caffenet_train_iter_8000.solverstate
deploy.prototxt
solver.prototxt
solver.prototxt.slow
train_val.prototxt
C02MX066FD58:rcc_net cusgadmin$ ls
bvlc_reference_rcnn_ilsvrc13.caffemodel	solver.prototxt
deploy.prototxt				solver.prototxt.slow
no_background				train_val.prototxt
C02MX066FD58:rcc_net cusgadmin$ caffe -train -solver solver.prototxt -weights bvlc_reference_rcnn_ilsvrc13.caffemodel 
ERROR: unknown command line flag 'train'
C02MX066FD58:rcc_net cusgadmin$ caffe train -solver solver.prototxt -weights bvlc_reference_rcnn_ilsvrc13.caffemodel 
I0501 15:43:30.347662 2050462464 caffe.cpp:117] Use CPU.
I0501 15:43:30.349865 2050462464 caffe.cpp:121] Starting Optimization
I0501 15:43:30.350075 2050462464 solver.cpp:32] Initializing solver from parameters: 
test_iter: 1000
test_interval: 1000
base_lr: 0.001
display: 20
max_iter: 10000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 100000
snapshot: 500
snapshot_prefix: "caffenet_train"
solver_mode: CPU
net: "train_val.prototxt"
I0501 15:43:30.350596 2050462464 solver.cpp:70] Creating training net from net file: train_val.prototxt
I0501 15:43:30.351758 2050462464 net.cpp:257] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0501 15:43:30.351809 2050462464 net.cpp:257] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0501 15:43:30.351826 2050462464 net.cpp:42] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "ImageData"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  image_data_param {
    source: "../train_list.txt"
    batch_size: 64
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8_VOC"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8_VOC"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  param {
    lr_mult: 20
    decay_mult: 0
  }
  inner_product_param {
    num_output: 5
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_VOC"
  bottom: "label"
  top: "loss"
}
I0501 15:43:30.352476 2050462464 layer_factory.hpp:74] Creating layer data
I0501 15:43:30.352774 2050462464 net.cpp:84] Creating Layer data
I0501 15:43:30.352794 2050462464 net.cpp:338] data -> data
I0501 15:43:30.352821 2050462464 net.cpp:338] data -> label
I0501 15:43:30.352831 2050462464 net.cpp:113] Setting up data
I0501 15:43:30.353057 2050462464 image_data_layer.cpp:36] Opening file ../train_list.txt
I0501 15:43:30.398411 2050462464 image_data_layer.cpp:51] A total of 32601 images.
E0501 15:43:30.398465 2050462464 io.cpp:77] Could not open or find file ../image_dump/cropped/pict_0.jpg
I0501 15:43:30.399123 2050462464 image_data_layer.cpp:80] output data size: 64,1,227,227
I0501 15:43:30.411710 2050462464 net.cpp:120] Top shape: 64 1 227 227 (3297856)
I0501 15:43:30.411743 2050462464 net.cpp:120] Top shape: 64 (64)
I0501 15:43:30.411753 2050462464 layer_factory.hpp:74] Creating layer conv1
I0501 15:43:30.411767 2050462464 net.cpp:84] Creating Layer conv1
I0501 15:43:30.411773 2050462464 net.cpp:380] conv1 <- data
I0501 15:43:30.411782 2050462464 net.cpp:338] conv1 -> conv1
I0501 15:43:30.411792 2050462464 net.cpp:113] Setting up conv1
E0501 15:43:30.411983 179355648 io.cpp:77] Could not open or find file ../image_dump/cropped/pict_0.jpg
F0501 15:43:30.412014 179355648 image_data_layer.cpp:135] Check failed: cv_img.data Could not load ../image_dump/cropped/pict_0.jpg
*** Check failure stack trace: ***
I0501 15:43:30.412437 2050462464 net.cpp:120] Top shape: 64 96 55 55 (18585600)
I0501 15:43:30.412770 2050462464 layer_factory.hpp:74] Creating layer relu1
I0501 15:43:30.412786 2050462464 net.cpp:84] Creating Layer relu1
I0501 15:43:30.412791 2050462464 net.cpp:380] relu1 <- conv1
I0501 15:43:30.412797 2050462464 net.cpp:327] relu1 -> conv1 (in-place)
I0501 15:43:30.412803 2050462464 net.cpp:113] Setting up relu1
I0501 15:43:30.412808 2050462464 net.cpp:120] Top shape: 64 96 55 55 (18585600)
I0501 15:43:30.412814 2050462464 layer_factory.hpp:74] Creating layer pool1
I0501 15:43:30.412822 2050462464 net.cpp:84] Creating Layer pool1
I0501 15:43:30.412825 2050462464 net.cpp:380] pool1 <- conv1
I0501 15:43:30.412832 2050462464 net.cpp:338] pool1 -> pool1
I0501 15:43:30.412840 2050462464 net.cpp:113] Setting up pool1
I0501 15:43:30.412858 2050462464 net.cpp:120] Top shape: 64 96 27 27 (4478976)
I0501 15:43:30.412865 2050462464 layer_factory.hpp:74] Creating layer norm1
I0501 15:43:30.412874 2050462464 net.cpp:84] Creating Layer norm1
I0501 15:43:30.412879 2050462464 net.cpp:380] norm1 <- pool1
I0501 15:43:30.412889 2050462464 net.cpp:338] norm1 -> norm1
I0501 15:43:30.412896 2050462464 net.cpp:113] Setting up norm1
I0501 15:43:30.412905 2050462464 net.cpp:120] Top shape: 64 96 27 27 (4478976)
I0501 15:43:30.412911 2050462464 layer_factory.hpp:74] Creating layer conv2
I0501 15:43:30.412919 2050462464 net.cpp:84] Creating Layer conv2
I0501 15:43:30.412922 2050462464 net.cpp:380] conv2 <- norm1
I0501 15:43:30.412928 2050462464 net.cpp:338] conv2 -> conv2
I0501 15:43:30.412935 2050462464 net.cpp:113] Setting up conv2
    @        0x108df4076  google::LogMessage::Fail()
    @        0x108df3757  google::LogMessage::SendToLog()
    @        0x108df3cc5  google::LogMessage::Flush()
    @        0x108df7015  google::LogMessageFatal::~LogMessageFatal()
    @        0x108df4363  google::LogMessageFatal::~LogMessageFatal()
    @        0x108c3d820  caffe::ImageDataLayer<>::InternalThreadEntry()
    @        0x109700d05  boost::(anonymous namespace)::thread_proxy()
    @     0x7fff92eca268  _pthread_body
    @     0x7fff92eca1e5  _pthread_start
    @     0x7fff92ec841d  thread_start
Abort trap: 6
C02MX066FD58:rcc_net cusgadmin$ caffe train -solver solver.prototxt -weights bvlc_reference_rcnn_ilsvrc13.caffemodel 
I0501 15:49:39.821236 2050462464 caffe.cpp:117] Use CPU.
I0501 15:49:39.822865 2050462464 caffe.cpp:121] Starting Optimization
I0501 15:49:39.823094 2050462464 solver.cpp:32] Initializing solver from parameters: 
test_iter: 1000
test_interval: 1000
base_lr: 0.001
display: 20
max_iter: 10000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 100000
snapshot: 500
snapshot_prefix: "caffenet_train"
solver_mode: CPU
net: "train_val.prototxt"
I0501 15:49:39.823194 2050462464 solver.cpp:70] Creating training net from net file: train_val.prototxt
I0501 15:49:39.824831 2050462464 net.cpp:257] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0501 15:49:39.824865 2050462464 net.cpp:257] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0501 15:49:39.824875 2050462464 net.cpp:42] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "ImageData"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  image_data_param {
    source: "../train_list.txt"
    batch_size: 64
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8_VOC"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8_VOC"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  param {
    lr_mult: 20
    decay_mult: 0
  }
  inner_product_param {
    num_output: 5
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_VOC"
  bottom: "label"
  top: "loss"
}
I0501 15:49:39.825714 2050462464 layer_factory.hpp:74] Creating layer data
I0501 15:49:39.825738 2050462464 net.cpp:84] Creating Layer data
I0501 15:49:39.825749 2050462464 net.cpp:338] data -> data
I0501 15:49:39.825772 2050462464 net.cpp:338] data -> label
I0501 15:49:39.825780 2050462464 net.cpp:113] Setting up data
I0501 15:49:39.825793 2050462464 image_data_layer.cpp:36] Opening file ../train_list.txt
I0501 15:49:39.868427 2050462464 image_data_layer.cpp:51] A total of 32601 images.
I0501 15:49:39.870414 2050462464 image_data_layer.cpp:80] output data size: 64,3,227,227
I0501 15:49:39.896838 2050462464 net.cpp:120] Top shape: 64 3 227 227 (9893568)
I0501 15:49:39.896878 2050462464 net.cpp:120] Top shape: 64 (64)
I0501 15:49:39.896893 2050462464 layer_factory.hpp:74] Creating layer conv1
I0501 15:49:39.896916 2050462464 net.cpp:84] Creating Layer conv1
I0501 15:49:39.896927 2050462464 net.cpp:380] conv1 <- data
I0501 15:49:39.896940 2050462464 net.cpp:338] conv1 -> conv1
I0501 15:49:39.896955 2050462464 net.cpp:113] Setting up conv1
I0501 15:49:39.897819 2050462464 net.cpp:120] Top shape: 64 96 55 55 (18585600)
I0501 15:49:39.897848 2050462464 layer_factory.hpp:74] Creating layer relu1
I0501 15:49:39.897874 2050462464 net.cpp:84] Creating Layer relu1
I0501 15:49:39.897884 2050462464 net.cpp:380] relu1 <- conv1
I0501 15:49:39.897898 2050462464 net.cpp:327] relu1 -> conv1 (in-place)
I0501 15:49:39.897910 2050462464 net.cpp:113] Setting up relu1
I0501 15:49:39.897919 2050462464 net.cpp:120] Top shape: 64 96 55 55 (18585600)
I0501 15:49:39.897930 2050462464 layer_factory.hpp:74] Creating layer pool1
I0501 15:49:39.897941 2050462464 net.cpp:84] Creating Layer pool1
I0501 15:49:39.897949 2050462464 net.cpp:380] pool1 <- conv1
I0501 15:49:39.897960 2050462464 net.cpp:338] pool1 -> pool1
I0501 15:49:39.897975 2050462464 net.cpp:113] Setting up pool1
I0501 15:49:39.898000 2050462464 net.cpp:120] Top shape: 64 96 27 27 (4478976)
I0501 15:49:39.898013 2050462464 layer_factory.hpp:74] Creating layer norm1
I0501 15:49:39.898028 2050462464 net.cpp:84] Creating Layer norm1
I0501 15:49:39.898037 2050462464 net.cpp:380] norm1 <- pool1
I0501 15:49:39.898048 2050462464 net.cpp:338] norm1 -> norm1
I0501 15:49:39.898061 2050462464 net.cpp:113] Setting up norm1
I0501 15:49:39.898075 2050462464 net.cpp:120] Top shape: 64 96 27 27 (4478976)
I0501 15:49:39.898087 2050462464 layer_factory.hpp:74] Creating layer conv2
I0501 15:49:39.898100 2050462464 net.cpp:84] Creating Layer conv2
I0501 15:49:39.898146 2050462464 net.cpp:380] conv2 <- norm1
I0501 15:49:39.898162 2050462464 net.cpp:338] conv2 -> conv2
I0501 15:49:39.898175 2050462464 net.cpp:113] Setting up conv2
I0501 15:49:39.905215 2050462464 net.cpp:120] Top shape: 64 256 27 27 (11943936)
I0501 15:49:39.905253 2050462464 layer_factory.hpp:74] Creating layer relu2
I0501 15:49:39.905268 2050462464 net.cpp:84] Creating Layer relu2
I0501 15:49:39.905277 2050462464 net.cpp:380] relu2 <- conv2
I0501 15:49:39.905288 2050462464 net.cpp:327] relu2 -> conv2 (in-place)
I0501 15:49:39.905302 2050462464 net.cpp:113] Setting up relu2
I0501 15:49:39.905309 2050462464 net.cpp:120] Top shape: 64 256 27 27 (11943936)
I0501 15:49:39.905321 2050462464 layer_factory.hpp:74] Creating layer pool2
I0501 15:49:39.905333 2050462464 net.cpp:84] Creating Layer pool2
I0501 15:49:39.905340 2050462464 net.cpp:380] pool2 <- conv2
I0501 15:49:39.905351 2050462464 net.cpp:338] pool2 -> pool2
I0501 15:49:39.905364 2050462464 net.cpp:113] Setting up pool2
I0501 15:49:39.905375 2050462464 net.cpp:120] Top shape: 64 256 13 13 (2768896)
I0501 15:49:39.905386 2050462464 layer_factory.hpp:74] Creating layer norm2
I0501 15:49:39.905400 2050462464 net.cpp:84] Creating Layer norm2
I0501 15:49:39.905468 2050462464 net.cpp:380] norm2 <- pool2
I0501 15:49:39.905485 2050462464 net.cpp:338] norm2 -> norm2
I0501 15:49:39.905498 2050462464 net.cpp:113] Setting up norm2
I0501 15:49:39.905508 2050462464 net.cpp:120] Top shape: 64 256 13 13 (2768896)
I0501 15:49:39.905520 2050462464 layer_factory.hpp:74] Creating layer conv3
I0501 15:49:39.905534 2050462464 net.cpp:84] Creating Layer conv3
I0501 15:49:39.905542 2050462464 net.cpp:380] conv3 <- norm2
I0501 15:49:39.905558 2050462464 net.cpp:338] conv3 -> conv3
I0501 15:49:39.905573 2050462464 net.cpp:113] Setting up conv3
I0501 15:49:39.925165 2050462464 net.cpp:120] Top shape: 64 384 13 13 (4153344)
I0501 15:49:39.925207 2050462464 layer_factory.hpp:74] Creating layer relu3
I0501 15:49:39.925216 2050462464 net.cpp:84] Creating Layer relu3
I0501 15:49:39.925247 2050462464 net.cpp:380] relu3 <- conv3
I0501 15:49:39.925266 2050462464 net.cpp:327] relu3 -> conv3 (in-place)
I0501 15:49:39.925274 2050462464 net.cpp:113] Setting up relu3
I0501 15:49:39.925281 2050462464 net.cpp:120] Top shape: 64 384 13 13 (4153344)
I0501 15:49:39.925287 2050462464 layer_factory.hpp:74] Creating layer conv4
I0501 15:49:39.925295 2050462464 net.cpp:84] Creating Layer conv4
I0501 15:49:39.925300 2050462464 net.cpp:380] conv4 <- conv3
I0501 15:49:39.925307 2050462464 net.cpp:338] conv4 -> conv4
I0501 15:49:39.925313 2050462464 net.cpp:113] Setting up conv4
I0501 15:49:39.937610 2050462464 net.cpp:120] Top shape: 64 384 13 13 (4153344)
I0501 15:49:39.937646 2050462464 layer_factory.hpp:74] Creating layer relu4
I0501 15:49:39.937662 2050462464 net.cpp:84] Creating Layer relu4
I0501 15:49:39.937667 2050462464 net.cpp:380] relu4 <- conv4
I0501 15:49:39.937674 2050462464 net.cpp:327] relu4 -> conv4 (in-place)
I0501 15:49:39.937680 2050462464 net.cpp:113] Setting up relu4
I0501 15:49:39.937686 2050462464 net.cpp:120] Top shape: 64 384 13 13 (4153344)
I0501 15:49:39.937691 2050462464 layer_factory.hpp:74] Creating layer conv5
I0501 15:49:39.937700 2050462464 net.cpp:84] Creating Layer conv5
I0501 15:49:39.937703 2050462464 net.cpp:380] conv5 <- conv4
I0501 15:49:39.937710 2050462464 net.cpp:338] conv5 -> conv5
I0501 15:49:39.937717 2050462464 net.cpp:113] Setting up conv5
I0501 15:49:39.945868 2050462464 net.cpp:120] Top shape: 64 256 13 13 (2768896)
I0501 15:49:39.945907 2050462464 layer_factory.hpp:74] Creating layer relu5
I0501 15:49:39.945924 2050462464 net.cpp:84] Creating Layer relu5
I0501 15:49:39.945933 2050462464 net.cpp:380] relu5 <- conv5
I0501 15:49:39.945953 2050462464 net.cpp:327] relu5 -> conv5 (in-place)
I0501 15:49:39.945967 2050462464 net.cpp:113] Setting up relu5
I0501 15:49:39.945976 2050462464 net.cpp:120] Top shape: 64 256 13 13 (2768896)
I0501 15:49:39.945988 2050462464 layer_factory.hpp:74] Creating layer pool5
I0501 15:49:39.946035 2050462464 net.cpp:84] Creating Layer pool5
I0501 15:49:39.946045 2050462464 net.cpp:380] pool5 <- conv5
I0501 15:49:39.946058 2050462464 net.cpp:338] pool5 -> pool5
I0501 15:49:39.946069 2050462464 net.cpp:113] Setting up pool5
I0501 15:49:39.946082 2050462464 net.cpp:120] Top shape: 64 256 6 6 (589824)
I0501 15:49:39.946094 2050462464 layer_factory.hpp:74] Creating layer fc6
I0501 15:49:39.946110 2050462464 net.cpp:84] Creating Layer fc6
I0501 15:49:39.946264 2050462464 net.cpp:380] fc6 <- pool5
I0501 15:49:39.946286 2050462464 net.cpp:338] fc6 -> fc6
I0501 15:49:39.946301 2050462464 net.cpp:113] Setting up fc6
I0501 15:49:40.706302 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0501 15:49:40.706346 2050462464 layer_factory.hpp:74] Creating layer relu6
I0501 15:49:40.706362 2050462464 net.cpp:84] Creating Layer relu6
I0501 15:49:40.706372 2050462464 net.cpp:380] relu6 <- fc6
I0501 15:49:40.706382 2050462464 net.cpp:327] relu6 -> fc6 (in-place)
I0501 15:49:40.706389 2050462464 net.cpp:113] Setting up relu6
I0501 15:49:40.706394 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0501 15:49:40.706441 2050462464 layer_factory.hpp:74] Creating layer drop6
I0501 15:49:40.706465 2050462464 net.cpp:84] Creating Layer drop6
I0501 15:49:40.706473 2050462464 net.cpp:380] drop6 <- fc6
I0501 15:49:40.706485 2050462464 net.cpp:327] drop6 -> fc6 (in-place)
I0501 15:49:40.706498 2050462464 net.cpp:113] Setting up drop6
I0501 15:49:40.706516 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0501 15:49:40.706528 2050462464 layer_factory.hpp:74] Creating layer fc7
I0501 15:49:40.706542 2050462464 net.cpp:84] Creating Layer fc7
I0501 15:49:40.706552 2050462464 net.cpp:380] fc7 <- fc6
I0501 15:49:40.706563 2050462464 net.cpp:338] fc7 -> fc7
I0501 15:49:40.706576 2050462464 net.cpp:113] Setting up fc7
I0501 15:49:41.040921 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0501 15:49:41.040957 2050462464 layer_factory.hpp:74] Creating layer relu7
I0501 15:49:41.040969 2050462464 net.cpp:84] Creating Layer relu7
I0501 15:49:41.040974 2050462464 net.cpp:380] relu7 <- fc7
I0501 15:49:41.040990 2050462464 net.cpp:327] relu7 -> fc7 (in-place)
I0501 15:49:41.040999 2050462464 net.cpp:113] Setting up relu7
I0501 15:49:41.041007 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0501 15:49:41.041012 2050462464 layer_factory.hpp:74] Creating layer drop7
I0501 15:49:41.041018 2050462464 net.cpp:84] Creating Layer drop7
I0501 15:49:41.041023 2050462464 net.cpp:380] drop7 <- fc7
I0501 15:49:41.041028 2050462464 net.cpp:327] drop7 -> fc7 (in-place)
I0501 15:49:41.041033 2050462464 net.cpp:113] Setting up drop7
I0501 15:49:41.041039 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0501 15:49:41.041044 2050462464 layer_factory.hpp:74] Creating layer fc8_VOC
I0501 15:49:41.041051 2050462464 net.cpp:84] Creating Layer fc8_VOC
I0501 15:49:41.041055 2050462464 net.cpp:380] fc8_VOC <- fc7
I0501 15:49:41.041061 2050462464 net.cpp:338] fc8_VOC -> fc8_VOC
I0501 15:49:41.041069 2050462464 net.cpp:113] Setting up fc8_VOC
I0501 15:49:41.041399 2050462464 net.cpp:120] Top shape: 64 5 (320)
I0501 15:49:41.041407 2050462464 layer_factory.hpp:74] Creating layer loss
I0501 15:49:41.041664 2050462464 net.cpp:84] Creating Layer loss
I0501 15:49:41.041682 2050462464 net.cpp:380] loss <- fc8_VOC
I0501 15:49:41.041687 2050462464 net.cpp:380] loss <- label
I0501 15:49:41.041694 2050462464 net.cpp:338] loss -> loss
I0501 15:49:41.041702 2050462464 net.cpp:113] Setting up loss
I0501 15:49:41.041708 2050462464 layer_factory.hpp:74] Creating layer loss
I0501 15:49:41.041721 2050462464 net.cpp:120] Top shape: (1)
I0501 15:49:41.041728 2050462464 net.cpp:122]     with loss weight 1
I0501 15:49:41.041741 2050462464 net.cpp:167] loss needs backward computation.
I0501 15:49:41.041746 2050462464 net.cpp:167] fc8_VOC needs backward computation.
I0501 15:49:41.041750 2050462464 net.cpp:167] drop7 needs backward computation.
I0501 15:49:41.041754 2050462464 net.cpp:167] relu7 needs backward computation.
I0501 15:49:41.041759 2050462464 net.cpp:167] fc7 needs backward computation.
I0501 15:49:41.041790 2050462464 net.cpp:167] drop6 needs backward computation.
I0501 15:49:41.041795 2050462464 net.cpp:167] relu6 needs backward computation.
I0501 15:49:41.041800 2050462464 net.cpp:167] fc6 needs backward computation.
I0501 15:49:41.041803 2050462464 net.cpp:167] pool5 needs backward computation.
I0501 15:49:41.041807 2050462464 net.cpp:167] relu5 needs backward computation.
I0501 15:49:41.041812 2050462464 net.cpp:167] conv5 needs backward computation.
I0501 15:49:41.041816 2050462464 net.cpp:167] relu4 needs backward computation.
I0501 15:49:41.041821 2050462464 net.cpp:167] conv4 needs backward computation.
I0501 15:49:41.041824 2050462464 net.cpp:167] relu3 needs backward computation.
I0501 15:49:41.041828 2050462464 net.cpp:167] conv3 needs backward computation.
I0501 15:49:41.041833 2050462464 net.cpp:167] norm2 needs backward computation.
I0501 15:49:41.041837 2050462464 net.cpp:167] pool2 needs backward computation.
I0501 15:49:41.041842 2050462464 net.cpp:167] relu2 needs backward computation.
I0501 15:49:41.041846 2050462464 net.cpp:167] conv2 needs backward computation.
I0501 15:49:41.041851 2050462464 net.cpp:167] norm1 needs backward computation.
I0501 15:49:41.041856 2050462464 net.cpp:167] pool1 needs backward computation.
I0501 15:49:41.041859 2050462464 net.cpp:167] relu1 needs backward computation.
I0501 15:49:41.041864 2050462464 net.cpp:167] conv1 needs backward computation.
I0501 15:49:41.041868 2050462464 net.cpp:169] data does not need backward computation.
I0501 15:49:41.041872 2050462464 net.cpp:205] This network produces output loss
I0501 15:49:41.041885 2050462464 net.cpp:447] Collecting Learning Rate and Weight Decay.
I0501 15:49:41.041893 2050462464 net.cpp:217] Network initialization done.
I0501 15:49:41.041898 2050462464 net.cpp:218] Memory required for data: 439050500
I0501 15:49:41.042515 2050462464 solver.cpp:154] Creating test net (#0) specified by net file: train_val.prototxt
I0501 15:49:41.042558 2050462464 net.cpp:257] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0501 15:49:41.042577 2050462464 net.cpp:42] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "ImageData"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  image_data_param {
    source: "../test_list.txt"
    batch_size: 64
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8_VOC"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8_VOC"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  param {
    lr_mult: 20
    decay_mult: 0
  }
  inner_product_param {
    num_output: 5
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_VOC"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_VOC"
  bottom: "label"
  top: "loss"
}
I0501 15:49:41.043256 2050462464 layer_factory.hpp:74] Creating layer data
I0501 15:49:41.043272 2050462464 net.cpp:84] Creating Layer data
I0501 15:49:41.043278 2050462464 net.cpp:338] data -> data
I0501 15:49:41.043288 2050462464 net.cpp:338] data -> label
I0501 15:49:41.043295 2050462464 net.cpp:113] Setting up data
I0501 15:49:41.043300 2050462464 image_data_layer.cpp:36] Opening file ../test_list.txt
I0501 15:49:41.050626 2050462464 image_data_layer.cpp:51] A total of 3703 images.
I0501 15:49:41.052788 2050462464 image_data_layer.cpp:80] output data size: 64,3,227,227
I0501 15:49:41.078548 2050462464 net.cpp:120] Top shape: 64 3 227 227 (9893568)
I0501 15:49:41.078596 2050462464 net.cpp:120] Top shape: 64 (64)
I0501 15:49:41.078621 2050462464 layer_factory.hpp:74] Creating layer label_data_1_split
I0501 15:49:41.078641 2050462464 net.cpp:84] Creating Layer label_data_1_split
I0501 15:49:41.078651 2050462464 net.cpp:380] label_data_1_split <- label
I0501 15:49:41.078694 2050462464 net.cpp:338] label_data_1_split -> label_data_1_split_0
I0501 15:49:41.078712 2050462464 net.cpp:338] label_data_1_split -> label_data_1_split_1
I0501 15:49:41.078726 2050462464 net.cpp:113] Setting up label_data_1_split
I0501 15:49:41.078737 2050462464 net.cpp:120] Top shape: 64 (64)
I0501 15:49:41.078745 2050462464 net.cpp:120] Top shape: 64 (64)
I0501 15:49:41.078755 2050462464 layer_factory.hpp:74] Creating layer conv1
I0501 15:49:41.078769 2050462464 net.cpp:84] Creating Layer conv1
I0501 15:49:41.078778 2050462464 net.cpp:380] conv1 <- data
I0501 15:49:41.078790 2050462464 net.cpp:338] conv1 -> conv1
I0501 15:49:41.078805 2050462464 net.cpp:113] Setting up conv1
I0501 15:49:41.079745 2050462464 net.cpp:120] Top shape: 64 96 55 55 (18585600)
I0501 15:49:41.079780 2050462464 layer_factory.hpp:74] Creating layer relu1
I0501 15:49:41.079797 2050462464 net.cpp:84] Creating Layer relu1
I0501 15:49:41.079807 2050462464 net.cpp:380] relu1 <- conv1
I0501 15:49:41.079818 2050462464 net.cpp:327] relu1 -> conv1 (in-place)
I0501 15:49:41.079830 2050462464 net.cpp:113] Setting up relu1
I0501 15:49:41.079839 2050462464 net.cpp:120] Top shape: 64 96 55 55 (18585600)
I0501 15:49:41.079852 2050462464 layer_factory.hpp:74] Creating layer pool1
I0501 15:49:41.079866 2050462464 net.cpp:84] Creating Layer pool1
I0501 15:49:41.079875 2050462464 net.cpp:380] pool1 <- conv1
I0501 15:49:41.079887 2050462464 net.cpp:338] pool1 -> pool1
I0501 15:49:41.079900 2050462464 net.cpp:113] Setting up pool1
I0501 15:49:41.079915 2050462464 net.cpp:120] Top shape: 64 96 27 27 (4478976)
I0501 15:49:41.079926 2050462464 layer_factory.hpp:74] Creating layer norm1
I0501 15:49:41.079941 2050462464 net.cpp:84] Creating Layer norm1
I0501 15:49:41.079949 2050462464 net.cpp:380] norm1 <- pool1
I0501 15:49:41.079960 2050462464 net.cpp:338] norm1 -> norm1
I0501 15:49:41.079973 2050462464 net.cpp:113] Setting up norm1
I0501 15:49:41.079985 2050462464 net.cpp:120] Top shape: 64 96 27 27 (4478976)
I0501 15:49:41.079998 2050462464 layer_factory.hpp:74] Creating layer conv2
I0501 15:49:41.080013 2050462464 net.cpp:84] Creating Layer conv2
I0501 15:49:41.080021 2050462464 net.cpp:380] conv2 <- norm1
I0501 15:49:41.080034 2050462464 net.cpp:338] conv2 -> conv2
I0501 15:49:41.080049 2050462464 net.cpp:113] Setting up conv2
I0501 15:49:41.088572 2050462464 net.cpp:120] Top shape: 64 256 27 27 (11943936)
I0501 15:49:41.088608 2050462464 layer_factory.hpp:74] Creating layer relu2
I0501 15:49:41.088621 2050462464 net.cpp:84] Creating Layer relu2
I0501 15:49:41.088630 2050462464 net.cpp:380] relu2 <- conv2
I0501 15:49:41.088641 2050462464 net.cpp:327] relu2 -> conv2 (in-place)
I0501 15:49:41.088652 2050462464 net.cpp:113] Setting up relu2
I0501 15:49:41.088661 2050462464 net.cpp:120] Top shape: 64 256 27 27 (11943936)
I0501 15:49:41.088673 2050462464 layer_factory.hpp:74] Creating layer pool2
I0501 15:49:41.088685 2050462464 net.cpp:84] Creating Layer pool2
I0501 15:49:41.088693 2050462464 net.cpp:380] pool2 <- conv2
I0501 15:49:41.088704 2050462464 net.cpp:338] pool2 -> pool2
I0501 15:49:41.088717 2050462464 net.cpp:113] Setting up pool2
I0501 15:49:41.088729 2050462464 net.cpp:120] Top shape: 64 256 13 13 (2768896)
I0501 15:49:41.088740 2050462464 layer_factory.hpp:74] Creating layer norm2
I0501 15:49:41.088752 2050462464 net.cpp:84] Creating Layer norm2
I0501 15:49:41.088759 2050462464 net.cpp:380] norm2 <- pool2
I0501 15:49:41.088769 2050462464 net.cpp:338] norm2 -> norm2
I0501 15:49:41.088781 2050462464 net.cpp:113] Setting up norm2
I0501 15:49:41.088791 2050462464 net.cpp:120] Top shape: 64 256 13 13 (2768896)
I0501 15:49:41.088803 2050462464 layer_factory.hpp:74] Creating layer conv3
I0501 15:49:41.088815 2050462464 net.cpp:84] Creating Layer conv3
I0501 15:49:41.088824 2050462464 net.cpp:380] conv3 <- norm2
I0501 15:49:41.088835 2050462464 net.cpp:338] conv3 -> conv3
I0501 15:49:41.088848 2050462464 net.cpp:113] Setting up conv3
I0501 15:49:41.111925 2050462464 net.cpp:120] Top shape: 64 384 13 13 (4153344)
I0501 15:49:41.112153 2050462464 layer_factory.hpp:74] Creating layer relu3
I0501 15:49:41.112174 2050462464 net.cpp:84] Creating Layer relu3
I0501 15:49:41.112184 2050462464 net.cpp:380] relu3 <- conv3
I0501 15:49:41.112306 2050462464 net.cpp:327] relu3 -> conv3 (in-place)
I0501 15:49:41.112323 2050462464 net.cpp:113] Setting up relu3
I0501 15:49:41.112334 2050462464 net.cpp:120] Top shape: 64 384 13 13 (4153344)
I0501 15:49:41.112344 2050462464 layer_factory.hpp:74] Creating layer conv4
I0501 15:49:41.112360 2050462464 net.cpp:84] Creating Layer conv4
I0501 15:49:41.112370 2050462464 net.cpp:380] conv4 <- conv3
I0501 15:49:41.112381 2050462464 net.cpp:338] conv4 -> conv4
I0501 15:49:41.112396 2050462464 net.cpp:113] Setting up conv4
I0501 15:49:41.130193 2050462464 net.cpp:120] Top shape: 64 384 13 13 (4153344)
I0501 15:49:41.130230 2050462464 layer_factory.hpp:74] Creating layer relu4
I0501 15:49:41.130245 2050462464 net.cpp:84] Creating Layer relu4
I0501 15:49:41.130264 2050462464 net.cpp:380] relu4 <- conv4
I0501 15:49:41.130276 2050462464 net.cpp:327] relu4 -> conv4 (in-place)
I0501 15:49:41.130324 2050462464 net.cpp:113] Setting up relu4
I0501 15:49:41.130345 2050462464 net.cpp:120] Top shape: 64 384 13 13 (4153344)
I0501 15:49:41.130358 2050462464 layer_factory.hpp:74] Creating layer conv5
I0501 15:49:41.130374 2050462464 net.cpp:84] Creating Layer conv5
I0501 15:49:41.130384 2050462464 net.cpp:380] conv5 <- conv4
I0501 15:49:41.130398 2050462464 net.cpp:338] conv5 -> conv5
I0501 15:49:41.130412 2050462464 net.cpp:113] Setting up conv5
I0501 15:49:41.138970 2050462464 net.cpp:120] Top shape: 64 256 13 13 (2768896)
I0501 15:49:41.139009 2050462464 layer_factory.hpp:74] Creating layer relu5
I0501 15:49:41.139024 2050462464 net.cpp:84] Creating Layer relu5
I0501 15:49:41.139031 2050462464 net.cpp:380] relu5 <- conv5
I0501 15:49:41.139041 2050462464 net.cpp:327] relu5 -> conv5 (in-place)
I0501 15:49:41.139052 2050462464 net.cpp:113] Setting up relu5
I0501 15:49:41.139060 2050462464 net.cpp:120] Top shape: 64 256 13 13 (2768896)
I0501 15:49:41.139068 2050462464 layer_factory.hpp:74] Creating layer pool5
I0501 15:49:41.139080 2050462464 net.cpp:84] Creating Layer pool5
I0501 15:49:41.139086 2050462464 net.cpp:380] pool5 <- conv5
I0501 15:49:41.139096 2050462464 net.cpp:338] pool5 -> pool5
I0501 15:49:41.139106 2050462464 net.cpp:113] Setting up pool5
I0501 15:49:41.139117 2050462464 net.cpp:120] Top shape: 64 256 6 6 (589824)
I0501 15:49:41.139125 2050462464 layer_factory.hpp:74] Creating layer fc6
I0501 15:49:41.139137 2050462464 net.cpp:84] Creating Layer fc6
I0501 15:49:41.139143 2050462464 net.cpp:380] fc6 <- pool5
I0501 15:49:41.139153 2050462464 net.cpp:338] fc6 -> fc6
I0501 15:49:41.139173 2050462464 net.cpp:113] Setting up fc6
I0501 15:49:41.968797 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0501 15:49:41.968852 2050462464 layer_factory.hpp:74] Creating layer relu6
I0501 15:49:41.968878 2050462464 net.cpp:84] Creating Layer relu6
I0501 15:49:41.968888 2050462464 net.cpp:380] relu6 <- fc6
I0501 15:49:41.968899 2050462464 net.cpp:327] relu6 -> fc6 (in-place)
I0501 15:49:41.968914 2050462464 net.cpp:113] Setting up relu6
I0501 15:49:41.968922 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0501 15:49:41.968932 2050462464 layer_factory.hpp:74] Creating layer drop6
I0501 15:49:41.968943 2050462464 net.cpp:84] Creating Layer drop6
I0501 15:49:41.968951 2050462464 net.cpp:380] drop6 <- fc6
I0501 15:49:41.968961 2050462464 net.cpp:327] drop6 -> fc6 (in-place)
I0501 15:49:41.968972 2050462464 net.cpp:113] Setting up drop6
I0501 15:49:41.968982 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0501 15:49:41.968992 2050462464 layer_factory.hpp:74] Creating layer fc7
I0501 15:49:41.969007 2050462464 net.cpp:84] Creating Layer fc7
I0501 15:49:41.969014 2050462464 net.cpp:380] fc7 <- fc6
I0501 15:49:41.969027 2050462464 net.cpp:338] fc7 -> fc7
I0501 15:49:41.969040 2050462464 net.cpp:113] Setting up fc7
I0501 15:49:42.317960 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0501 15:49:42.318017 2050462464 layer_factory.hpp:74] Creating layer relu7
I0501 15:49:42.318090 2050462464 net.cpp:84] Creating Layer relu7
I0501 15:49:42.318105 2050462464 net.cpp:380] relu7 <- fc7
I0501 15:49:42.318120 2050462464 net.cpp:327] relu7 -> fc7 (in-place)
I0501 15:49:42.318137 2050462464 net.cpp:113] Setting up relu7
I0501 15:49:42.318148 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0501 15:49:42.318161 2050462464 layer_factory.hpp:74] Creating layer drop7
I0501 15:49:42.318176 2050462464 net.cpp:84] Creating Layer drop7
I0501 15:49:42.318186 2050462464 net.cpp:380] drop7 <- fc7
I0501 15:49:42.318231 2050462464 net.cpp:327] drop7 -> fc7 (in-place)
I0501 15:49:42.318248 2050462464 net.cpp:113] Setting up drop7
I0501 15:49:42.318254 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0501 15:49:42.318261 2050462464 layer_factory.hpp:74] Creating layer fc8_VOC
I0501 15:49:42.318271 2050462464 net.cpp:84] Creating Layer fc8_VOC
I0501 15:49:42.318276 2050462464 net.cpp:380] fc8_VOC <- fc7
I0501 15:49:42.318285 2050462464 net.cpp:338] fc8_VOC -> fc8_VOC
I0501 15:49:42.318302 2050462464 net.cpp:113] Setting up fc8_VOC
I0501 15:49:42.318713 2050462464 net.cpp:120] Top shape: 64 5 (320)
I0501 15:49:42.318734 2050462464 layer_factory.hpp:74] Creating layer fc8_VOC_fc8_VOC_0_split
I0501 15:49:42.318744 2050462464 net.cpp:84] Creating Layer fc8_VOC_fc8_VOC_0_split
I0501 15:49:42.318749 2050462464 net.cpp:380] fc8_VOC_fc8_VOC_0_split <- fc8_VOC
I0501 15:49:42.318758 2050462464 net.cpp:338] fc8_VOC_fc8_VOC_0_split -> fc8_VOC_fc8_VOC_0_split_0
I0501 15:49:42.318768 2050462464 net.cpp:338] fc8_VOC_fc8_VOC_0_split -> fc8_VOC_fc8_VOC_0_split_1
I0501 15:49:42.318776 2050462464 net.cpp:113] Setting up fc8_VOC_fc8_VOC_0_split
I0501 15:49:42.318799 2050462464 net.cpp:120] Top shape: 64 5 (320)
I0501 15:49:42.318837 2050462464 net.cpp:120] Top shape: 64 5 (320)
I0501 15:49:42.318846 2050462464 layer_factory.hpp:74] Creating layer accuracy
I0501 15:49:42.319072 2050462464 net.cpp:84] Creating Layer accuracy
I0501 15:49:42.319088 2050462464 net.cpp:380] accuracy <- fc8_VOC_fc8_VOC_0_split_0
I0501 15:49:42.319097 2050462464 net.cpp:380] accuracy <- label_data_1_split_0
I0501 15:49:42.319104 2050462464 net.cpp:338] accuracy -> accuracy
I0501 15:49:42.319114 2050462464 net.cpp:113] Setting up accuracy
I0501 15:49:42.319120 2050462464 net.cpp:120] Top shape: (1)
I0501 15:49:42.319128 2050462464 layer_factory.hpp:74] Creating layer loss
I0501 15:49:42.319135 2050462464 net.cpp:84] Creating Layer loss
I0501 15:49:42.319180 2050462464 net.cpp:380] loss <- fc8_VOC_fc8_VOC_0_split_1
I0501 15:49:42.319196 2050462464 net.cpp:380] loss <- label_data_1_split_1
I0501 15:49:42.319206 2050462464 net.cpp:338] loss -> loss
I0501 15:49:42.319216 2050462464 net.cpp:113] Setting up loss
I0501 15:49:42.319226 2050462464 layer_factory.hpp:74] Creating layer loss
I0501 15:49:42.319288 2050462464 net.cpp:120] Top shape: (1)
I0501 15:49:42.319303 2050462464 net.cpp:122]     with loss weight 1
I0501 15:49:42.319313 2050462464 net.cpp:167] loss needs backward computation.
I0501 15:49:42.319319 2050462464 net.cpp:169] accuracy does not need backward computation.
I0501 15:49:42.319324 2050462464 net.cpp:167] fc8_VOC_fc8_VOC_0_split needs backward computation.
I0501 15:49:42.319332 2050462464 net.cpp:167] fc8_VOC needs backward computation.
I0501 15:49:42.319341 2050462464 net.cpp:167] drop7 needs backward computation.
I0501 15:49:42.319349 2050462464 net.cpp:167] relu7 needs backward computation.
I0501 15:49:42.319375 2050462464 net.cpp:167] fc7 needs backward computation.
I0501 15:49:42.319403 2050462464 net.cpp:167] drop6 needs backward computation.
I0501 15:49:42.319408 2050462464 net.cpp:167] relu6 needs backward computation.
I0501 15:49:42.319413 2050462464 net.cpp:167] fc6 needs backward computation.
I0501 15:49:42.319419 2050462464 net.cpp:167] pool5 needs backward computation.
I0501 15:49:42.319424 2050462464 net.cpp:167] relu5 needs backward computation.
I0501 15:49:42.319429 2050462464 net.cpp:167] conv5 needs backward computation.
I0501 15:49:42.319434 2050462464 net.cpp:167] relu4 needs backward computation.
I0501 15:49:42.319511 2050462464 net.cpp:167] conv4 needs backward computation.
I0501 15:49:42.319555 2050462464 net.cpp:167] relu3 needs backward computation.
I0501 15:49:42.319586 2050462464 net.cpp:167] conv3 needs backward computation.
I0501 15:49:42.319597 2050462464 net.cpp:167] norm2 needs backward computation.
I0501 15:49:42.319602 2050462464 net.cpp:167] pool2 needs backward computation.
I0501 15:49:42.319608 2050462464 net.cpp:167] relu2 needs backward computation.
I0501 15:49:42.319617 2050462464 net.cpp:167] conv2 needs backward computation.
I0501 15:49:42.319625 2050462464 net.cpp:167] norm1 needs backward computation.
I0501 15:49:42.319630 2050462464 net.cpp:167] pool1 needs backward computation.
I0501 15:49:42.319665 2050462464 net.cpp:167] relu1 needs backward computation.
I0501 15:49:42.319671 2050462464 net.cpp:167] conv1 needs backward computation.
I0501 15:49:42.319677 2050462464 net.cpp:169] label_data_1_split does not need backward computation.
I0501 15:49:42.319682 2050462464 net.cpp:169] data does not need backward computation.
I0501 15:49:42.319686 2050462464 net.cpp:205] This network produces output accuracy
I0501 15:49:42.319692 2050462464 net.cpp:205] This network produces output loss
I0501 15:49:42.319711 2050462464 net.cpp:447] Collecting Learning Rate and Weight Decay.
I0501 15:49:42.319736 2050462464 net.cpp:217] Network initialization done.
I0501 15:49:42.319758 2050462464 net.cpp:218] Memory required for data: 439053576
I0501 15:49:42.319895 2050462464 solver.cpp:42] Solver scaffolding done.
I0501 15:49:42.320227 2050462464 caffe.cpp:86] Finetuning from bvlc_reference_rcnn_ilsvrc13.caffemodel
E0501 15:49:43.151669 2050462464 upgrade_proto.cpp:618] Attempting to upgrade input file specified using deprecated V1LayerParameter: bvlc_reference_rcnn_ilsvrc13.caffemodel
I0501 15:49:43.666364 2050462464 upgrade_proto.cpp:626] Successfully upgraded file specified using deprecated V1LayerParameter
E0501 15:49:44.439512 2050462464 upgrade_proto.cpp:618] Attempting to upgrade input file specified using deprecated V1LayerParameter: bvlc_reference_rcnn_ilsvrc13.caffemodel
I0501 15:49:44.776233 2050462464 upgrade_proto.cpp:626] Successfully upgraded file specified using deprecated V1LayerParameter
I0501 15:49:44.924295 2050462464 solver.cpp:222] Solving CaffeNet
I0501 15:49:44.924330 2050462464 solver.cpp:223] Learning Rate Policy: step
I0501 15:49:44.924657 2050462464 solver.cpp:266] Iteration 0, Testing net (#0)
I0501 16:34:30.973464 2050462464 solver.cpp:315]     Test net output #0: accuracy = 0.237094
I0501 16:34:30.973631 2050462464 solver.cpp:315]     Test net output #1: loss = 1.59138 (* 1 = 1.59138 loss)
I0501 16:34:37.858831 2050462464 solver.cpp:189] Iteration 0, loss = 1.60353
I0501 16:34:37.858865 2050462464 solver.cpp:204]     Train net output #0: loss = 1.60353 (* 1 = 1.60353 loss)
I0501 16:34:37.859151 2050462464 solver.cpp:464] Iteration 0, lr = 0.001
I0501 16:36:50.798899 2050462464 solver.cpp:189] Iteration 20, loss = 0.220773
I0501 16:36:50.799206 2050462464 solver.cpp:204]     Train net output #0: loss = 0.220773 (* 1 = 0.220773 loss)
I0501 16:36:50.799221 2050462464 solver.cpp:464] Iteration 20, lr = 0.001
I0501 16:39:02.309976 2050462464 solver.cpp:189] Iteration 40, loss = 0.290698
I0501 16:39:02.310036 2050462464 solver.cpp:204]     Train net output #0: loss = 0.290698 (* 1 = 0.290698 loss)
I0501 16:39:02.310050 2050462464 solver.cpp:464] Iteration 40, lr = 0.001
I0501 16:41:14.523991 2050462464 solver.cpp:189] Iteration 60, loss = 0.114269
I0501 16:41:14.524042 2050462464 solver.cpp:204]     Train net output #0: loss = 0.114269 (* 1 = 0.114269 loss)
I0501 16:41:14.524056 2050462464 solver.cpp:464] Iteration 60, lr = 0.001
I0501 16:43:29.525604 2050462464 solver.cpp:189] Iteration 80, loss = 1.55483
I0501 16:43:29.525652 2050462464 solver.cpp:204]     Train net output #0: loss = 1.55483 (* 1 = 1.55483 loss)
I0501 16:43:29.525691 2050462464 solver.cpp:464] Iteration 80, lr = 0.001
I0501 16:45:44.510294 2050462464 solver.cpp:189] Iteration 100, loss = 2.58098
I0501 16:45:44.510582 2050462464 solver.cpp:204]     Train net output #0: loss = 2.58098 (* 1 = 2.58098 loss)
I0501 16:45:44.510594 2050462464 solver.cpp:464] Iteration 100, lr = 0.001
I0501 16:47:59.925809 2050462464 solver.cpp:189] Iteration 120, loss = 1.09286
I0501 16:47:59.925855 2050462464 solver.cpp:204]     Train net output #0: loss = 1.09286 (* 1 = 1.09286 loss)
I0501 16:47:59.925865 2050462464 solver.cpp:464] Iteration 120, lr = 0.001
I0501 16:50:15.732357 2050462464 solver.cpp:189] Iteration 140, loss = 0.947292
I0501 16:50:15.732692 2050462464 solver.cpp:204]     Train net output #0: loss = 0.947292 (* 1 = 0.947292 loss)
I0501 16:50:15.732713 2050462464 solver.cpp:464] Iteration 140, lr = 0.001
I0501 16:52:30.515728 2050462464 solver.cpp:189] Iteration 160, loss = 0.886139
I0501 16:52:30.515893 2050462464 solver.cpp:204]     Train net output #0: loss = 0.886139 (* 1 = 0.886139 loss)
I0501 16:52:30.515914 2050462464 solver.cpp:464] Iteration 160, lr = 0.001
I0501 16:54:44.109930 2050462464 solver.cpp:189] Iteration 180, loss = 1.95188
I0501 16:54:44.109989 2050462464 solver.cpp:204]     Train net output #0: loss = 1.95188 (* 1 = 1.95188 loss)
I0501 16:54:44.110007 2050462464 solver.cpp:464] Iteration 180, lr = 0.001
I0501 16:56:55.687023 2050462464 solver.cpp:189] Iteration 200, loss = 2.46913
I0501 16:56:55.687072 2050462464 solver.cpp:204]     Train net output #0: loss = 2.46913 (* 1 = 2.46913 loss)
I0501 16:56:55.687080 2050462464 solver.cpp:464] Iteration 200, lr = 0.001
I0501 16:59:06.624388 2050462464 solver.cpp:189] Iteration 220, loss = 1.40864
I0501 16:59:06.624441 2050462464 solver.cpp:204]     Train net output #0: loss = 1.40864 (* 1 = 1.40864 loss)
I0501 16:59:06.624451 2050462464 solver.cpp:464] Iteration 220, lr = 0.001
I0501 17:01:18.939764 2050462464 solver.cpp:189] Iteration 240, loss = 0.000145627
I0501 17:01:18.940184 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00014573 (* 1 = 0.00014573 loss)
I0501 17:01:18.940232 2050462464 solver.cpp:464] Iteration 240, lr = 0.001
I0501 17:03:30.668267 2050462464 solver.cpp:189] Iteration 260, loss = -7.35207e-08
I0501 17:03:30.668571 2050462464 solver.cpp:204]     Train net output #0: loss = 2.98023e-08 (* 1 = 2.98023e-08 loss)
I0501 17:03:30.668592 2050462464 solver.cpp:464] Iteration 260, lr = 0.001
I0501 17:05:40.863773 2050462464 solver.cpp:189] Iteration 280, loss = -1.03323e-07
I0501 17:05:40.863844 2050462464 solver.cpp:204]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0501 17:05:40.863870 2050462464 solver.cpp:464] Iteration 280, lr = 0.001
I0501 17:07:51.030885 2050462464 solver.cpp:189] Iteration 300, loss = 3.47443e-07
I0501 17:07:51.031173 2050462464 solver.cpp:204]     Train net output #0: loss = 4.50767e-07 (* 1 = 4.50767e-07 loss)
I0501 17:07:51.031201 2050462464 solver.cpp:464] Iteration 300, lr = 0.001
I0501 17:10:00.511430 2050462464 solver.cpp:189] Iteration 320, loss = 0.00262626
I0501 17:10:00.512485 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00262611 (* 1 = 0.00262611 loss)
I0501 17:10:00.512511 2050462464 solver.cpp:464] Iteration 320, lr = 0.001
I0501 17:14:12.138869 2050462464 solver.cpp:189] Iteration 340, loss = 1.55764e-07
I0501 17:14:12.139145 2050462464 solver.cpp:204]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0501 17:14:12.139160 2050462464 solver.cpp:464] Iteration 340, lr = 0.001
I0501 17:18:24.707775 2050462464 solver.cpp:189] Iteration 360, loss = 1.55764e-07
I0501 17:18:24.708077 2050462464 solver.cpp:204]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0501 17:18:24.708093 2050462464 solver.cpp:464] Iteration 360, lr = 0.001
I0501 17:22:23.654144 2050462464 solver.cpp:189] Iteration 380, loss = 1.55764e-07
I0501 17:22:23.654433 2050462464 solver.cpp:204]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0501 17:22:23.654448 2050462464 solver.cpp:464] Iteration 380, lr = 0.001
I0501 17:26:13.796309 2050462464 solver.cpp:189] Iteration 400, loss = 1.55764e-07
I0501 17:26:13.797117 2050462464 solver.cpp:204]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0501 17:26:13.797132 2050462464 solver.cpp:464] Iteration 400, lr = 0.001
I0501 17:30:23.801976 2050462464 solver.cpp:189] Iteration 420, loss = 5.22942e-07
I0501 17:30:23.802239 2050462464 solver.cpp:204]     Train net output #0: loss = 3.66945e-07 (* 1 = 3.66945e-07 loss)
I0501 17:30:23.802253 2050462464 solver.cpp:464] Iteration 420, lr = 0.001
I0501 17:34:30.854826 2050462464 solver.cpp:189] Iteration 440, loss = 1.55531e-07
I0501 17:34:30.855176 2050462464 solver.cpp:204]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0501 17:34:30.855196 2050462464 solver.cpp:464] Iteration 440, lr = 0.001
I0501 17:38:29.378093 2050462464 solver.cpp:189] Iteration 460, loss = 1.62981e-07
I0501 17:38:29.378448 2050462464 solver.cpp:204]     Train net output #0: loss = 7.45058e-09 (* 1 = 7.45058e-09 loss)
I0501 17:38:29.378463 2050462464 solver.cpp:464] Iteration 460, lr = 0.001
I0501 17:42:39.921043 2050462464 solver.cpp:189] Iteration 480, loss = 1.55531e-07
I0501 17:42:39.921344 2050462464 solver.cpp:204]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0501 17:42:39.921362 2050462464 solver.cpp:464] Iteration 480, lr = 0.001
I0501 17:46:29.024324 2050462464 solver.cpp:334] Snapshotting to caffenet_train_iter_500.caffemodel
I0501 17:46:31.245702 2050462464 solver.cpp:342] Snapshotting solver state to caffenet_train_iter_500.solverstate
I0501 17:46:45.941182 2050462464 solver.cpp:189] Iteration 500, loss = 1.55531e-07
I0501 17:46:45.941215 2050462464 solver.cpp:204]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0501 17:46:45.941223 2050462464 solver.cpp:464] Iteration 500, lr = 0.001
I0501 17:49:43.233011 2050462464 solver.cpp:189] Iteration 520, loss = 12.9648
I0501 17:49:43.233297 2050462464 solver.cpp:204]     Train net output #0: loss = 12.9648 (* 1 = 12.9648 loss)
I0501 17:49:43.233310 2050462464 solver.cpp:464] Iteration 520, lr = 0.001
I0501 17:51:52.417737 2050462464 solver.cpp:189] Iteration 540, loss = 3.38163
I0501 17:51:52.417785 2050462464 solver.cpp:204]     Train net output #0: loss = 3.38163 (* 1 = 3.38163 loss)
I0501 17:51:52.417794 2050462464 solver.cpp:464] Iteration 540, lr = 0.001
I0501 17:53:59.746134 2050462464 solver.cpp:189] Iteration 560, loss = 0.778139
I0501 17:53:59.746196 2050462464 solver.cpp:204]     Train net output #0: loss = 0.778141 (* 1 = 0.778141 loss)
I0501 17:53:59.746212 2050462464 solver.cpp:464] Iteration 560, lr = 0.001
I0501 17:56:10.655086 2050462464 solver.cpp:189] Iteration 580, loss = 1.82962
I0501 17:56:10.655130 2050462464 solver.cpp:204]     Train net output #0: loss = 1.82962 (* 1 = 1.82962 loss)
I0501 17:56:10.655163 2050462464 solver.cpp:464] Iteration 580, lr = 0.001
I0501 17:58:24.648980 2050462464 solver.cpp:189] Iteration 600, loss = 1.12665
I0501 17:58:24.649045 2050462464 solver.cpp:204]     Train net output #0: loss = 1.12665 (* 1 = 1.12665 loss)
I0501 17:58:24.649062 2050462464 solver.cpp:464] Iteration 600, lr = 0.001
I0501 18:00:39.614205 2050462464 solver.cpp:189] Iteration 620, loss = 1.38572
I0501 18:00:39.615330 2050462464 solver.cpp:204]     Train net output #0: loss = 1.38573 (* 1 = 1.38573 loss)
I0501 18:00:39.615521 2050462464 solver.cpp:464] Iteration 620, lr = 0.001
I0501 18:02:55.844835 2050462464 solver.cpp:189] Iteration 640, loss = 2.42424
I0501 18:02:55.845592 2050462464 solver.cpp:204]     Train net output #0: loss = 2.42424 (* 1 = 2.42424 loss)
I0501 18:02:55.845612 2050462464 solver.cpp:464] Iteration 640, lr = 0.001
I0501 18:05:12.537418 2050462464 solver.cpp:189] Iteration 660, loss = 0.850178
I0501 18:05:12.537467 2050462464 solver.cpp:204]     Train net output #0: loss = 0.850181 (* 1 = 0.850181 loss)
I0501 18:05:12.537475 2050462464 solver.cpp:464] Iteration 660, lr = 0.001
I0501 18:07:29.451225 2050462464 solver.cpp:189] Iteration 680, loss = 1.02475
I0501 18:07:29.451993 2050462464 solver.cpp:204]     Train net output #0: loss = 1.02476 (* 1 = 1.02476 loss)
I0501 18:07:29.452013 2050462464 solver.cpp:464] Iteration 680, lr = 0.001
I0501 18:09:49.834719 2050462464 solver.cpp:189] Iteration 700, loss = 0.28529
I0501 18:09:49.834993 2050462464 solver.cpp:204]     Train net output #0: loss = 0.285292 (* 1 = 0.285292 loss)
I0501 18:09:49.835006 2050462464 solver.cpp:464] Iteration 700, lr = 0.001
I0501 18:11:58.502537 2050462464 solver.cpp:189] Iteration 720, loss = 2.66281
I0501 18:11:58.502588 2050462464 solver.cpp:204]     Train net output #0: loss = 2.66281 (* 1 = 2.66281 loss)
I0501 18:11:58.502604 2050462464 solver.cpp:464] Iteration 720, lr = 0.001
I0501 18:14:07.688565 2050462464 solver.cpp:189] Iteration 740, loss = 1.175
I0501 18:14:07.688612 2050462464 solver.cpp:204]     Train net output #0: loss = 1.175 (* 1 = 1.175 loss)
I0501 18:14:07.688621 2050462464 solver.cpp:464] Iteration 740, lr = 0.001
I0501 18:16:23.019107 2050462464 solver.cpp:189] Iteration 760, loss = -2.62244e-06
I0501 18:16:23.019896 2050462464 solver.cpp:204]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0501 18:16:23.019908 2050462464 solver.cpp:464] Iteration 760, lr = 0.001
I0501 18:20:34.526257 2050462464 solver.cpp:189] Iteration 780, loss = -2.62244e-06
I0501 18:20:34.526307 2050462464 solver.cpp:204]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0501 18:20:34.526316 2050462464 solver.cpp:464] Iteration 780, lr = 0.001
I0501 18:25:25.067792 2050462464 solver.cpp:189] Iteration 800, loss = -2.62244e-06
I0501 18:25:25.067841 2050462464 solver.cpp:204]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0501 18:25:25.067849 2050462464 solver.cpp:464] Iteration 800, lr = 0.001
I0501 18:30:19.375358 2050462464 solver.cpp:189] Iteration 820, loss = -2.62244e-06
I0501 18:30:19.375619 2050462464 solver.cpp:204]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0501 18:30:19.375633 2050462464 solver.cpp:464] Iteration 820, lr = 0.001
I0501 18:33:03.643388 2050462464 solver.cpp:189] Iteration 840, loss = -5.25716e-06
I0501 18:33:03.643668 2050462464 solver.cpp:204]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0501 18:33:03.643681 2050462464 solver.cpp:464] Iteration 840, lr = 0.001
I0501 18:35:18.559672 2050462464 solver.cpp:189] Iteration 860, loss = -5.25716e-06
I0501 18:35:18.560016 2050462464 solver.cpp:204]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0501 18:35:18.560039 2050462464 solver.cpp:464] Iteration 860, lr = 0.001
I0501 18:37:28.790112 2050462464 solver.cpp:189] Iteration 880, loss = -5.25716e-06
I0501 18:37:28.790868 2050462464 solver.cpp:204]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0501 18:37:28.790881 2050462464 solver.cpp:464] Iteration 880, lr = 0.001
I0501 18:39:41.608209 2050462464 solver.cpp:189] Iteration 900, loss = -5.25716e-06
I0501 18:39:41.608258 2050462464 solver.cpp:204]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0501 18:39:41.608273 2050462464 solver.cpp:464] Iteration 900, lr = 0.001
I0501 18:41:53.402937 2050462464 solver.cpp:189] Iteration 920, loss = -5.25716e-06
I0501 18:41:53.402983 2050462464 solver.cpp:204]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0501 18:41:53.402997 2050462464 solver.cpp:464] Iteration 920, lr = 0.001
I0501 18:44:22.201448 2050462464 solver.cpp:189] Iteration 940, loss = -5.25716e-06
I0501 18:44:22.201509 2050462464 solver.cpp:204]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0501 18:44:22.201524 2050462464 solver.cpp:464] Iteration 940, lr = 0.001
I0501 18:46:52.953663 2050462464 solver.cpp:189] Iteration 960, loss = -5.25716e-06
I0501 18:46:52.953847 2050462464 solver.cpp:204]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0501 18:46:52.953876 2050462464 solver.cpp:464] Iteration 960, lr = 0.001
I0501 18:49:32.250191 2050462464 solver.cpp:189] Iteration 980, loss = -5.25716e-06
I0501 18:49:32.250253 2050462464 solver.cpp:204]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0501 18:49:32.250269 2050462464 solver.cpp:464] Iteration 980, lr = 0.001
I0501 18:51:48.932317 2050462464 solver.cpp:334] Snapshotting to caffenet_train_iter_1000.caffemodel
I0501 18:51:50.842367 2050462464 solver.cpp:342] Snapshotting solver state to caffenet_train_iter_1000.solverstate
I0501 18:51:52.186580 2050462464 solver.cpp:266] Iteration 1000, Testing net (#0)
I0501 19:22:40.932617 2050462464 solver.cpp:315]     Test net output #0: accuracy = 0.400562
I0501 19:22:40.933423 2050462464 solver.cpp:315]     Test net output #1: loss = 50.7097 (* 1 = 50.7097 loss)
I0501 19:22:45.259862 2050462464 solver.cpp:189] Iteration 1000, loss = -5.25716e-06
I0501 19:22:45.259899 2050462464 solver.cpp:204]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0501 19:22:45.259910 2050462464 solver.cpp:464] Iteration 1000, lr = 0.001
I0501 19:24:13.876318 2050462464 solver.cpp:189] Iteration 1020, loss = 1.2121
I0501 19:24:13.876365 2050462464 solver.cpp:204]     Train net output #0: loss = 1.2121 (* 1 = 1.2121 loss)
I0501 19:24:13.876379 2050462464 solver.cpp:464] Iteration 1020, lr = 0.001
I0501 19:25:41.957213 2050462464 solver.cpp:189] Iteration 1040, loss = 0.791761
I0501 19:25:41.957258 2050462464 solver.cpp:204]     Train net output #0: loss = 0.791767 (* 1 = 0.791767 loss)
I0501 19:25:41.957267 2050462464 solver.cpp:464] Iteration 1040, lr = 0.001
I0501 19:27:09.253451 2050462464 solver.cpp:189] Iteration 1060, loss = 1.21416
I0501 19:27:09.253504 2050462464 solver.cpp:204]     Train net output #0: loss = 1.21417 (* 1 = 1.21417 loss)
I0501 19:27:09.253514 2050462464 solver.cpp:464] Iteration 1060, lr = 0.001
I0501 19:28:37.343297 2050462464 solver.cpp:189] Iteration 1080, loss = 1.20084
I0501 19:28:37.343349 2050462464 solver.cpp:204]     Train net output #0: loss = 1.20084 (* 1 = 1.20084 loss)
I0501 19:28:37.343358 2050462464 solver.cpp:464] Iteration 1080, lr = 0.001
I0501 19:30:05.730322 2050462464 solver.cpp:189] Iteration 1100, loss = 1.08884
I0501 19:30:05.730373 2050462464 solver.cpp:204]     Train net output #0: loss = 1.08884 (* 1 = 1.08884 loss)
I0501 19:30:05.730383 2050462464 solver.cpp:464] Iteration 1100, lr = 0.001
I0501 19:31:33.903345 2050462464 solver.cpp:189] Iteration 1120, loss = 1.38088
I0501 19:31:33.903398 2050462464 solver.cpp:204]     Train net output #0: loss = 1.38089 (* 1 = 1.38089 loss)
I0501 19:31:33.903408 2050462464 solver.cpp:464] Iteration 1120, lr = 0.001
I0501 19:33:01.922889 2050462464 solver.cpp:189] Iteration 1140, loss = 1.4097
I0501 19:33:01.922932 2050462464 solver.cpp:204]     Train net output #0: loss = 1.4097 (* 1 = 1.4097 loss)
I0501 19:33:01.922941 2050462464 solver.cpp:464] Iteration 1140, lr = 0.001
I0501 19:34:30.096457 2050462464 solver.cpp:189] Iteration 1160, loss = 2.63978
I0501 19:34:30.096503 2050462464 solver.cpp:204]     Train net output #0: loss = 2.63978 (* 1 = 2.63978 loss)
I0501 19:34:30.096511 2050462464 solver.cpp:464] Iteration 1160, lr = 0.001
I0501 19:35:58.077523 2050462464 solver.cpp:189] Iteration 1180, loss = 0.932693
I0501 19:35:58.077565 2050462464 solver.cpp:204]     Train net output #0: loss = 0.932699 (* 1 = 0.932699 loss)
I0501 19:35:58.077574 2050462464 solver.cpp:464] Iteration 1180, lr = 0.001
I0501 19:37:26.349931 2050462464 solver.cpp:189] Iteration 1200, loss = 1.14823
I0501 19:37:26.349973 2050462464 solver.cpp:204]     Train net output #0: loss = 1.14823 (* 1 = 1.14823 loss)
I0501 19:37:26.349982 2050462464 solver.cpp:464] Iteration 1200, lr = 0.001
I0501 19:38:54.819144 2050462464 solver.cpp:189] Iteration 1220, loss = 0.695365
I0501 19:38:54.819197 2050462464 solver.cpp:204]     Train net output #0: loss = 0.695371 (* 1 = 0.695371 loss)
I0501 19:38:54.819207 2050462464 solver.cpp:464] Iteration 1220, lr = 0.001
I0501 19:40:22.984380 2050462464 solver.cpp:189] Iteration 1240, loss = 0.916303
I0501 19:40:22.984433 2050462464 solver.cpp:204]     Train net output #0: loss = 0.916309 (* 1 = 0.916309 loss)
I0501 19:40:22.984442 2050462464 solver.cpp:464] Iteration 1240, lr = 0.001
I0501 19:41:51.047022 2050462464 solver.cpp:189] Iteration 1260, loss = 0.224268
I0501 19:41:51.047066 2050462464 solver.cpp:204]     Train net output #0: loss = 0.224274 (* 1 = 0.224274 loss)
I0501 19:41:51.047075 2050462464 solver.cpp:464] Iteration 1260, lr = 0.001
I0501 19:43:19.159574 2050462464 solver.cpp:189] Iteration 1280, loss = 0.0763326
I0501 19:43:19.159739 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0763388 (* 1 = 0.0763388 loss)
I0501 19:43:19.159757 2050462464 solver.cpp:464] Iteration 1280, lr = 0.001
I0501 19:44:47.520768 2050462464 solver.cpp:189] Iteration 1300, loss = 0.0402681
I0501 19:44:47.520820 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0402743 (* 1 = 0.0402743 loss)
I0501 19:44:47.520835 2050462464 solver.cpp:464] Iteration 1300, lr = 0.001
I0501 19:46:19.678217 2050462464 solver.cpp:189] Iteration 1320, loss = 0.0312524
I0501 19:46:19.678264 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0312585 (* 1 = 0.0312585 loss)
I0501 19:46:19.678275 2050462464 solver.cpp:464] Iteration 1320, lr = 0.001
I0501 19:47:52.228396 2050462464 solver.cpp:189] Iteration 1340, loss = 3.36688
I0501 19:47:52.228452 2050462464 solver.cpp:204]     Train net output #0: loss = 3.36689 (* 1 = 3.36689 loss)
I0501 19:47:52.228467 2050462464 solver.cpp:464] Iteration 1340, lr = 0.001
I0501 19:49:24.444037 2050462464 solver.cpp:189] Iteration 1360, loss = 0.241217
I0501 19:49:24.444097 2050462464 solver.cpp:204]     Train net output #0: loss = 0.241224 (* 1 = 0.241224 loss)
I0501 19:49:24.444105 2050462464 solver.cpp:464] Iteration 1360, lr = 0.001
I0501 19:50:57.145454 2050462464 solver.cpp:189] Iteration 1380, loss = 0.0551506
I0501 19:50:57.145500 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0551568 (* 1 = 0.0551568 loss)
I0501 19:50:57.145516 2050462464 solver.cpp:464] Iteration 1380, lr = 0.001
I0501 19:52:29.449563 2050462464 solver.cpp:189] Iteration 1400, loss = 0.0351488
I0501 19:52:29.449709 2050462464 solver.cpp:204]     Train net output #0: loss = 0.035155 (* 1 = 0.035155 loss)
I0501 19:52:29.449740 2050462464 solver.cpp:464] Iteration 1400, lr = 0.001
I0501 19:53:59.164306 2050462464 solver.cpp:189] Iteration 1420, loss = 0.0204749
I0501 19:53:59.164350 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0204811 (* 1 = 0.0204811 loss)
I0501 19:53:59.164360 2050462464 solver.cpp:464] Iteration 1420, lr = 0.001
I0501 19:55:28.478610 2050462464 solver.cpp:189] Iteration 1440, loss = 0.01356
I0501 19:55:28.478657 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0135662 (* 1 = 0.0135662 loss)
I0501 19:55:28.478665 2050462464 solver.cpp:464] Iteration 1440, lr = 0.001
I0501 19:56:56.670258 2050462464 solver.cpp:189] Iteration 1460, loss = 0.0103007
I0501 19:56:56.670310 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0103069 (* 1 = 0.0103069 loss)
I0501 19:56:56.670320 2050462464 solver.cpp:464] Iteration 1460, lr = 0.001
I0501 21:58:44.390247 2050462464 solver.cpp:189] Iteration 1480, loss = 0.0155321
I0501 21:58:44.390300 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0155383 (* 1 = 0.0155383 loss)
I0501 21:58:44.390311 2050462464 solver.cpp:464] Iteration 1480, lr = 0.001
I0502 02:02:02.654456 2050462464 solver.cpp:334] Snapshotting to caffenet_train_iter_1500.caffemodel
I0502 02:02:07.990561 2050462464 solver.cpp:342] Snapshotting solver state to caffenet_train_iter_1500.solverstate
I0502 03:02:22.218835 2050462464 solver.cpp:189] Iteration 1500, loss = 0.0115556
I0502 03:02:22.218879 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0115618 (* 1 = 0.0115618 loss)
I0502 03:02:22.218889 2050462464 solver.cpp:464] Iteration 1500, lr = 0.001
I0502 07:05:52.090901 2050462464 solver.cpp:189] Iteration 1520, loss = 0.0127055
I0502 07:05:52.090945 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0127117 (* 1 = 0.0127117 loss)
I0502 07:05:52.090955 2050462464 solver.cpp:464] Iteration 1520, lr = 0.001
I0502 11:09:11.187556 2050462464 solver.cpp:189] Iteration 1540, loss = 3.29043
I0502 11:09:11.187688 2050462464 solver.cpp:204]     Train net output #0: loss = 3.29044 (* 1 = 3.29044 loss)
I0502 11:09:11.187733 2050462464 solver.cpp:464] Iteration 1540, lr = 0.001
I0502 13:07:38.539501 2050462464 solver.cpp:189] Iteration 1560, loss = 0.74454
I0502 13:07:38.540208 2050462464 solver.cpp:204]     Train net output #0: loss = 0.744547 (* 1 = 0.744547 loss)
I0502 13:07:38.540222 2050462464 solver.cpp:464] Iteration 1560, lr = 0.001
I0502 13:09:14.031219 2050462464 solver.cpp:189] Iteration 1580, loss = 0.996551
I0502 13:09:14.031285 2050462464 solver.cpp:204]     Train net output #0: loss = 0.996558 (* 1 = 0.996558 loss)
I0502 13:09:14.031294 2050462464 solver.cpp:464] Iteration 1580, lr = 0.001
I0502 13:10:45.842185 2050462464 solver.cpp:189] Iteration 1600, loss = 1.00713
I0502 13:10:45.842232 2050462464 solver.cpp:204]     Train net output #0: loss = 1.00714 (* 1 = 1.00714 loss)
I0502 13:10:45.842242 2050462464 solver.cpp:464] Iteration 1600, lr = 0.001
I0502 13:12:24.990480 2050462464 solver.cpp:189] Iteration 1620, loss = 0.974582
I0502 13:12:24.991228 2050462464 solver.cpp:204]     Train net output #0: loss = 0.974588 (* 1 = 0.974588 loss)
I0502 13:12:24.991240 2050462464 solver.cpp:464] Iteration 1620, lr = 0.001
I0502 13:14:04.875967 2050462464 solver.cpp:189] Iteration 1640, loss = 1.1182
I0502 13:14:04.876010 2050462464 solver.cpp:204]     Train net output #0: loss = 1.1182 (* 1 = 1.1182 loss)
I0502 13:14:04.876019 2050462464 solver.cpp:464] Iteration 1640, lr = 0.001
I0502 13:15:45.167376 2050462464 solver.cpp:189] Iteration 1660, loss = 1.21235
I0502 13:15:45.168112 2050462464 solver.cpp:204]     Train net output #0: loss = 1.21236 (* 1 = 1.21236 loss)
I0502 13:15:45.168125 2050462464 solver.cpp:464] Iteration 1660, lr = 0.001
I0502 13:17:24.291067 2050462464 solver.cpp:189] Iteration 1680, loss = 1.18378
I0502 13:17:24.291879 2050462464 solver.cpp:204]     Train net output #0: loss = 1.18379 (* 1 = 1.18379 loss)
I0502 13:17:24.291893 2050462464 solver.cpp:464] Iteration 1680, lr = 0.001
I0502 13:19:03.927302 2050462464 solver.cpp:189] Iteration 1700, loss = 0.915358
I0502 13:19:03.928212 2050462464 solver.cpp:204]     Train net output #0: loss = 0.915364 (* 1 = 0.915364 loss)
I0502 13:19:03.928228 2050462464 solver.cpp:464] Iteration 1700, lr = 0.001
I0502 13:20:39.385438 2050462464 solver.cpp:189] Iteration 1720, loss = 0.442635
I0502 13:20:39.385496 2050462464 solver.cpp:204]     Train net output #0: loss = 0.442641 (* 1 = 0.442641 loss)
I0502 13:20:39.385512 2050462464 solver.cpp:464] Iteration 1720, lr = 0.001
I0502 13:22:16.474447 2050462464 solver.cpp:189] Iteration 1740, loss = 0.904129
I0502 13:22:16.475209 2050462464 solver.cpp:204]     Train net output #0: loss = 0.904135 (* 1 = 0.904135 loss)
I0502 13:22:16.475224 2050462464 solver.cpp:464] Iteration 1740, lr = 0.001
I0502 13:23:57.504503 2050462464 solver.cpp:189] Iteration 1760, loss = 0.947443
I0502 13:23:57.504784 2050462464 solver.cpp:204]     Train net output #0: loss = 0.947449 (* 1 = 0.947449 loss)
I0502 13:23:57.504801 2050462464 solver.cpp:464] Iteration 1760, lr = 0.001
I0502 13:25:36.997042 2050462464 solver.cpp:189] Iteration 1780, loss = 0.154587
I0502 13:25:36.997359 2050462464 solver.cpp:204]     Train net output #0: loss = 0.154594 (* 1 = 0.154594 loss)
I0502 13:25:36.997377 2050462464 solver.cpp:464] Iteration 1780, lr = 0.001
I0502 13:27:21.641697 2050462464 solver.cpp:189] Iteration 1800, loss = 0.0582205
I0502 13:27:21.641963 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0582267 (* 1 = 0.0582267 loss)
I0502 13:27:21.641976 2050462464 solver.cpp:464] Iteration 1800, lr = 0.001
I0502 13:29:21.934669 2050462464 solver.cpp:189] Iteration 1820, loss = 0.0274916
I0502 13:29:21.934962 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0274978 (* 1 = 0.0274978 loss)
I0502 13:29:21.934983 2050462464 solver.cpp:464] Iteration 1820, lr = 0.001
I0502 13:31:01.319247 2050462464 solver.cpp:189] Iteration 1840, loss = 0.0289611
I0502 13:31:01.319311 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0289673 (* 1 = 0.0289673 loss)
I0502 13:31:01.319330 2050462464 solver.cpp:464] Iteration 1840, lr = 0.001
I0502 13:32:42.417567 2050462464 solver.cpp:189] Iteration 1860, loss = 1.00615
I0502 13:32:42.417613 2050462464 solver.cpp:204]     Train net output #0: loss = 1.00615 (* 1 = 1.00615 loss)
I0502 13:32:42.417621 2050462464 solver.cpp:464] Iteration 1860, lr = 0.001
I0502 13:34:40.101992 2050462464 solver.cpp:189] Iteration 1880, loss = 0.0814448
I0502 13:34:40.102273 2050462464 solver.cpp:204]     Train net output #0: loss = 0.081451 (* 1 = 0.081451 loss)
I0502 13:34:40.102287 2050462464 solver.cpp:464] Iteration 1880, lr = 0.001
I0502 13:36:24.832273 2050462464 solver.cpp:189] Iteration 1900, loss = 0.0404424
I0502 13:36:24.832322 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0404486 (* 1 = 0.0404486 loss)
I0502 13:36:24.832337 2050462464 solver.cpp:464] Iteration 1900, lr = 0.001
I0502 13:38:06.385457 2050462464 solver.cpp:189] Iteration 1920, loss = 0.0312751
I0502 13:38:06.385501 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0312813 (* 1 = 0.0312813 loss)
I0502 13:38:06.385511 2050462464 solver.cpp:464] Iteration 1920, lr = 0.001
I0502 13:40:01.016274 2050462464 solver.cpp:189] Iteration 1940, loss = 0.0264138
I0502 13:40:01.016652 2050462464 solver.cpp:204]     Train net output #0: loss = 0.02642 (* 1 = 0.02642 loss)
I0502 13:40:01.016664 2050462464 solver.cpp:464] Iteration 1940, lr = 0.001
I0502 13:41:47.370609 2050462464 solver.cpp:189] Iteration 1960, loss = 0.0230571
I0502 13:41:47.370880 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0230633 (* 1 = 0.0230633 loss)
I0502 13:41:47.370894 2050462464 solver.cpp:464] Iteration 1960, lr = 0.001
I0502 13:43:34.815958 2050462464 solver.cpp:189] Iteration 1980, loss = 0.0199818
I0502 13:43:34.816007 2050462464 solver.cpp:204]     Train net output #0: loss = 0.019988 (* 1 = 0.019988 loss)
I0502 13:43:34.816017 2050462464 solver.cpp:464] Iteration 1980, lr = 0.001
I0502 13:45:28.286833 2050462464 solver.cpp:334] Snapshotting to caffenet_train_iter_2000.caffemodel
I0502 13:45:29.897438 2050462464 solver.cpp:342] Snapshotting solver state to caffenet_train_iter_2000.solverstate
I0502 13:45:31.188359 2050462464 solver.cpp:266] Iteration 2000, Testing net (#0)
I0502 14:18:35.345684 2050462464 solver.cpp:315]     Test net output #0: accuracy = 0.415437
I0502 14:18:35.345973 2050462464 solver.cpp:315]     Test net output #1: loss = 3.03365 (* 1 = 3.03365 loss)
I0502 14:18:40.793683 2050462464 solver.cpp:189] Iteration 2000, loss = 0.0174028
I0502 14:18:40.793725 2050462464 solver.cpp:204]     Train net output #0: loss = 0.017409 (* 1 = 0.017409 loss)
I0502 14:18:40.793758 2050462464 solver.cpp:464] Iteration 2000, lr = 0.001
I0502 14:20:40.539510 2050462464 solver.cpp:189] Iteration 2020, loss = 0.0017854
I0502 14:20:40.539791 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0017916 (* 1 = 0.0017916 loss)
I0502 14:20:40.539808 2050462464 solver.cpp:464] Iteration 2020, lr = 0.001
I0502 14:22:29.143183 2050462464 solver.cpp:189] Iteration 2040, loss = 4.72007
I0502 14:22:29.143229 2050462464 solver.cpp:204]     Train net output #0: loss = 4.72007 (* 1 = 4.72007 loss)
I0502 14:22:29.143239 2050462464 solver.cpp:464] Iteration 2040, lr = 0.001
I0502 14:24:08.317488 2050462464 solver.cpp:189] Iteration 2060, loss = 0.828585
I0502 14:24:08.317533 2050462464 solver.cpp:204]     Train net output #0: loss = 0.828591 (* 1 = 0.828591 loss)
I0502 14:24:08.317574 2050462464 solver.cpp:464] Iteration 2060, lr = 0.001
I0502 14:26:01.436267 2050462464 solver.cpp:189] Iteration 2080, loss = 0.62901
I0502 14:26:01.436555 2050462464 solver.cpp:204]     Train net output #0: loss = 0.629016 (* 1 = 0.629016 loss)
I0502 14:26:01.436568 2050462464 solver.cpp:464] Iteration 2080, lr = 0.001
I0502 14:28:08.979951 2050462464 solver.cpp:189] Iteration 2100, loss = 0.988266
I0502 14:28:08.980273 2050462464 solver.cpp:204]     Train net output #0: loss = 0.988272 (* 1 = 0.988272 loss)
I0502 14:28:08.980284 2050462464 solver.cpp:464] Iteration 2100, lr = 0.001
I0502 14:29:49.333639 2050462464 solver.cpp:189] Iteration 2120, loss = 1.3934
I0502 14:29:49.333685 2050462464 solver.cpp:204]     Train net output #0: loss = 1.39341 (* 1 = 1.39341 loss)
I0502 14:29:49.333694 2050462464 solver.cpp:464] Iteration 2120, lr = 0.001
I0502 14:31:29.692385 2050462464 solver.cpp:189] Iteration 2140, loss = 1.39174
I0502 14:31:29.692446 2050462464 solver.cpp:204]     Train net output #0: loss = 1.39175 (* 1 = 1.39175 loss)
I0502 14:31:29.692456 2050462464 solver.cpp:464] Iteration 2140, lr = 0.001
I0502 14:33:10.239928 2050462464 solver.cpp:189] Iteration 2160, loss = 1.20424
I0502 14:33:10.239974 2050462464 solver.cpp:204]     Train net output #0: loss = 1.20425 (* 1 = 1.20425 loss)
I0502 14:33:10.239984 2050462464 solver.cpp:464] Iteration 2160, lr = 0.001
I0502 14:34:51.163305 2050462464 solver.cpp:189] Iteration 2180, loss = 1.16222
I0502 14:34:51.163353 2050462464 solver.cpp:204]     Train net output #0: loss = 1.16222 (* 1 = 1.16222 loss)
I0502 14:34:51.163363 2050462464 solver.cpp:464] Iteration 2180, lr = 0.001
I0502 14:36:30.099861 2050462464 solver.cpp:189] Iteration 2200, loss = 0.988248
I0502 14:36:30.099905 2050462464 solver.cpp:204]     Train net output #0: loss = 0.988254 (* 1 = 0.988254 loss)
I0502 14:36:30.099913 2050462464 solver.cpp:464] Iteration 2200, lr = 0.001
I0502 14:38:11.236263 2050462464 solver.cpp:189] Iteration 2220, loss = 1.31387
I0502 14:38:11.236307 2050462464 solver.cpp:204]     Train net output #0: loss = 1.31388 (* 1 = 1.31388 loss)
I0502 14:38:11.236315 2050462464 solver.cpp:464] Iteration 2220, lr = 0.001
I0502 14:39:50.548549 2050462464 solver.cpp:189] Iteration 2240, loss = 2.61306
I0502 14:39:50.548600 2050462464 solver.cpp:204]     Train net output #0: loss = 2.61306 (* 1 = 2.61306 loss)
I0502 14:39:50.548614 2050462464 solver.cpp:464] Iteration 2240, lr = 0.001
I0502 14:41:31.956373 2050462464 solver.cpp:189] Iteration 2260, loss = 0.899321
I0502 14:41:31.956506 2050462464 solver.cpp:204]     Train net output #0: loss = 0.899327 (* 1 = 0.899327 loss)
I0502 14:41:31.956531 2050462464 solver.cpp:464] Iteration 2260, lr = 0.001
I0502 14:43:23.360093 2050462464 solver.cpp:189] Iteration 2280, loss = 0.0682685
I0502 14:43:23.360137 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0682747 (* 1 = 0.0682747 loss)
I0502 14:43:23.360148 2050462464 solver.cpp:464] Iteration 2280, lr = 0.001
I0502 14:45:04.672029 2050462464 solver.cpp:189] Iteration 2300, loss = 0.0182392
I0502 14:45:04.672076 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0182454 (* 1 = 0.0182454 loss)
I0502 14:45:04.672086 2050462464 solver.cpp:464] Iteration 2300, lr = 0.001
I0502 14:47:05.726915 2050462464 solver.cpp:189] Iteration 2320, loss = 0.0205405
I0502 14:47:05.727224 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0205467 (* 1 = 0.0205467 loss)
I0502 14:47:05.727239 2050462464 solver.cpp:464] Iteration 2320, lr = 0.001
I0502 14:49:23.128690 2050462464 solver.cpp:189] Iteration 2340, loss = 0.0078105
I0502 14:49:23.128921 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0078167 (* 1 = 0.0078167 loss)
I0502 14:49:23.128936 2050462464 solver.cpp:464] Iteration 2340, lr = 0.001
I0502 14:51:25.138036 2050462464 solver.cpp:189] Iteration 2360, loss = 1.20835
I0502 14:51:25.138355 2050462464 solver.cpp:204]     Train net output #0: loss = 1.20835 (* 1 = 1.20835 loss)
I0502 14:51:25.138375 2050462464 solver.cpp:464] Iteration 2360, lr = 0.001
I0502 14:53:25.418776 2050462464 solver.cpp:189] Iteration 2380, loss = 0.121817
I0502 14:53:25.418817 2050462464 solver.cpp:204]     Train net output #0: loss = 0.121824 (* 1 = 0.121824 loss)
I0502 14:53:25.418825 2050462464 solver.cpp:464] Iteration 2380, lr = 0.001
I0502 14:55:02.330114 2050462464 solver.cpp:189] Iteration 2400, loss = 0.0513783
I0502 14:55:02.330158 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0513849 (* 1 = 0.0513849 loss)
I0502 14:55:02.330168 2050462464 solver.cpp:464] Iteration 2400, lr = 0.001
I0502 14:56:38.432157 2050462464 solver.cpp:189] Iteration 2420, loss = 0.0370334
I0502 14:56:38.432204 2050462464 solver.cpp:204]     Train net output #0: loss = 0.03704 (* 1 = 0.03704 loss)
I0502 14:56:38.432224 2050462464 solver.cpp:464] Iteration 2420, lr = 0.001
I0502 14:58:16.293536 2050462464 solver.cpp:189] Iteration 2440, loss = 0.0303299
I0502 14:58:16.293598 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0303365 (* 1 = 0.0303365 loss)
I0502 14:58:16.293609 2050462464 solver.cpp:464] Iteration 2440, lr = 0.001
I0502 14:59:55.581673 2050462464 solver.cpp:189] Iteration 2460, loss = 0.0259239
I0502 14:59:55.581730 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0259304 (* 1 = 0.0259304 loss)
I0502 14:59:55.581746 2050462464 solver.cpp:464] Iteration 2460, lr = 0.001
I0502 15:01:33.423379 2050462464 solver.cpp:189] Iteration 2480, loss = 0.0227046
I0502 15:01:33.423424 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0227112 (* 1 = 0.0227112 loss)
I0502 15:01:33.423432 2050462464 solver.cpp:464] Iteration 2480, lr = 0.001
I0502 15:03:04.318745 2050462464 solver.cpp:334] Snapshotting to caffenet_train_iter_2500.caffemodel
I0502 15:03:05.859323 2050462464 solver.cpp:342] Snapshotting solver state to caffenet_train_iter_2500.solverstate
I0502 15:03:11.801415 2050462464 solver.cpp:189] Iteration 2500, loss = 0.02022
I0502 15:03:11.801452 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0202266 (* 1 = 0.0202266 loss)
I0502 15:03:11.801466 2050462464 solver.cpp:464] Iteration 2500, lr = 0.001
I0502 15:04:48.441673 2050462464 solver.cpp:189] Iteration 2520, loss = 0.0182267
I0502 15:04:48.441738 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0182333 (* 1 = 0.0182333 loss)
I0502 15:04:48.441754 2050462464 solver.cpp:464] Iteration 2520, lr = 0.001
I0502 15:06:33.227462 2050462464 solver.cpp:189] Iteration 2540, loss = 0.0166183
I0502 15:06:33.227509 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0166249 (* 1 = 0.0166249 loss)
I0502 15:06:33.227519 2050462464 solver.cpp:464] Iteration 2540, lr = 0.001
I0502 15:08:22.850838 2050462464 solver.cpp:189] Iteration 2560, loss = 3.23284
I0502 15:08:22.850900 2050462464 solver.cpp:204]     Train net output #0: loss = 3.23285 (* 1 = 3.23285 loss)
I0502 15:08:22.850915 2050462464 solver.cpp:464] Iteration 2560, lr = 0.001
I0502 15:10:13.660145 2050462464 solver.cpp:189] Iteration 2580, loss = 0.542291
I0502 15:10:13.660193 2050462464 solver.cpp:204]     Train net output #0: loss = 0.542298 (* 1 = 0.542298 loss)
I0502 15:10:13.660202 2050462464 solver.cpp:464] Iteration 2580, lr = 0.001
I0502 15:11:55.962033 2050462464 solver.cpp:189] Iteration 2600, loss = 0.680369
I0502 15:11:55.962079 2050462464 solver.cpp:204]     Train net output #0: loss = 0.680375 (* 1 = 0.680375 loss)
I0502 15:11:55.962087 2050462464 solver.cpp:464] Iteration 2600, lr = 0.001
I0502 15:13:36.513108 2050462464 solver.cpp:189] Iteration 2620, loss = 1.16807
I0502 15:13:36.513154 2050462464 solver.cpp:204]     Train net output #0: loss = 1.16807 (* 1 = 1.16807 loss)
I0502 15:13:36.513164 2050462464 solver.cpp:464] Iteration 2620, lr = 0.001
I0502 15:15:18.503913 2050462464 solver.cpp:189] Iteration 2640, loss = 1.07257
I0502 15:15:18.503955 2050462464 solver.cpp:204]     Train net output #0: loss = 1.07258 (* 1 = 1.07258 loss)
I0502 15:15:18.503964 2050462464 solver.cpp:464] Iteration 2640, lr = 0.001
I0502 15:16:58.247526 2050462464 solver.cpp:189] Iteration 2660, loss = 1.14755
I0502 15:16:58.247573 2050462464 solver.cpp:204]     Train net output #0: loss = 1.14756 (* 1 = 1.14756 loss)
I0502 15:16:58.247582 2050462464 solver.cpp:464] Iteration 2660, lr = 0.001
I0502 15:18:37.693027 2050462464 solver.cpp:189] Iteration 2680, loss = 1.13412
I0502 15:18:37.693086 2050462464 solver.cpp:204]     Train net output #0: loss = 1.13413 (* 1 = 1.13413 loss)
I0502 15:18:37.693102 2050462464 solver.cpp:464] Iteration 2680, lr = 0.001
I0502 15:20:19.260414 2050462464 solver.cpp:189] Iteration 2700, loss = 1.0175
I0502 15:20:19.260684 2050462464 solver.cpp:204]     Train net output #0: loss = 1.0175 (* 1 = 1.0175 loss)
I0502 15:20:19.260696 2050462464 solver.cpp:464] Iteration 2700, lr = 0.001
I0502 15:21:59.190242 2050462464 solver.cpp:189] Iteration 2720, loss = 1.15925
I0502 15:21:59.190289 2050462464 solver.cpp:204]     Train net output #0: loss = 1.15926 (* 1 = 1.15926 loss)
I0502 15:21:59.190297 2050462464 solver.cpp:464] Iteration 2720, lr = 0.001
I0502 15:23:39.148156 2050462464 solver.cpp:189] Iteration 2740, loss = 0.417519
I0502 15:23:39.148218 2050462464 solver.cpp:204]     Train net output #0: loss = 0.417525 (* 1 = 0.417525 loss)
I0502 15:23:39.148228 2050462464 solver.cpp:464] Iteration 2740, lr = 0.001
I0502 15:25:18.643029 2050462464 solver.cpp:189] Iteration 2760, loss = 0.938009
I0502 15:25:18.643076 2050462464 solver.cpp:204]     Train net output #0: loss = 0.938016 (* 1 = 0.938016 loss)
I0502 15:25:18.643085 2050462464 solver.cpp:464] Iteration 2760, lr = 0.001
I0502 15:27:06.631296 2050462464 solver.cpp:189] Iteration 2780, loss = 0.314985
I0502 15:27:06.631541 2050462464 solver.cpp:204]     Train net output #0: loss = 0.314992 (* 1 = 0.314992 loss)
I0502 15:27:06.631551 2050462464 solver.cpp:464] Iteration 2780, lr = 0.001
I0502 15:29:35.840767 2050462464 solver.cpp:189] Iteration 2800, loss = 7.57677e-05
I0502 15:29:35.841051 2050462464 solver.cpp:204]     Train net output #0: loss = 8.26823e-05 (* 1 = 8.26823e-05 loss)
I0502 15:29:35.841063 2050462464 solver.cpp:464] Iteration 2800, lr = 0.001
I0502 15:31:33.146461 2050462464 solver.cpp:189] Iteration 2820, loss = 0.00545722
I0502 15:31:33.146518 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00546413 (* 1 = 0.00546413 loss)
I0502 15:31:33.146529 2050462464 solver.cpp:464] Iteration 2820, lr = 0.001
I0502 15:34:07.286247 2050462464 solver.cpp:189] Iteration 2840, loss = -6.65351e-06
I0502 15:34:07.286304 2050462464 solver.cpp:204]     Train net output #0: loss = 2.60772e-07 (* 1 = 2.60772e-07 loss)
I0502 15:34:07.286320 2050462464 solver.cpp:464] Iteration 2840, lr = 0.001
I0502 15:36:40.541332 2050462464 solver.cpp:189] Iteration 2860, loss = 4.89392
I0502 15:36:40.541399 2050462464 solver.cpp:204]     Train net output #0: loss = 4.89392 (* 1 = 4.89392 loss)
I0502 15:36:40.541415 2050462464 solver.cpp:464] Iteration 2860, lr = 0.001
I0502 15:38:38.435313 2050462464 solver.cpp:189] Iteration 2880, loss = -7.59811e-06
I0502 15:38:38.435361 2050462464 solver.cpp:204]     Train net output #0: loss = 5.58794e-09 (* 1 = 5.58794e-09 loss)
I0502 15:38:38.435371 2050462464 solver.cpp:464] Iteration 2880, lr = 0.001
I0502 15:40:39.018766 2050462464 solver.cpp:189] Iteration 2900, loss = -7.6037e-06
I0502 15:40:39.018847 2050462464 solver.cpp:204]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0502 15:40:39.018870 2050462464 solver.cpp:464] Iteration 2900, lr = 0.001
I0502 15:42:33.913199 2050462464 solver.cpp:189] Iteration 2920, loss = -7.6037e-06
I0502 15:42:33.913316 2050462464 solver.cpp:204]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0502 15:42:33.913327 2050462464 solver.cpp:464] Iteration 2920, lr = 0.001
I0502 15:44:33.029148 2050462464 solver.cpp:189] Iteration 2940, loss = -7.6037e-06
I0502 15:44:33.029197 2050462464 solver.cpp:204]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0502 15:44:33.029206 2050462464 solver.cpp:464] Iteration 2940, lr = 0.001
I0502 15:46:24.857548 2050462464 solver.cpp:189] Iteration 2960, loss = -7.6037e-06
I0502 15:46:24.857590 2050462464 solver.cpp:204]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0502 15:46:24.857599 2050462464 solver.cpp:464] Iteration 2960, lr = 0.001
I0502 15:48:15.349136 2050462464 solver.cpp:189] Iteration 2980, loss = -7.6037e-06
I0502 15:48:15.349190 2050462464 solver.cpp:204]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0502 15:48:15.349203 2050462464 solver.cpp:464] Iteration 2980, lr = 0.001
I0502 15:50:04.948822 2050462464 solver.cpp:334] Snapshotting to caffenet_train_iter_3000.caffemodel
I0502 15:50:06.538410 2050462464 solver.cpp:342] Snapshotting solver state to caffenet_train_iter_3000.solverstate
I0502 15:50:07.838495 2050462464 solver.cpp:266] Iteration 3000, Testing net (#0)
I0502 16:23:56.510205 2050462464 solver.cpp:315]     Test net output #0: accuracy = 0.40925
I0502 16:23:56.510494 2050462464 solver.cpp:315]     Test net output #1: loss = 49.9837 (* 1 = 49.9837 loss)
I0502 16:24:01.879639 2050462464 solver.cpp:189] Iteration 3000, loss = -7.6037e-06
I0502 16:24:01.879680 2050462464 solver.cpp:204]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0502 16:24:01.879708 2050462464 solver.cpp:464] Iteration 3000, lr = 0.001
I0502 16:25:58.640112 2050462464 solver.cpp:189] Iteration 3020, loss = -7.6037e-06
I0502 16:25:58.640210 2050462464 solver.cpp:204]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0502 16:25:58.640229 2050462464 solver.cpp:464] Iteration 3020, lr = 0.001
I0502 16:27:53.141057 2050462464 solver.cpp:189] Iteration 3040, loss = -7.6037e-06
I0502 16:27:53.141103 2050462464 solver.cpp:204]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0502 16:27:53.141113 2050462464 solver.cpp:464] Iteration 3040, lr = 0.001
I0502 16:29:39.622601 2050462464 solver.cpp:189] Iteration 3060, loss = 2.85718
I0502 16:29:39.622648 2050462464 solver.cpp:204]     Train net output #0: loss = 2.85719 (* 1 = 2.85719 loss)
I0502 16:29:39.622658 2050462464 solver.cpp:464] Iteration 3060, lr = 0.001
I0502 16:31:18.931063 2050462464 solver.cpp:189] Iteration 3080, loss = 5.71648
I0502 16:31:18.931114 2050462464 solver.cpp:204]     Train net output #0: loss = 5.71648 (* 1 = 5.71648 loss)
I0502 16:31:18.931123 2050462464 solver.cpp:464] Iteration 3080, lr = 0.001
I0502 16:33:02.443150 2050462464 solver.cpp:189] Iteration 3100, loss = nan
I0502 16:33:02.443193 2050462464 solver.cpp:204]     Train net output #0: loss = nan (* 1 = nan loss)
I0502 16:33:02.443203 2050462464 solver.cpp:464] Iteration 3100, lr = 0.001
I0502 16:34:41.356830 2050462464 solver.cpp:189] Iteration 3120, loss = nan
I0502 16:34:41.356876 2050462464 solver.cpp:204]     Train net output #0: loss = nan (* 1 = nan loss)
I0502 16:34:41.356900 2050462464 solver.cpp:464] Iteration 3120, lr = 0.001
^C
C02MX066FD58:rcc_net cusgadmin$ caffe train -solver solver_background.prototxt -weights bvlc_reference_rcnn_ilsvrc13.caffemodel 
I0502 16:47:41.086725 2050462464 caffe.cpp:117] Use CPU.
I0502 16:47:41.093785 2050462464 caffe.cpp:121] Starting Optimization
I0502 16:47:41.094048 2050462464 solver.cpp:32] Initializing solver from parameters: 
test_iter: 1000
test_interval: 1000
base_lr: 0.001
display: 20
max_iter: 10000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 100000
snapshot: 500
snapshot_prefix: "caffenet_train_background"
solver_mode: CPU
net: "train_val_background.prototxt"
I0502 16:47:41.094348 2050462464 solver.cpp:70] Creating training net from net file: train_val_background.prototxt
I0502 16:47:41.094954 2050462464 net.cpp:257] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0502 16:47:41.094995 2050462464 net.cpp:257] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0502 16:47:41.095011 2050462464 net.cpp:42] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "ImageData"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  image_data_param {
    source: "../train_list.txt"
    batch_size: 64
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8_VOC"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8_VOC"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  param {
    lr_mult: 20
    decay_mult: 0
  }
  inner_product_param {
    num_output: 6
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_VOC"
  bottom: "label"
  top: "loss"
}
I0502 16:47:41.095860 2050462464 layer_factory.hpp:74] Creating layer data
I0502 16:47:41.096142 2050462464 net.cpp:84] Creating Layer data
I0502 16:47:41.096159 2050462464 net.cpp:338] data -> data
I0502 16:47:41.096186 2050462464 net.cpp:338] data -> label
I0502 16:47:41.096199 2050462464 net.cpp:113] Setting up data
I0502 16:47:41.096379 2050462464 image_data_layer.cpp:36] Opening file ../train_list.txt
I0502 16:47:41.133525 2050462464 image_data_layer.cpp:51] A total of 32601 images.
I0502 16:47:41.135416 2050462464 image_data_layer.cpp:80] output data size: 64,3,227,227
I0502 16:47:41.161146 2050462464 net.cpp:120] Top shape: 64 3 227 227 (9893568)
I0502 16:47:41.161195 2050462464 net.cpp:120] Top shape: 64 (64)
I0502 16:47:41.161212 2050462464 layer_factory.hpp:74] Creating layer conv1
I0502 16:47:41.161239 2050462464 net.cpp:84] Creating Layer conv1
I0502 16:47:41.161252 2050462464 net.cpp:380] conv1 <- data
I0502 16:47:41.161270 2050462464 net.cpp:338] conv1 -> conv1
I0502 16:47:41.161295 2050462464 net.cpp:113] Setting up conv1
I0502 16:47:41.162134 2050462464 net.cpp:120] Top shape: 64 96 55 55 (18585600)
I0502 16:47:41.162156 2050462464 layer_factory.hpp:74] Creating layer relu1
I0502 16:47:41.162171 2050462464 net.cpp:84] Creating Layer relu1
I0502 16:47:41.162179 2050462464 net.cpp:380] relu1 <- conv1
I0502 16:47:41.162189 2050462464 net.cpp:327] relu1 -> conv1 (in-place)
I0502 16:47:41.162195 2050462464 net.cpp:113] Setting up relu1
I0502 16:47:41.162200 2050462464 net.cpp:120] Top shape: 64 96 55 55 (18585600)
I0502 16:47:41.162206 2050462464 layer_factory.hpp:74] Creating layer pool1
I0502 16:47:41.162214 2050462464 net.cpp:84] Creating Layer pool1
I0502 16:47:41.162220 2050462464 net.cpp:380] pool1 <- conv1
I0502 16:47:41.162225 2050462464 net.cpp:338] pool1 -> pool1
I0502 16:47:41.162235 2050462464 net.cpp:113] Setting up pool1
I0502 16:47:41.162253 2050462464 net.cpp:120] Top shape: 64 96 27 27 (4478976)
I0502 16:47:41.162261 2050462464 layer_factory.hpp:74] Creating layer norm1
I0502 16:47:41.162269 2050462464 net.cpp:84] Creating Layer norm1
I0502 16:47:41.162274 2050462464 net.cpp:380] norm1 <- pool1
I0502 16:47:41.162281 2050462464 net.cpp:338] norm1 -> norm1
I0502 16:47:41.162287 2050462464 net.cpp:113] Setting up norm1
I0502 16:47:41.162297 2050462464 net.cpp:120] Top shape: 64 96 27 27 (4478976)
I0502 16:47:41.162303 2050462464 layer_factory.hpp:74] Creating layer conv2
I0502 16:47:41.162343 2050462464 net.cpp:84] Creating Layer conv2
I0502 16:47:41.162348 2050462464 net.cpp:380] conv2 <- norm1
I0502 16:47:41.162354 2050462464 net.cpp:338] conv2 -> conv2
I0502 16:47:41.162361 2050462464 net.cpp:113] Setting up conv2
I0502 16:47:41.167886 2050462464 net.cpp:120] Top shape: 64 256 27 27 (11943936)
I0502 16:47:41.167914 2050462464 layer_factory.hpp:74] Creating layer relu2
I0502 16:47:41.167927 2050462464 net.cpp:84] Creating Layer relu2
I0502 16:47:41.167935 2050462464 net.cpp:380] relu2 <- conv2
I0502 16:47:41.167947 2050462464 net.cpp:327] relu2 -> conv2 (in-place)
I0502 16:47:41.167958 2050462464 net.cpp:113] Setting up relu2
I0502 16:47:41.167968 2050462464 net.cpp:120] Top shape: 64 256 27 27 (11943936)
I0502 16:47:41.167973 2050462464 layer_factory.hpp:74] Creating layer pool2
I0502 16:47:41.167980 2050462464 net.cpp:84] Creating Layer pool2
I0502 16:47:41.167984 2050462464 net.cpp:380] pool2 <- conv2
I0502 16:47:41.167990 2050462464 net.cpp:338] pool2 -> pool2
I0502 16:47:41.167997 2050462464 net.cpp:113] Setting up pool2
I0502 16:47:41.168004 2050462464 net.cpp:120] Top shape: 64 256 13 13 (2768896)
I0502 16:47:41.168009 2050462464 layer_factory.hpp:74] Creating layer norm2
I0502 16:47:41.168016 2050462464 net.cpp:84] Creating Layer norm2
I0502 16:47:41.168022 2050462464 net.cpp:380] norm2 <- pool2
I0502 16:47:41.168032 2050462464 net.cpp:338] norm2 -> norm2
I0502 16:47:41.168056 2050462464 net.cpp:113] Setting up norm2
I0502 16:47:41.168074 2050462464 net.cpp:120] Top shape: 64 256 13 13 (2768896)
I0502 16:47:41.168083 2050462464 layer_factory.hpp:74] Creating layer conv3
I0502 16:47:41.168102 2050462464 net.cpp:84] Creating Layer conv3
I0502 16:47:41.168112 2050462464 net.cpp:380] conv3 <- norm2
I0502 16:47:41.168125 2050462464 net.cpp:338] conv3 -> conv3
I0502 16:47:41.168139 2050462464 net.cpp:113] Setting up conv3
I0502 16:47:41.184538 2050462464 net.cpp:120] Top shape: 64 384 13 13 (4153344)
I0502 16:47:41.184603 2050462464 layer_factory.hpp:74] Creating layer relu3
I0502 16:47:41.184633 2050462464 net.cpp:84] Creating Layer relu3
I0502 16:47:41.184649 2050462464 net.cpp:380] relu3 <- conv3
I0502 16:47:41.184666 2050462464 net.cpp:327] relu3 -> conv3 (in-place)
I0502 16:47:41.184689 2050462464 net.cpp:113] Setting up relu3
I0502 16:47:41.184702 2050462464 net.cpp:120] Top shape: 64 384 13 13 (4153344)
I0502 16:47:41.184720 2050462464 layer_factory.hpp:74] Creating layer conv4
I0502 16:47:41.184741 2050462464 net.cpp:84] Creating Layer conv4
I0502 16:47:41.184751 2050462464 net.cpp:380] conv4 <- conv3
I0502 16:47:41.184766 2050462464 net.cpp:338] conv4 -> conv4
I0502 16:47:41.184782 2050462464 net.cpp:113] Setting up conv4
I0502 16:47:41.199199 2050462464 net.cpp:120] Top shape: 64 384 13 13 (4153344)
I0502 16:47:41.199252 2050462464 layer_factory.hpp:74] Creating layer relu4
I0502 16:47:41.199270 2050462464 net.cpp:84] Creating Layer relu4
I0502 16:47:41.199280 2050462464 net.cpp:380] relu4 <- conv4
I0502 16:47:41.199293 2050462464 net.cpp:327] relu4 -> conv4 (in-place)
I0502 16:47:41.199306 2050462464 net.cpp:113] Setting up relu4
I0502 16:47:41.199316 2050462464 net.cpp:120] Top shape: 64 384 13 13 (4153344)
I0502 16:47:41.199326 2050462464 layer_factory.hpp:74] Creating layer conv5
I0502 16:47:41.199342 2050462464 net.cpp:84] Creating Layer conv5
I0502 16:47:41.199349 2050462464 net.cpp:380] conv5 <- conv4
I0502 16:47:41.199362 2050462464 net.cpp:338] conv5 -> conv5
I0502 16:47:41.199375 2050462464 net.cpp:113] Setting up conv5
I0502 16:47:41.209010 2050462464 net.cpp:120] Top shape: 64 256 13 13 (2768896)
I0502 16:47:41.209045 2050462464 layer_factory.hpp:74] Creating layer relu5
I0502 16:47:41.209069 2050462464 net.cpp:84] Creating Layer relu5
I0502 16:47:41.209079 2050462464 net.cpp:380] relu5 <- conv5
I0502 16:47:41.209090 2050462464 net.cpp:327] relu5 -> conv5 (in-place)
I0502 16:47:41.209102 2050462464 net.cpp:113] Setting up relu5
I0502 16:47:41.209111 2050462464 net.cpp:120] Top shape: 64 256 13 13 (2768896)
I0502 16:47:41.209122 2050462464 layer_factory.hpp:74] Creating layer pool5
I0502 16:47:41.209167 2050462464 net.cpp:84] Creating Layer pool5
I0502 16:47:41.209178 2050462464 net.cpp:380] pool5 <- conv5
I0502 16:47:41.209189 2050462464 net.cpp:338] pool5 -> pool5
I0502 16:47:41.209202 2050462464 net.cpp:113] Setting up pool5
I0502 16:47:41.209213 2050462464 net.cpp:120] Top shape: 64 256 6 6 (589824)
I0502 16:47:41.209224 2050462464 layer_factory.hpp:74] Creating layer fc6
I0502 16:47:41.209240 2050462464 net.cpp:84] Creating Layer fc6
I0502 16:47:41.209249 2050462464 net.cpp:380] fc6 <- pool5
I0502 16:47:41.209264 2050462464 net.cpp:338] fc6 -> fc6
I0502 16:47:41.209278 2050462464 net.cpp:113] Setting up fc6
I0502 16:47:41.825434 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0502 16:47:41.825470 2050462464 layer_factory.hpp:74] Creating layer relu6
I0502 16:47:41.825481 2050462464 net.cpp:84] Creating Layer relu6
I0502 16:47:41.825487 2050462464 net.cpp:380] relu6 <- fc6
I0502 16:47:41.825494 2050462464 net.cpp:327] relu6 -> fc6 (in-place)
I0502 16:47:41.825501 2050462464 net.cpp:113] Setting up relu6
I0502 16:47:41.825506 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0502 16:47:41.825511 2050462464 layer_factory.hpp:74] Creating layer drop6
I0502 16:47:41.825517 2050462464 net.cpp:84] Creating Layer drop6
I0502 16:47:41.825521 2050462464 net.cpp:380] drop6 <- fc6
I0502 16:47:41.825527 2050462464 net.cpp:327] drop6 -> fc6 (in-place)
I0502 16:47:41.825559 2050462464 net.cpp:113] Setting up drop6
I0502 16:47:41.825593 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0502 16:47:41.825609 2050462464 layer_factory.hpp:74] Creating layer fc7
I0502 16:47:41.825620 2050462464 net.cpp:84] Creating Layer fc7
I0502 16:47:41.825625 2050462464 net.cpp:380] fc7 <- fc6
I0502 16:47:41.825634 2050462464 net.cpp:338] fc7 -> fc7
I0502 16:47:41.825642 2050462464 net.cpp:113] Setting up fc7
I0502 16:47:42.102093 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0502 16:47:42.102144 2050462464 layer_factory.hpp:74] Creating layer relu7
I0502 16:47:42.102157 2050462464 net.cpp:84] Creating Layer relu7
I0502 16:47:42.102164 2050462464 net.cpp:380] relu7 <- fc7
I0502 16:47:42.102171 2050462464 net.cpp:327] relu7 -> fc7 (in-place)
I0502 16:47:42.102180 2050462464 net.cpp:113] Setting up relu7
I0502 16:47:42.102185 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0502 16:47:42.102191 2050462464 layer_factory.hpp:74] Creating layer drop7
I0502 16:47:42.102200 2050462464 net.cpp:84] Creating Layer drop7
I0502 16:47:42.102203 2050462464 net.cpp:380] drop7 <- fc7
I0502 16:47:42.102210 2050462464 net.cpp:327] drop7 -> fc7 (in-place)
I0502 16:47:42.102216 2050462464 net.cpp:113] Setting up drop7
I0502 16:47:42.102221 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0502 16:47:42.102226 2050462464 layer_factory.hpp:74] Creating layer fc8_VOC
I0502 16:47:42.102236 2050462464 net.cpp:84] Creating Layer fc8_VOC
I0502 16:47:42.102239 2050462464 net.cpp:380] fc8_VOC <- fc7
I0502 16:47:42.102246 2050462464 net.cpp:338] fc8_VOC -> fc8_VOC
I0502 16:47:42.102254 2050462464 net.cpp:113] Setting up fc8_VOC
I0502 16:47:42.102680 2050462464 net.cpp:120] Top shape: 64 6 (384)
I0502 16:47:42.102694 2050462464 layer_factory.hpp:74] Creating layer loss
I0502 16:47:42.102972 2050462464 net.cpp:84] Creating Layer loss
I0502 16:47:42.102991 2050462464 net.cpp:380] loss <- fc8_VOC
I0502 16:47:42.102998 2050462464 net.cpp:380] loss <- label
I0502 16:47:42.103006 2050462464 net.cpp:338] loss -> loss
I0502 16:47:42.103014 2050462464 net.cpp:113] Setting up loss
I0502 16:47:42.103021 2050462464 layer_factory.hpp:74] Creating layer loss
I0502 16:47:42.103042 2050462464 net.cpp:120] Top shape: (1)
I0502 16:47:42.103049 2050462464 net.cpp:122]     with loss weight 1
I0502 16:47:42.103065 2050462464 net.cpp:167] loss needs backward computation.
I0502 16:47:42.103070 2050462464 net.cpp:167] fc8_VOC needs backward computation.
I0502 16:47:42.103075 2050462464 net.cpp:167] drop7 needs backward computation.
I0502 16:47:42.103080 2050462464 net.cpp:167] relu7 needs backward computation.
I0502 16:47:42.103114 2050462464 net.cpp:167] fc7 needs backward computation.
I0502 16:47:42.103119 2050462464 net.cpp:167] drop6 needs backward computation.
I0502 16:47:42.103124 2050462464 net.cpp:167] relu6 needs backward computation.
I0502 16:47:42.103128 2050462464 net.cpp:167] fc6 needs backward computation.
I0502 16:47:42.103133 2050462464 net.cpp:167] pool5 needs backward computation.
I0502 16:47:42.103138 2050462464 net.cpp:167] relu5 needs backward computation.
I0502 16:47:42.103143 2050462464 net.cpp:167] conv5 needs backward computation.
I0502 16:47:42.103147 2050462464 net.cpp:167] relu4 needs backward computation.
I0502 16:47:42.103152 2050462464 net.cpp:167] conv4 needs backward computation.
I0502 16:47:42.103157 2050462464 net.cpp:167] relu3 needs backward computation.
I0502 16:47:42.103216 2050462464 net.cpp:167] conv3 needs backward computation.
I0502 16:47:42.103234 2050462464 net.cpp:167] norm2 needs backward computation.
I0502 16:47:42.103238 2050462464 net.cpp:167] pool2 needs backward computation.
I0502 16:47:42.103243 2050462464 net.cpp:167] relu2 needs backward computation.
I0502 16:47:42.103248 2050462464 net.cpp:167] conv2 needs backward computation.
I0502 16:47:42.103253 2050462464 net.cpp:167] norm1 needs backward computation.
I0502 16:47:42.103257 2050462464 net.cpp:167] pool1 needs backward computation.
I0502 16:47:42.103262 2050462464 net.cpp:167] relu1 needs backward computation.
I0502 16:47:42.103266 2050462464 net.cpp:167] conv1 needs backward computation.
I0502 16:47:42.103271 2050462464 net.cpp:169] data does not need backward computation.
I0502 16:47:42.103276 2050462464 net.cpp:205] This network produces output loss
I0502 16:47:42.103289 2050462464 net.cpp:447] Collecting Learning Rate and Weight Decay.
I0502 16:47:42.103297 2050462464 net.cpp:217] Network initialization done.
I0502 16:47:42.103332 2050462464 net.cpp:218] Memory required for data: 439050756
I0502 16:47:42.104024 2050462464 solver.cpp:154] Creating test net (#0) specified by net file: train_val_background.prototxt
I0502 16:47:42.104100 2050462464 net.cpp:257] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0502 16:47:42.104136 2050462464 net.cpp:42] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "ImageData"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  image_data_param {
    source: "../test_list.txt"
    batch_size: 64
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8_VOC"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8_VOC"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  param {
    lr_mult: 20
    decay_mult: 0
  }
  inner_product_param {
    num_output: 6
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_VOC"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_VOC"
  bottom: "label"
  top: "loss"
}
I0502 16:47:42.104554 2050462464 layer_factory.hpp:74] Creating layer data
I0502 16:47:42.104569 2050462464 net.cpp:84] Creating Layer data
I0502 16:47:42.104578 2050462464 net.cpp:338] data -> data
I0502 16:47:42.104589 2050462464 net.cpp:338] data -> label
I0502 16:47:42.104596 2050462464 net.cpp:113] Setting up data
I0502 16:47:42.104601 2050462464 image_data_layer.cpp:36] Opening file ../test_list.txt
I0502 16:47:42.109459 2050462464 image_data_layer.cpp:51] A total of 3703 images.
I0502 16:47:42.111187 2050462464 image_data_layer.cpp:80] output data size: 64,3,227,227
I0502 16:47:42.133710 2050462464 net.cpp:120] Top shape: 64 3 227 227 (9893568)
I0502 16:47:42.133769 2050462464 net.cpp:120] Top shape: 64 (64)
I0502 16:47:42.133779 2050462464 layer_factory.hpp:74] Creating layer label_data_1_split
I0502 16:47:42.133908 2050462464 net.cpp:84] Creating Layer label_data_1_split
I0502 16:47:42.133997 2050462464 net.cpp:380] label_data_1_split <- label
I0502 16:47:42.134125 2050462464 net.cpp:338] label_data_1_split -> label_data_1_split_0
I0502 16:47:42.134244 2050462464 net.cpp:338] label_data_1_split -> label_data_1_split_1
I0502 16:47:42.134321 2050462464 net.cpp:113] Setting up label_data_1_split
I0502 16:47:42.134397 2050462464 net.cpp:120] Top shape: 64 (64)
I0502 16:47:42.134416 2050462464 net.cpp:120] Top shape: 64 (64)
I0502 16:47:42.134481 2050462464 layer_factory.hpp:74] Creating layer conv1
I0502 16:47:42.134546 2050462464 net.cpp:84] Creating Layer conv1
I0502 16:47:42.134584 2050462464 net.cpp:380] conv1 <- data
I0502 16:47:42.134641 2050462464 net.cpp:338] conv1 -> conv1
I0502 16:47:42.134714 2050462464 net.cpp:113] Setting up conv1
I0502 16:47:42.135789 2050462464 net.cpp:120] Top shape: 64 96 55 55 (18585600)
I0502 16:47:42.135810 2050462464 layer_factory.hpp:74] Creating layer relu1
I0502 16:47:42.135820 2050462464 net.cpp:84] Creating Layer relu1
I0502 16:47:42.135825 2050462464 net.cpp:380] relu1 <- conv1
I0502 16:47:42.135833 2050462464 net.cpp:327] relu1 -> conv1 (in-place)
I0502 16:47:42.135839 2050462464 net.cpp:113] Setting up relu1
I0502 16:47:42.135844 2050462464 net.cpp:120] Top shape: 64 96 55 55 (18585600)
I0502 16:47:42.135850 2050462464 layer_factory.hpp:74] Creating layer pool1
I0502 16:47:42.135864 2050462464 net.cpp:84] Creating Layer pool1
I0502 16:47:42.135869 2050462464 net.cpp:380] pool1 <- conv1
I0502 16:47:42.135875 2050462464 net.cpp:338] pool1 -> pool1
I0502 16:47:42.135902 2050462464 net.cpp:113] Setting up pool1
I0502 16:47:42.135910 2050462464 net.cpp:120] Top shape: 64 96 27 27 (4478976)
I0502 16:47:42.135922 2050462464 layer_factory.hpp:74] Creating layer norm1
I0502 16:47:42.135931 2050462464 net.cpp:84] Creating Layer norm1
I0502 16:47:42.135954 2050462464 net.cpp:380] norm1 <- pool1
I0502 16:47:42.135984 2050462464 net.cpp:338] norm1 -> norm1
I0502 16:47:42.136008 2050462464 net.cpp:113] Setting up norm1
I0502 16:47:42.136015 2050462464 net.cpp:120] Top shape: 64 96 27 27 (4478976)
I0502 16:47:42.136021 2050462464 layer_factory.hpp:74] Creating layer conv2
I0502 16:47:42.136060 2050462464 net.cpp:84] Creating Layer conv2
I0502 16:47:42.136076 2050462464 net.cpp:380] conv2 <- norm1
I0502 16:47:42.136131 2050462464 net.cpp:338] conv2 -> conv2
I0502 16:47:42.136152 2050462464 net.cpp:113] Setting up conv2
I0502 16:47:42.143733 2050462464 net.cpp:120] Top shape: 64 256 27 27 (11943936)
I0502 16:47:42.143795 2050462464 layer_factory.hpp:74] Creating layer relu2
I0502 16:47:42.143813 2050462464 net.cpp:84] Creating Layer relu2
I0502 16:47:42.143820 2050462464 net.cpp:380] relu2 <- conv2
I0502 16:47:42.143826 2050462464 net.cpp:327] relu2 -> conv2 (in-place)
I0502 16:47:42.143836 2050462464 net.cpp:113] Setting up relu2
I0502 16:47:42.143862 2050462464 net.cpp:120] Top shape: 64 256 27 27 (11943936)
I0502 16:47:42.143884 2050462464 layer_factory.hpp:74] Creating layer pool2
I0502 16:47:42.143920 2050462464 net.cpp:84] Creating Layer pool2
I0502 16:47:42.143937 2050462464 net.cpp:380] pool2 <- conv2
I0502 16:47:42.143952 2050462464 net.cpp:338] pool2 -> pool2
I0502 16:47:42.143970 2050462464 net.cpp:113] Setting up pool2
I0502 16:47:42.143996 2050462464 net.cpp:120] Top shape: 64 256 13 13 (2768896)
I0502 16:47:42.144009 2050462464 layer_factory.hpp:74] Creating layer norm2
I0502 16:47:42.144023 2050462464 net.cpp:84] Creating Layer norm2
I0502 16:47:42.144054 2050462464 net.cpp:380] norm2 <- pool2
I0502 16:47:42.144095 2050462464 net.cpp:338] norm2 -> norm2
I0502 16:47:42.144127 2050462464 net.cpp:113] Setting up norm2
I0502 16:47:42.144142 2050462464 net.cpp:120] Top shape: 64 256 13 13 (2768896)
I0502 16:47:42.144165 2050462464 layer_factory.hpp:74] Creating layer conv3
I0502 16:47:42.144193 2050462464 net.cpp:84] Creating Layer conv3
I0502 16:47:42.144209 2050462464 net.cpp:380] conv3 <- norm2
I0502 16:47:42.144235 2050462464 net.cpp:338] conv3 -> conv3
I0502 16:47:42.144251 2050462464 net.cpp:113] Setting up conv3
I0502 16:47:42.166776 2050462464 net.cpp:120] Top shape: 64 384 13 13 (4153344)
I0502 16:47:42.166826 2050462464 layer_factory.hpp:74] Creating layer relu3
I0502 16:47:42.166838 2050462464 net.cpp:84] Creating Layer relu3
I0502 16:47:42.166846 2050462464 net.cpp:380] relu3 <- conv3
I0502 16:47:42.166854 2050462464 net.cpp:327] relu3 -> conv3 (in-place)
I0502 16:47:42.166864 2050462464 net.cpp:113] Setting up relu3
I0502 16:47:42.166872 2050462464 net.cpp:120] Top shape: 64 384 13 13 (4153344)
I0502 16:47:42.166879 2050462464 layer_factory.hpp:74] Creating layer conv4
I0502 16:47:42.166889 2050462464 net.cpp:84] Creating Layer conv4
I0502 16:47:42.166896 2050462464 net.cpp:380] conv4 <- conv3
I0502 16:47:42.166905 2050462464 net.cpp:338] conv4 -> conv4
I0502 16:47:42.166915 2050462464 net.cpp:113] Setting up conv4
I0502 16:47:42.182893 2050462464 net.cpp:120] Top shape: 64 384 13 13 (4153344)
I0502 16:47:42.182920 2050462464 layer_factory.hpp:74] Creating layer relu4
I0502 16:47:42.182934 2050462464 net.cpp:84] Creating Layer relu4
I0502 16:47:42.182942 2050462464 net.cpp:380] relu4 <- conv4
I0502 16:47:42.182952 2050462464 net.cpp:327] relu4 -> conv4 (in-place)
I0502 16:47:42.182963 2050462464 net.cpp:113] Setting up relu4
I0502 16:47:42.182971 2050462464 net.cpp:120] Top shape: 64 384 13 13 (4153344)
I0502 16:47:42.182982 2050462464 layer_factory.hpp:74] Creating layer conv5
I0502 16:47:42.183002 2050462464 net.cpp:84] Creating Layer conv5
I0502 16:47:42.183010 2050462464 net.cpp:380] conv5 <- conv4
I0502 16:47:42.183020 2050462464 net.cpp:338] conv5 -> conv5
I0502 16:47:42.183028 2050462464 net.cpp:113] Setting up conv5
I0502 16:47:42.190143 2050462464 net.cpp:120] Top shape: 64 256 13 13 (2768896)
I0502 16:47:42.190176 2050462464 layer_factory.hpp:74] Creating layer relu5
I0502 16:47:42.190187 2050462464 net.cpp:84] Creating Layer relu5
I0502 16:47:42.190193 2050462464 net.cpp:380] relu5 <- conv5
I0502 16:47:42.190201 2050462464 net.cpp:327] relu5 -> conv5 (in-place)
I0502 16:47:42.190219 2050462464 net.cpp:113] Setting up relu5
I0502 16:47:42.190224 2050462464 net.cpp:120] Top shape: 64 256 13 13 (2768896)
I0502 16:47:42.190232 2050462464 layer_factory.hpp:74] Creating layer pool5
I0502 16:47:42.190242 2050462464 net.cpp:84] Creating Layer pool5
I0502 16:47:42.190245 2050462464 net.cpp:380] pool5 <- conv5
I0502 16:47:42.190251 2050462464 net.cpp:338] pool5 -> pool5
I0502 16:47:42.190292 2050462464 net.cpp:113] Setting up pool5
I0502 16:47:42.190313 2050462464 net.cpp:120] Top shape: 64 256 6 6 (589824)
I0502 16:47:42.190323 2050462464 layer_factory.hpp:74] Creating layer fc6
I0502 16:47:42.190333 2050462464 net.cpp:84] Creating Layer fc6
I0502 16:47:42.190338 2050462464 net.cpp:380] fc6 <- pool5
I0502 16:47:42.190346 2050462464 net.cpp:338] fc6 -> fc6
I0502 16:47:42.190356 2050462464 net.cpp:113] Setting up fc6
I0502 16:47:42.863700 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0502 16:47:42.863737 2050462464 layer_factory.hpp:74] Creating layer relu6
I0502 16:47:42.863749 2050462464 net.cpp:84] Creating Layer relu6
I0502 16:47:42.863755 2050462464 net.cpp:380] relu6 <- fc6
I0502 16:47:42.863764 2050462464 net.cpp:327] relu6 -> fc6 (in-place)
I0502 16:47:42.863772 2050462464 net.cpp:113] Setting up relu6
I0502 16:47:42.863778 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0502 16:47:42.863783 2050462464 layer_factory.hpp:74] Creating layer drop6
I0502 16:47:42.863791 2050462464 net.cpp:84] Creating Layer drop6
I0502 16:47:42.863797 2050462464 net.cpp:380] drop6 <- fc6
I0502 16:47:42.863807 2050462464 net.cpp:327] drop6 -> fc6 (in-place)
I0502 16:47:42.863818 2050462464 net.cpp:113] Setting up drop6
I0502 16:47:42.863827 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0502 16:47:42.863833 2050462464 layer_factory.hpp:74] Creating layer fc7
I0502 16:47:42.863842 2050462464 net.cpp:84] Creating Layer fc7
I0502 16:47:42.863847 2050462464 net.cpp:380] fc7 <- fc6
I0502 16:47:42.863852 2050462464 net.cpp:338] fc7 -> fc7
I0502 16:47:42.863862 2050462464 net.cpp:113] Setting up fc7
I0502 16:47:43.140831 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0502 16:47:43.140913 2050462464 layer_factory.hpp:74] Creating layer relu7
I0502 16:47:43.140924 2050462464 net.cpp:84] Creating Layer relu7
I0502 16:47:43.140929 2050462464 net.cpp:380] relu7 <- fc7
I0502 16:47:43.140936 2050462464 net.cpp:327] relu7 -> fc7 (in-place)
I0502 16:47:43.140944 2050462464 net.cpp:113] Setting up relu7
I0502 16:47:43.140949 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0502 16:47:43.140954 2050462464 layer_factory.hpp:74] Creating layer drop7
I0502 16:47:43.140961 2050462464 net.cpp:84] Creating Layer drop7
I0502 16:47:43.140965 2050462464 net.cpp:380] drop7 <- fc7
I0502 16:47:43.140971 2050462464 net.cpp:327] drop7 -> fc7 (in-place)
I0502 16:47:43.140977 2050462464 net.cpp:113] Setting up drop7
I0502 16:47:43.140982 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0502 16:47:43.140987 2050462464 layer_factory.hpp:74] Creating layer fc8_VOC
I0502 16:47:43.140995 2050462464 net.cpp:84] Creating Layer fc8_VOC
I0502 16:47:43.141000 2050462464 net.cpp:380] fc8_VOC <- fc7
I0502 16:47:43.141005 2050462464 net.cpp:338] fc8_VOC -> fc8_VOC
I0502 16:47:43.141016 2050462464 net.cpp:113] Setting up fc8_VOC
I0502 16:47:43.141440 2050462464 net.cpp:120] Top shape: 64 6 (384)
I0502 16:47:43.141463 2050462464 layer_factory.hpp:74] Creating layer fc8_VOC_fc8_VOC_0_split
I0502 16:47:43.141471 2050462464 net.cpp:84] Creating Layer fc8_VOC_fc8_VOC_0_split
I0502 16:47:43.141476 2050462464 net.cpp:380] fc8_VOC_fc8_VOC_0_split <- fc8_VOC
I0502 16:47:43.141484 2050462464 net.cpp:338] fc8_VOC_fc8_VOC_0_split -> fc8_VOC_fc8_VOC_0_split_0
I0502 16:47:43.141494 2050462464 net.cpp:338] fc8_VOC_fc8_VOC_0_split -> fc8_VOC_fc8_VOC_0_split_1
I0502 16:47:43.141500 2050462464 net.cpp:113] Setting up fc8_VOC_fc8_VOC_0_split
I0502 16:47:43.141507 2050462464 net.cpp:120] Top shape: 64 6 (384)
I0502 16:47:43.141512 2050462464 net.cpp:120] Top shape: 64 6 (384)
I0502 16:47:43.141517 2050462464 layer_factory.hpp:74] Creating layer accuracy
I0502 16:47:43.141530 2050462464 net.cpp:84] Creating Layer accuracy
I0502 16:47:43.141535 2050462464 net.cpp:380] accuracy <- fc8_VOC_fc8_VOC_0_split_0
I0502 16:47:43.141541 2050462464 net.cpp:380] accuracy <- label_data_1_split_0
I0502 16:47:43.141547 2050462464 net.cpp:338] accuracy -> accuracy
I0502 16:47:43.141554 2050462464 net.cpp:113] Setting up accuracy
I0502 16:47:43.141559 2050462464 net.cpp:120] Top shape: (1)
I0502 16:47:43.141564 2050462464 layer_factory.hpp:74] Creating layer loss
I0502 16:47:43.141571 2050462464 net.cpp:84] Creating Layer loss
I0502 16:47:43.141576 2050462464 net.cpp:380] loss <- fc8_VOC_fc8_VOC_0_split_1
I0502 16:47:43.141580 2050462464 net.cpp:380] loss <- label_data_1_split_1
I0502 16:47:43.141587 2050462464 net.cpp:338] loss -> loss
I0502 16:47:43.141592 2050462464 net.cpp:113] Setting up loss
I0502 16:47:43.141598 2050462464 layer_factory.hpp:74] Creating layer loss
I0502 16:47:43.141610 2050462464 net.cpp:120] Top shape: (1)
I0502 16:47:43.141615 2050462464 net.cpp:122]     with loss weight 1
I0502 16:47:43.141623 2050462464 net.cpp:167] loss needs backward computation.
I0502 16:47:43.141628 2050462464 net.cpp:169] accuracy does not need backward computation.
I0502 16:47:43.141633 2050462464 net.cpp:167] fc8_VOC_fc8_VOC_0_split needs backward computation.
I0502 16:47:43.141636 2050462464 net.cpp:167] fc8_VOC needs backward computation.
I0502 16:47:43.141640 2050462464 net.cpp:167] drop7 needs backward computation.
I0502 16:47:43.141645 2050462464 net.cpp:167] relu7 needs backward computation.
I0502 16:47:43.141649 2050462464 net.cpp:167] fc7 needs backward computation.
I0502 16:47:43.141654 2050462464 net.cpp:167] drop6 needs backward computation.
I0502 16:47:43.141659 2050462464 net.cpp:167] relu6 needs backward computation.
I0502 16:47:43.141662 2050462464 net.cpp:167] fc6 needs backward computation.
I0502 16:47:43.141666 2050462464 net.cpp:167] pool5 needs backward computation.
I0502 16:47:43.141671 2050462464 net.cpp:167] relu5 needs backward computation.
I0502 16:47:43.141675 2050462464 net.cpp:167] conv5 needs backward computation.
I0502 16:47:43.141700 2050462464 net.cpp:167] relu4 needs backward computation.
I0502 16:47:43.141705 2050462464 net.cpp:167] conv4 needs backward computation.
I0502 16:47:43.141710 2050462464 net.cpp:167] relu3 needs backward computation.
I0502 16:47:43.141713 2050462464 net.cpp:167] conv3 needs backward computation.
I0502 16:47:43.141718 2050462464 net.cpp:167] norm2 needs backward computation.
I0502 16:47:43.141723 2050462464 net.cpp:167] pool2 needs backward computation.
I0502 16:47:43.141727 2050462464 net.cpp:167] relu2 needs backward computation.
I0502 16:47:43.141731 2050462464 net.cpp:167] conv2 needs backward computation.
I0502 16:47:43.141736 2050462464 net.cpp:167] norm1 needs backward computation.
I0502 16:47:43.141741 2050462464 net.cpp:167] pool1 needs backward computation.
I0502 16:47:43.141746 2050462464 net.cpp:167] relu1 needs backward computation.
I0502 16:47:43.141749 2050462464 net.cpp:167] conv1 needs backward computation.
I0502 16:47:43.141754 2050462464 net.cpp:169] label_data_1_split does not need backward computation.
I0502 16:47:43.141759 2050462464 net.cpp:169] data does not need backward computation.
I0502 16:47:43.141763 2050462464 net.cpp:205] This network produces output accuracy
I0502 16:47:43.141768 2050462464 net.cpp:205] This network produces output loss
I0502 16:47:43.141782 2050462464 net.cpp:447] Collecting Learning Rate and Weight Decay.
I0502 16:47:43.141789 2050462464 net.cpp:217] Network initialization done.
I0502 16:47:43.141793 2050462464 net.cpp:218] Memory required for data: 439054344
I0502 16:47:43.141904 2050462464 solver.cpp:42] Solver scaffolding done.
I0502 16:47:43.142488 2050462464 caffe.cpp:86] Finetuning from bvlc_reference_rcnn_ilsvrc13.caffemodel
E0502 16:47:43.674573 2050462464 upgrade_proto.cpp:618] Attempting to upgrade input file specified using deprecated V1LayerParameter: bvlc_reference_rcnn_ilsvrc13.caffemodel
I0502 16:47:43.987061 2050462464 upgrade_proto.cpp:626] Successfully upgraded file specified using deprecated V1LayerParameter
E0502 16:47:44.509083 2050462464 upgrade_proto.cpp:618] Attempting to upgrade input file specified using deprecated V1LayerParameter: bvlc_reference_rcnn_ilsvrc13.caffemodel
I0502 16:47:44.787571 2050462464 upgrade_proto.cpp:626] Successfully upgraded file specified using deprecated V1LayerParameter
I0502 16:47:44.895293 2050462464 solver.cpp:222] Solving CaffeNet
I0502 16:47:44.895323 2050462464 solver.cpp:223] Learning Rate Policy: step
I0502 16:47:44.895334 2050462464 solver.cpp:266] Iteration 0, Testing net (#0)
^C
C02MX066FD58:rcc_net cusgadmin$ clear

C02MX066FD58:rcc_net cusgadmin$ caffe train -solver solver_background.prototxt -weights bvlc_reference_rcnn_ilsvrc13.caffemodel 
I0502 16:50:26.860466 2050462464 caffe.cpp:117] Use CPU.
I0502 16:50:26.861974 2050462464 caffe.cpp:121] Starting Optimization
I0502 16:50:26.861989 2050462464 solver.cpp:32] Initializing solver from parameters: 
test_iter: 1000
test_interval: 1000
base_lr: 0.001
display: 20
max_iter: 10000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 100000
snapshot: 500
snapshot_prefix: "caffenet_train_background"
solver_mode: CPU
net: "train_val_background.prototxt"
I0502 16:50:26.862062 2050462464 solver.cpp:70] Creating training net from net file: train_val_background.prototxt
I0502 16:50:26.862398 2050462464 net.cpp:257] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0502 16:50:26.862435 2050462464 net.cpp:257] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0502 16:50:26.862443 2050462464 net.cpp:42] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "ImageData"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  image_data_param {
    source: "../train_list.txt"
    batch_size: 64
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8_VOC"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8_VOC"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  param {
    lr_mult: 20
    decay_mult: 0
  }
  inner_product_param {
    num_output: 6
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_VOC"
  bottom: "label"
  top: "loss"
}
I0502 16:50:26.863095 2050462464 layer_factory.hpp:74] Creating layer data
I0502 16:50:26.863121 2050462464 net.cpp:84] Creating Layer data
I0502 16:50:26.863128 2050462464 net.cpp:338] data -> data
I0502 16:50:26.863152 2050462464 net.cpp:338] data -> label
I0502 16:50:26.863159 2050462464 net.cpp:113] Setting up data
I0502 16:50:26.863172 2050462464 image_data_layer.cpp:36] Opening file ../train_list.txt
I0502 16:50:26.901188 2050462464 image_data_layer.cpp:51] A total of 32601 images.
I0502 16:50:26.902544 2050462464 image_data_layer.cpp:80] output data size: 64,3,227,227
I0502 16:50:26.929924 2050462464 net.cpp:120] Top shape: 64 3 227 227 (9893568)
I0502 16:50:26.929972 2050462464 net.cpp:120] Top shape: 64 (64)
I0502 16:50:26.929987 2050462464 layer_factory.hpp:74] Creating layer conv1
I0502 16:50:26.930009 2050462464 net.cpp:84] Creating Layer conv1
I0502 16:50:26.930019 2050462464 net.cpp:380] conv1 <- data
I0502 16:50:26.930034 2050462464 net.cpp:338] conv1 -> conv1
I0502 16:50:26.930052 2050462464 net.cpp:113] Setting up conv1
I0502 16:50:26.930924 2050462464 net.cpp:120] Top shape: 64 96 55 55 (18585600)
I0502 16:50:26.930948 2050462464 layer_factory.hpp:74] Creating layer relu1
I0502 16:50:26.930960 2050462464 net.cpp:84] Creating Layer relu1
I0502 16:50:26.930966 2050462464 net.cpp:380] relu1 <- conv1
I0502 16:50:26.930974 2050462464 net.cpp:327] relu1 -> conv1 (in-place)
I0502 16:50:26.930980 2050462464 net.cpp:113] Setting up relu1
I0502 16:50:26.930995 2050462464 net.cpp:120] Top shape: 64 96 55 55 (18585600)
I0502 16:50:26.931000 2050462464 layer_factory.hpp:74] Creating layer pool1
I0502 16:50:26.931007 2050462464 net.cpp:84] Creating Layer pool1
I0502 16:50:26.931011 2050462464 net.cpp:380] pool1 <- conv1
I0502 16:50:26.931017 2050462464 net.cpp:338] pool1 -> pool1
I0502 16:50:26.931026 2050462464 net.cpp:113] Setting up pool1
I0502 16:50:26.931062 2050462464 net.cpp:120] Top shape: 64 96 27 27 (4478976)
I0502 16:50:26.931080 2050462464 layer_factory.hpp:74] Creating layer norm1
I0502 16:50:26.931097 2050462464 net.cpp:84] Creating Layer norm1
I0502 16:50:26.931107 2050462464 net.cpp:380] norm1 <- pool1
I0502 16:50:26.931116 2050462464 net.cpp:338] norm1 -> norm1
I0502 16:50:26.931129 2050462464 net.cpp:113] Setting up norm1
I0502 16:50:26.931144 2050462464 net.cpp:120] Top shape: 64 96 27 27 (4478976)
I0502 16:50:26.931155 2050462464 layer_factory.hpp:74] Creating layer conv2
I0502 16:50:26.931200 2050462464 net.cpp:84] Creating Layer conv2
I0502 16:50:26.931210 2050462464 net.cpp:380] conv2 <- norm1
I0502 16:50:26.931221 2050462464 net.cpp:338] conv2 -> conv2
I0502 16:50:26.931234 2050462464 net.cpp:113] Setting up conv2
I0502 16:50:26.937845 2050462464 net.cpp:120] Top shape: 64 256 27 27 (11943936)
I0502 16:50:26.937878 2050462464 layer_factory.hpp:74] Creating layer relu2
I0502 16:50:26.937892 2050462464 net.cpp:84] Creating Layer relu2
I0502 16:50:26.937901 2050462464 net.cpp:380] relu2 <- conv2
I0502 16:50:26.937911 2050462464 net.cpp:327] relu2 -> conv2 (in-place)
I0502 16:50:26.937923 2050462464 net.cpp:113] Setting up relu2
I0502 16:50:26.937932 2050462464 net.cpp:120] Top shape: 64 256 27 27 (11943936)
I0502 16:50:26.937942 2050462464 layer_factory.hpp:74] Creating layer pool2
I0502 16:50:26.937953 2050462464 net.cpp:84] Creating Layer pool2
I0502 16:50:26.937961 2050462464 net.cpp:380] pool2 <- conv2
I0502 16:50:26.937973 2050462464 net.cpp:338] pool2 -> pool2
I0502 16:50:26.937984 2050462464 net.cpp:113] Setting up pool2
I0502 16:50:26.937995 2050462464 net.cpp:120] Top shape: 64 256 13 13 (2768896)
I0502 16:50:26.938005 2050462464 layer_factory.hpp:74] Creating layer norm2
I0502 16:50:26.938016 2050462464 net.cpp:84] Creating Layer norm2
I0502 16:50:26.938025 2050462464 net.cpp:380] norm2 <- pool2
I0502 16:50:26.938030 2050462464 net.cpp:338] norm2 -> norm2
I0502 16:50:26.938037 2050462464 net.cpp:113] Setting up norm2
I0502 16:50:26.938042 2050462464 net.cpp:120] Top shape: 64 256 13 13 (2768896)
I0502 16:50:26.938048 2050462464 layer_factory.hpp:74] Creating layer conv3
I0502 16:50:26.938056 2050462464 net.cpp:84] Creating Layer conv3
I0502 16:50:26.938060 2050462464 net.cpp:380] conv3 <- norm2
I0502 16:50:26.938071 2050462464 net.cpp:338] conv3 -> conv3
I0502 16:50:26.938084 2050462464 net.cpp:113] Setting up conv3
I0502 16:50:26.957686 2050462464 net.cpp:120] Top shape: 64 384 13 13 (4153344)
I0502 16:50:26.957715 2050462464 layer_factory.hpp:74] Creating layer relu3
I0502 16:50:26.957726 2050462464 net.cpp:84] Creating Layer relu3
I0502 16:50:26.957731 2050462464 net.cpp:380] relu3 <- conv3
I0502 16:50:26.957737 2050462464 net.cpp:327] relu3 -> conv3 (in-place)
I0502 16:50:26.957744 2050462464 net.cpp:113] Setting up relu3
I0502 16:50:26.957749 2050462464 net.cpp:120] Top shape: 64 384 13 13 (4153344)
I0502 16:50:26.957756 2050462464 layer_factory.hpp:74] Creating layer conv4
I0502 16:50:26.957762 2050462464 net.cpp:84] Creating Layer conv4
I0502 16:50:26.957767 2050462464 net.cpp:380] conv4 <- conv3
I0502 16:50:26.957779 2050462464 net.cpp:338] conv4 -> conv4
I0502 16:50:26.957787 2050462464 net.cpp:113] Setting up conv4
I0502 16:50:26.970742 2050462464 net.cpp:120] Top shape: 64 384 13 13 (4153344)
I0502 16:50:26.970765 2050462464 layer_factory.hpp:74] Creating layer relu4
I0502 16:50:26.970775 2050462464 net.cpp:84] Creating Layer relu4
I0502 16:50:26.970780 2050462464 net.cpp:380] relu4 <- conv4
I0502 16:50:26.970813 2050462464 net.cpp:327] relu4 -> conv4 (in-place)
I0502 16:50:26.970826 2050462464 net.cpp:113] Setting up relu4
I0502 16:50:26.970854 2050462464 net.cpp:120] Top shape: 64 384 13 13 (4153344)
I0502 16:50:26.970903 2050462464 layer_factory.hpp:74] Creating layer conv5
I0502 16:50:26.970923 2050462464 net.cpp:84] Creating Layer conv5
I0502 16:50:26.970933 2050462464 net.cpp:380] conv5 <- conv4
I0502 16:50:26.970957 2050462464 net.cpp:338] conv5 -> conv5
I0502 16:50:26.970986 2050462464 net.cpp:113] Setting up conv5
I0502 16:50:26.979840 2050462464 net.cpp:120] Top shape: 64 256 13 13 (2768896)
I0502 16:50:26.979876 2050462464 layer_factory.hpp:74] Creating layer relu5
I0502 16:50:26.979892 2050462464 net.cpp:84] Creating Layer relu5
I0502 16:50:26.979902 2050462464 net.cpp:380] relu5 <- conv5
I0502 16:50:26.979912 2050462464 net.cpp:327] relu5 -> conv5 (in-place)
I0502 16:50:26.979924 2050462464 net.cpp:113] Setting up relu5
I0502 16:50:26.979933 2050462464 net.cpp:120] Top shape: 64 256 13 13 (2768896)
I0502 16:50:26.979944 2050462464 layer_factory.hpp:74] Creating layer pool5
I0502 16:50:26.979990 2050462464 net.cpp:84] Creating Layer pool5
I0502 16:50:26.980001 2050462464 net.cpp:380] pool5 <- conv5
I0502 16:50:26.980012 2050462464 net.cpp:338] pool5 -> pool5
I0502 16:50:26.980025 2050462464 net.cpp:113] Setting up pool5
I0502 16:50:26.980036 2050462464 net.cpp:120] Top shape: 64 256 6 6 (589824)
I0502 16:50:26.980047 2050462464 layer_factory.hpp:74] Creating layer fc6
I0502 16:50:26.980082 2050462464 net.cpp:84] Creating Layer fc6
I0502 16:50:26.980093 2050462464 net.cpp:380] fc6 <- pool5
I0502 16:50:26.980105 2050462464 net.cpp:338] fc6 -> fc6
I0502 16:50:26.980119 2050462464 net.cpp:113] Setting up fc6
I0502 16:50:27.606473 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0502 16:50:27.606505 2050462464 layer_factory.hpp:74] Creating layer relu6
I0502 16:50:27.606542 2050462464 net.cpp:84] Creating Layer relu6
I0502 16:50:27.606552 2050462464 net.cpp:380] relu6 <- fc6
I0502 16:50:27.606564 2050462464 net.cpp:327] relu6 -> fc6 (in-place)
I0502 16:50:27.606576 2050462464 net.cpp:113] Setting up relu6
I0502 16:50:27.606581 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0502 16:50:27.606587 2050462464 layer_factory.hpp:74] Creating layer drop6
I0502 16:50:27.606595 2050462464 net.cpp:84] Creating Layer drop6
I0502 16:50:27.606598 2050462464 net.cpp:380] drop6 <- fc6
I0502 16:50:27.606605 2050462464 net.cpp:327] drop6 -> fc6 (in-place)
I0502 16:50:27.606611 2050462464 net.cpp:113] Setting up drop6
I0502 16:50:27.606634 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0502 16:50:27.606642 2050462464 layer_factory.hpp:74] Creating layer fc7
I0502 16:50:27.606650 2050462464 net.cpp:84] Creating Layer fc7
I0502 16:50:27.606655 2050462464 net.cpp:380] fc7 <- fc6
I0502 16:50:27.606665 2050462464 net.cpp:338] fc7 -> fc7
I0502 16:50:27.606673 2050462464 net.cpp:113] Setting up fc7
I0502 16:50:27.901135 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0502 16:50:27.901170 2050462464 layer_factory.hpp:74] Creating layer relu7
I0502 16:50:27.901181 2050462464 net.cpp:84] Creating Layer relu7
I0502 16:50:27.901186 2050462464 net.cpp:380] relu7 <- fc7
I0502 16:50:27.901193 2050462464 net.cpp:327] relu7 -> fc7 (in-place)
I0502 16:50:27.901201 2050462464 net.cpp:113] Setting up relu7
I0502 16:50:27.901206 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0502 16:50:27.901211 2050462464 layer_factory.hpp:74] Creating layer drop7
I0502 16:50:27.901217 2050462464 net.cpp:84] Creating Layer drop7
I0502 16:50:27.901222 2050462464 net.cpp:380] drop7 <- fc7
I0502 16:50:27.901227 2050462464 net.cpp:327] drop7 -> fc7 (in-place)
I0502 16:50:27.901233 2050462464 net.cpp:113] Setting up drop7
I0502 16:50:27.901239 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0502 16:50:27.901244 2050462464 layer_factory.hpp:74] Creating layer fc8_VOC
I0502 16:50:27.901252 2050462464 net.cpp:84] Creating Layer fc8_VOC
I0502 16:50:27.901257 2050462464 net.cpp:380] fc8_VOC <- fc7
I0502 16:50:27.901262 2050462464 net.cpp:338] fc8_VOC -> fc8_VOC
I0502 16:50:27.901270 2050462464 net.cpp:113] Setting up fc8_VOC
I0502 16:50:27.901762 2050462464 net.cpp:120] Top shape: 64 6 (384)
I0502 16:50:27.901780 2050462464 layer_factory.hpp:74] Creating layer loss
I0502 16:50:27.901798 2050462464 net.cpp:84] Creating Layer loss
I0502 16:50:27.901809 2050462464 net.cpp:380] loss <- fc8_VOC
I0502 16:50:27.901818 2050462464 net.cpp:380] loss <- label
I0502 16:50:27.901829 2050462464 net.cpp:338] loss -> loss
I0502 16:50:27.901841 2050462464 net.cpp:113] Setting up loss
I0502 16:50:27.901852 2050462464 layer_factory.hpp:74] Creating layer loss
I0502 16:50:27.901877 2050462464 net.cpp:120] Top shape: (1)
I0502 16:50:27.901887 2050462464 net.cpp:122]     with loss weight 1
I0502 16:50:27.901906 2050462464 net.cpp:167] loss needs backward computation.
I0502 16:50:27.901916 2050462464 net.cpp:167] fc8_VOC needs backward computation.
I0502 16:50:27.901923 2050462464 net.cpp:167] drop7 needs backward computation.
I0502 16:50:27.901932 2050462464 net.cpp:167] relu7 needs backward computation.
I0502 16:50:27.901980 2050462464 net.cpp:167] fc7 needs backward computation.
I0502 16:50:27.901990 2050462464 net.cpp:167] drop6 needs backward computation.
I0502 16:50:27.901998 2050462464 net.cpp:167] relu6 needs backward computation.
I0502 16:50:27.902006 2050462464 net.cpp:167] fc6 needs backward computation.
I0502 16:50:27.902014 2050462464 net.cpp:167] pool5 needs backward computation.
I0502 16:50:27.902022 2050462464 net.cpp:167] relu5 needs backward computation.
I0502 16:50:27.902030 2050462464 net.cpp:167] conv5 needs backward computation.
I0502 16:50:27.902040 2050462464 net.cpp:167] relu4 needs backward computation.
I0502 16:50:27.902047 2050462464 net.cpp:167] conv4 needs backward computation.
I0502 16:50:27.902055 2050462464 net.cpp:167] relu3 needs backward computation.
I0502 16:50:27.902065 2050462464 net.cpp:167] conv3 needs backward computation.
I0502 16:50:27.902072 2050462464 net.cpp:167] norm2 needs backward computation.
I0502 16:50:27.902081 2050462464 net.cpp:167] pool2 needs backward computation.
I0502 16:50:27.902089 2050462464 net.cpp:167] relu2 needs backward computation.
I0502 16:50:27.902097 2050462464 net.cpp:167] conv2 needs backward computation.
I0502 16:50:27.902106 2050462464 net.cpp:167] norm1 needs backward computation.
I0502 16:50:27.902113 2050462464 net.cpp:167] pool1 needs backward computation.
I0502 16:50:27.902122 2050462464 net.cpp:167] relu1 needs backward computation.
I0502 16:50:27.902129 2050462464 net.cpp:167] conv1 needs backward computation.
I0502 16:50:27.902137 2050462464 net.cpp:169] data does not need backward computation.
I0502 16:50:27.902145 2050462464 net.cpp:205] This network produces output loss
I0502 16:50:27.902164 2050462464 net.cpp:447] Collecting Learning Rate and Weight Decay.
I0502 16:50:27.902179 2050462464 net.cpp:217] Network initialization done.
I0502 16:50:27.902186 2050462464 net.cpp:218] Memory required for data: 439050756
I0502 16:50:27.902628 2050462464 solver.cpp:154] Creating test net (#0) specified by net file: train_val_background.prototxt
I0502 16:50:27.902698 2050462464 net.cpp:257] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0502 16:50:27.902730 2050462464 net.cpp:42] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "ImageData"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  image_data_param {
    source: "../test_list.txt"
    batch_size: 64
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8_VOC"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8_VOC"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  param {
    lr_mult: 20
    decay_mult: 0
  }
  inner_product_param {
    num_output: 6
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_VOC"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_VOC"
  bottom: "label"
  top: "loss"
}
I0502 16:50:27.903491 2050462464 layer_factory.hpp:74] Creating layer data
I0502 16:50:27.903508 2050462464 net.cpp:84] Creating Layer data
I0502 16:50:27.903516 2050462464 net.cpp:338] data -> data
I0502 16:50:27.903528 2050462464 net.cpp:338] data -> label
I0502 16:50:27.903542 2050462464 net.cpp:113] Setting up data
I0502 16:50:27.903549 2050462464 image_data_layer.cpp:36] Opening file ../test_list.txt
I0502 16:50:27.907202 2050462464 image_data_layer.cpp:51] A total of 3703 images.
I0502 16:50:27.908303 2050462464 image_data_layer.cpp:80] output data size: 64,3,227,227
I0502 16:50:27.926653 2050462464 net.cpp:120] Top shape: 64 3 227 227 (9893568)
I0502 16:50:27.926681 2050462464 net.cpp:120] Top shape: 64 (64)
I0502 16:50:27.926689 2050462464 layer_factory.hpp:74] Creating layer label_data_1_split
I0502 16:50:27.926712 2050462464 net.cpp:84] Creating Layer label_data_1_split
I0502 16:50:27.926745 2050462464 net.cpp:380] label_data_1_split <- label
I0502 16:50:27.926777 2050462464 net.cpp:338] label_data_1_split -> label_data_1_split_0
I0502 16:50:27.926808 2050462464 net.cpp:338] label_data_1_split -> label_data_1_split_1
I0502 16:50:27.926823 2050462464 net.cpp:113] Setting up label_data_1_split
I0502 16:50:27.926836 2050462464 net.cpp:120] Top shape: 64 (64)
I0502 16:50:27.926846 2050462464 net.cpp:120] Top shape: 64 (64)
I0502 16:50:27.926854 2050462464 layer_factory.hpp:74] Creating layer conv1
I0502 16:50:27.926877 2050462464 net.cpp:84] Creating Layer conv1
I0502 16:50:27.926887 2050462464 net.cpp:380] conv1 <- data
I0502 16:50:27.926899 2050462464 net.cpp:338] conv1 -> conv1
I0502 16:50:27.926915 2050462464 net.cpp:113] Setting up conv1
I0502 16:50:27.927598 2050462464 net.cpp:120] Top shape: 64 96 55 55 (18585600)
I0502 16:50:27.927620 2050462464 layer_factory.hpp:74] Creating layer relu1
I0502 16:50:27.927634 2050462464 net.cpp:84] Creating Layer relu1
I0502 16:50:27.927641 2050462464 net.cpp:380] relu1 <- conv1
I0502 16:50:27.927652 2050462464 net.cpp:327] relu1 -> conv1 (in-place)
I0502 16:50:27.927664 2050462464 net.cpp:113] Setting up relu1
I0502 16:50:27.927672 2050462464 net.cpp:120] Top shape: 64 96 55 55 (18585600)
I0502 16:50:27.927685 2050462464 layer_factory.hpp:74] Creating layer pool1
I0502 16:50:27.927697 2050462464 net.cpp:84] Creating Layer pool1
I0502 16:50:27.927706 2050462464 net.cpp:380] pool1 <- conv1
I0502 16:50:27.927714 2050462464 net.cpp:338] pool1 -> pool1
I0502 16:50:27.927726 2050462464 net.cpp:113] Setting up pool1
I0502 16:50:27.927736 2050462464 net.cpp:120] Top shape: 64 96 27 27 (4478976)
I0502 16:50:27.927747 2050462464 layer_factory.hpp:74] Creating layer norm1
I0502 16:50:27.927757 2050462464 net.cpp:84] Creating Layer norm1
I0502 16:50:27.927765 2050462464 net.cpp:380] norm1 <- pool1
I0502 16:50:27.927778 2050462464 net.cpp:338] norm1 -> norm1
I0502 16:50:27.927790 2050462464 net.cpp:113] Setting up norm1
I0502 16:50:27.927803 2050462464 net.cpp:120] Top shape: 64 96 27 27 (4478976)
I0502 16:50:27.927814 2050462464 layer_factory.hpp:74] Creating layer conv2
I0502 16:50:27.927825 2050462464 net.cpp:84] Creating Layer conv2
I0502 16:50:27.927834 2050462464 net.cpp:380] conv2 <- norm1
I0502 16:50:27.927844 2050462464 net.cpp:338] conv2 -> conv2
I0502 16:50:27.927855 2050462464 net.cpp:113] Setting up conv2
I0502 16:50:27.934434 2050462464 net.cpp:120] Top shape: 64 256 27 27 (11943936)
I0502 16:50:27.934483 2050462464 layer_factory.hpp:74] Creating layer relu2
I0502 16:50:27.934499 2050462464 net.cpp:84] Creating Layer relu2
I0502 16:50:27.934509 2050462464 net.cpp:380] relu2 <- conv2
I0502 16:50:27.934520 2050462464 net.cpp:327] relu2 -> conv2 (in-place)
I0502 16:50:27.934532 2050462464 net.cpp:113] Setting up relu2
I0502 16:50:27.934540 2050462464 net.cpp:120] Top shape: 64 256 27 27 (11943936)
I0502 16:50:27.934551 2050462464 layer_factory.hpp:74] Creating layer pool2
I0502 16:50:27.934562 2050462464 net.cpp:84] Creating Layer pool2
I0502 16:50:27.934571 2050462464 net.cpp:380] pool2 <- conv2
I0502 16:50:27.934581 2050462464 net.cpp:338] pool2 -> pool2
I0502 16:50:27.934604 2050462464 net.cpp:113] Setting up pool2
I0502 16:50:27.934617 2050462464 net.cpp:120] Top shape: 64 256 13 13 (2768896)
I0502 16:50:27.934628 2050462464 layer_factory.hpp:74] Creating layer norm2
I0502 16:50:27.934638 2050462464 net.cpp:84] Creating Layer norm2
I0502 16:50:27.934646 2050462464 net.cpp:380] norm2 <- pool2
I0502 16:50:27.934655 2050462464 net.cpp:338] norm2 -> norm2
I0502 16:50:27.934666 2050462464 net.cpp:113] Setting up norm2
I0502 16:50:27.934676 2050462464 net.cpp:120] Top shape: 64 256 13 13 (2768896)
I0502 16:50:27.934687 2050462464 layer_factory.hpp:74] Creating layer conv3
I0502 16:50:27.934700 2050462464 net.cpp:84] Creating Layer conv3
I0502 16:50:27.934708 2050462464 net.cpp:380] conv3 <- norm2
I0502 16:50:27.934718 2050462464 net.cpp:338] conv3 -> conv3
I0502 16:50:27.934731 2050462464 net.cpp:113] Setting up conv3
I0502 16:50:27.953861 2050462464 net.cpp:120] Top shape: 64 384 13 13 (4153344)
I0502 16:50:27.953917 2050462464 layer_factory.hpp:74] Creating layer relu3
I0502 16:50:27.953934 2050462464 net.cpp:84] Creating Layer relu3
I0502 16:50:27.953943 2050462464 net.cpp:380] relu3 <- conv3
I0502 16:50:27.953953 2050462464 net.cpp:327] relu3 -> conv3 (in-place)
I0502 16:50:27.953964 2050462464 net.cpp:113] Setting up relu3
I0502 16:50:27.953970 2050462464 net.cpp:120] Top shape: 64 384 13 13 (4153344)
I0502 16:50:27.953981 2050462464 layer_factory.hpp:74] Creating layer conv4
I0502 16:50:27.953992 2050462464 net.cpp:84] Creating Layer conv4
I0502 16:50:27.953999 2050462464 net.cpp:380] conv4 <- conv3
I0502 16:50:27.954010 2050462464 net.cpp:338] conv4 -> conv4
I0502 16:50:27.954021 2050462464 net.cpp:113] Setting up conv4
I0502 16:50:27.967746 2050462464 net.cpp:120] Top shape: 64 384 13 13 (4153344)
I0502 16:50:27.967787 2050462464 layer_factory.hpp:74] Creating layer relu4
I0502 16:50:27.967803 2050462464 net.cpp:84] Creating Layer relu4
I0502 16:50:27.967813 2050462464 net.cpp:380] relu4 <- conv4
I0502 16:50:27.967824 2050462464 net.cpp:327] relu4 -> conv4 (in-place)
I0502 16:50:27.967838 2050462464 net.cpp:113] Setting up relu4
I0502 16:50:27.967846 2050462464 net.cpp:120] Top shape: 64 384 13 13 (4153344)
I0502 16:50:27.967857 2050462464 layer_factory.hpp:74] Creating layer conv5
I0502 16:50:27.967870 2050462464 net.cpp:84] Creating Layer conv5
I0502 16:50:27.967880 2050462464 net.cpp:380] conv5 <- conv4
I0502 16:50:27.967891 2050462464 net.cpp:338] conv5 -> conv5
I0502 16:50:27.967905 2050462464 net.cpp:113] Setting up conv5
I0502 16:50:27.975803 2050462464 net.cpp:120] Top shape: 64 256 13 13 (2768896)
I0502 16:50:27.975823 2050462464 layer_factory.hpp:74] Creating layer relu5
I0502 16:50:27.975831 2050462464 net.cpp:84] Creating Layer relu5
I0502 16:50:27.975836 2050462464 net.cpp:380] relu5 <- conv5
I0502 16:50:27.975842 2050462464 net.cpp:327] relu5 -> conv5 (in-place)
I0502 16:50:27.975848 2050462464 net.cpp:113] Setting up relu5
I0502 16:50:27.975853 2050462464 net.cpp:120] Top shape: 64 256 13 13 (2768896)
I0502 16:50:27.975858 2050462464 layer_factory.hpp:74] Creating layer pool5
I0502 16:50:27.975867 2050462464 net.cpp:84] Creating Layer pool5
I0502 16:50:27.975870 2050462464 net.cpp:380] pool5 <- conv5
I0502 16:50:27.975877 2050462464 net.cpp:338] pool5 -> pool5
I0502 16:50:27.975883 2050462464 net.cpp:113] Setting up pool5
I0502 16:50:27.975890 2050462464 net.cpp:120] Top shape: 64 256 6 6 (589824)
I0502 16:50:27.975895 2050462464 layer_factory.hpp:74] Creating layer fc6
I0502 16:50:27.975903 2050462464 net.cpp:84] Creating Layer fc6
I0502 16:50:27.975908 2050462464 net.cpp:380] fc6 <- pool5
I0502 16:50:27.975914 2050462464 net.cpp:338] fc6 -> fc6
I0502 16:50:27.975921 2050462464 net.cpp:113] Setting up fc6
I0502 16:50:28.582370 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0502 16:50:28.582406 2050462464 layer_factory.hpp:74] Creating layer relu6
I0502 16:50:28.582417 2050462464 net.cpp:84] Creating Layer relu6
I0502 16:50:28.582423 2050462464 net.cpp:380] relu6 <- fc6
I0502 16:50:28.582430 2050462464 net.cpp:327] relu6 -> fc6 (in-place)
I0502 16:50:28.582438 2050462464 net.cpp:113] Setting up relu6
I0502 16:50:28.582444 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0502 16:50:28.582449 2050462464 layer_factory.hpp:74] Creating layer drop6
I0502 16:50:28.582463 2050462464 net.cpp:84] Creating Layer drop6
I0502 16:50:28.582468 2050462464 net.cpp:380] drop6 <- fc6
I0502 16:50:28.582475 2050462464 net.cpp:327] drop6 -> fc6 (in-place)
I0502 16:50:28.582484 2050462464 net.cpp:113] Setting up drop6
I0502 16:50:28.582504 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0502 16:50:28.582515 2050462464 layer_factory.hpp:74] Creating layer fc7
I0502 16:50:28.582525 2050462464 net.cpp:84] Creating Layer fc7
I0502 16:50:28.582530 2050462464 net.cpp:380] fc7 <- fc6
I0502 16:50:28.582536 2050462464 net.cpp:338] fc7 -> fc7
I0502 16:50:28.582545 2050462464 net.cpp:113] Setting up fc7
I0502 16:50:28.848763 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0502 16:50:28.848839 2050462464 layer_factory.hpp:74] Creating layer relu7
I0502 16:50:28.848860 2050462464 net.cpp:84] Creating Layer relu7
I0502 16:50:28.848866 2050462464 net.cpp:380] relu7 <- fc7
I0502 16:50:28.848891 2050462464 net.cpp:327] relu7 -> fc7 (in-place)
I0502 16:50:28.848899 2050462464 net.cpp:113] Setting up relu7
I0502 16:50:28.848904 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0502 16:50:28.848909 2050462464 layer_factory.hpp:74] Creating layer drop7
I0502 16:50:28.848917 2050462464 net.cpp:84] Creating Layer drop7
I0502 16:50:28.848920 2050462464 net.cpp:380] drop7 <- fc7
I0502 16:50:28.848925 2050462464 net.cpp:327] drop7 -> fc7 (in-place)
I0502 16:50:28.848942 2050462464 net.cpp:113] Setting up drop7
I0502 16:50:28.848948 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0502 16:50:28.848953 2050462464 layer_factory.hpp:74] Creating layer fc8_VOC
I0502 16:50:28.848960 2050462464 net.cpp:84] Creating Layer fc8_VOC
I0502 16:50:28.848964 2050462464 net.cpp:380] fc8_VOC <- fc7
I0502 16:50:28.848971 2050462464 net.cpp:338] fc8_VOC -> fc8_VOC
I0502 16:50:28.848983 2050462464 net.cpp:113] Setting up fc8_VOC
I0502 16:50:28.849462 2050462464 net.cpp:120] Top shape: 64 6 (384)
I0502 16:50:28.849470 2050462464 layer_factory.hpp:74] Creating layer fc8_VOC_fc8_VOC_0_split
I0502 16:50:28.849488 2050462464 net.cpp:84] Creating Layer fc8_VOC_fc8_VOC_0_split
I0502 16:50:28.849493 2050462464 net.cpp:380] fc8_VOC_fc8_VOC_0_split <- fc8_VOC
I0502 16:50:28.849498 2050462464 net.cpp:338] fc8_VOC_fc8_VOC_0_split -> fc8_VOC_fc8_VOC_0_split_0
I0502 16:50:28.849505 2050462464 net.cpp:338] fc8_VOC_fc8_VOC_0_split -> fc8_VOC_fc8_VOC_0_split_1
I0502 16:50:28.849513 2050462464 net.cpp:113] Setting up fc8_VOC_fc8_VOC_0_split
I0502 16:50:28.849517 2050462464 net.cpp:120] Top shape: 64 6 (384)
I0502 16:50:28.849522 2050462464 net.cpp:120] Top shape: 64 6 (384)
I0502 16:50:28.849539 2050462464 layer_factory.hpp:74] Creating layer accuracy
I0502 16:50:28.849550 2050462464 net.cpp:84] Creating Layer accuracy
I0502 16:50:28.849555 2050462464 net.cpp:380] accuracy <- fc8_VOC_fc8_VOC_0_split_0
I0502 16:50:28.849560 2050462464 net.cpp:380] accuracy <- label_data_1_split_0
I0502 16:50:28.849567 2050462464 net.cpp:338] accuracy -> accuracy
I0502 16:50:28.849584 2050462464 net.cpp:113] Setting up accuracy
I0502 16:50:28.849589 2050462464 net.cpp:120] Top shape: (1)
I0502 16:50:28.849593 2050462464 layer_factory.hpp:74] Creating layer loss
I0502 16:50:28.849599 2050462464 net.cpp:84] Creating Layer loss
I0502 16:50:28.849603 2050462464 net.cpp:380] loss <- fc8_VOC_fc8_VOC_0_split_1
I0502 16:50:28.849608 2050462464 net.cpp:380] loss <- label_data_1_split_1
I0502 16:50:28.849613 2050462464 net.cpp:338] loss -> loss
I0502 16:50:28.849619 2050462464 net.cpp:113] Setting up loss
I0502 16:50:28.849624 2050462464 layer_factory.hpp:74] Creating layer loss
I0502 16:50:28.849635 2050462464 net.cpp:120] Top shape: (1)
I0502 16:50:28.849652 2050462464 net.cpp:122]     with loss weight 1
I0502 16:50:28.849658 2050462464 net.cpp:167] loss needs backward computation.
I0502 16:50:28.849694 2050462464 net.cpp:169] accuracy does not need backward computation.
I0502 16:50:28.849699 2050462464 net.cpp:167] fc8_VOC_fc8_VOC_0_split needs backward computation.
I0502 16:50:28.849714 2050462464 net.cpp:167] fc8_VOC needs backward computation.
I0502 16:50:28.849719 2050462464 net.cpp:167] drop7 needs backward computation.
I0502 16:50:28.849722 2050462464 net.cpp:167] relu7 needs backward computation.
I0502 16:50:28.849726 2050462464 net.cpp:167] fc7 needs backward computation.
I0502 16:50:28.849731 2050462464 net.cpp:167] drop6 needs backward computation.
I0502 16:50:28.849735 2050462464 net.cpp:167] relu6 needs backward computation.
I0502 16:50:28.849740 2050462464 net.cpp:167] fc6 needs backward computation.
I0502 16:50:28.849743 2050462464 net.cpp:167] pool5 needs backward computation.
I0502 16:50:28.849748 2050462464 net.cpp:167] relu5 needs backward computation.
I0502 16:50:28.849762 2050462464 net.cpp:167] conv5 needs backward computation.
I0502 16:50:28.849779 2050462464 net.cpp:167] relu4 needs backward computation.
I0502 16:50:28.849784 2050462464 net.cpp:167] conv4 needs backward computation.
I0502 16:50:28.849788 2050462464 net.cpp:167] relu3 needs backward computation.
I0502 16:50:28.849793 2050462464 net.cpp:167] conv3 needs backward computation.
I0502 16:50:28.849797 2050462464 net.cpp:167] norm2 needs backward computation.
I0502 16:50:28.849802 2050462464 net.cpp:167] pool2 needs backward computation.
I0502 16:50:28.849906 2050462464 net.cpp:167] relu2 needs backward computation.
I0502 16:50:28.849928 2050462464 net.cpp:167] conv2 needs backward computation.
I0502 16:50:28.849933 2050462464 net.cpp:167] norm1 needs backward computation.
I0502 16:50:28.849938 2050462464 net.cpp:167] pool1 needs backward computation.
I0502 16:50:28.849942 2050462464 net.cpp:167] relu1 needs backward computation.
I0502 16:50:28.849947 2050462464 net.cpp:167] conv1 needs backward computation.
I0502 16:50:28.849951 2050462464 net.cpp:169] label_data_1_split does not need backward computation.
I0502 16:50:28.849956 2050462464 net.cpp:169] data does not need backward computation.
I0502 16:50:28.849970 2050462464 net.cpp:205] This network produces output accuracy
I0502 16:50:28.849975 2050462464 net.cpp:205] This network produces output loss
I0502 16:50:28.849992 2050462464 net.cpp:447] Collecting Learning Rate and Weight Decay.
I0502 16:50:28.849999 2050462464 net.cpp:217] Network initialization done.
I0502 16:50:28.850003 2050462464 net.cpp:218] Memory required for data: 439054344
I0502 16:50:28.850152 2050462464 solver.cpp:42] Solver scaffolding done.
I0502 16:50:28.850226 2050462464 caffe.cpp:86] Finetuning from bvlc_reference_rcnn_ilsvrc13.caffemodel
E0502 16:50:29.297093 2050462464 upgrade_proto.cpp:618] Attempting to upgrade input file specified using deprecated V1LayerParameter: bvlc_reference_rcnn_ilsvrc13.caffemodel
I0502 16:50:29.622813 2050462464 upgrade_proto.cpp:626] Successfully upgraded file specified using deprecated V1LayerParameter
E0502 16:50:30.176532 2050462464 upgrade_proto.cpp:618] Attempting to upgrade input file specified using deprecated V1LayerParameter: bvlc_reference_rcnn_ilsvrc13.caffemodel
I0502 16:50:30.438390 2050462464 upgrade_proto.cpp:626] Successfully upgraded file specified using deprecated V1LayerParameter
I0502 16:50:30.541270 2050462464 solver.cpp:222] Solving CaffeNet
I0502 16:50:30.541297 2050462464 solver.cpp:223] Learning Rate Policy: step
I0502 16:50:30.541317 2050462464 solver.cpp:266] Iteration 0, Testing net (#0)
I0502 17:24:15.094189 2050462464 solver.cpp:315]     Test net output #0: accuracy = 0.143125
I0502 17:24:15.094566 2050462464 solver.cpp:315]     Test net output #1: loss = 1.83667 (* 1 = 1.83667 loss)
I0502 17:24:20.339607 2050462464 solver.cpp:189] Iteration 0, loss = 1.83363
I0502 17:24:20.339651 2050462464 solver.cpp:204]     Train net output #0: loss = 1.83363 (* 1 = 1.83363 loss)
I0502 17:24:20.339666 2050462464 solver.cpp:464] Iteration 0, lr = 0.001
I0502 17:26:03.691763 2050462464 solver.cpp:189] Iteration 20, loss = 0.102634
I0502 17:26:03.692046 2050462464 solver.cpp:204]     Train net output #0: loss = 0.102634 (* 1 = 0.102634 loss)
I0502 17:26:03.692061 2050462464 solver.cpp:464] Iteration 20, lr = 0.001
I0502 17:27:43.824165 2050462464 solver.cpp:189] Iteration 40, loss = 0.114834
I0502 17:27:43.824484 2050462464 solver.cpp:204]     Train net output #0: loss = 0.114834 (* 1 = 0.114834 loss)
I0502 17:27:43.824498 2050462464 solver.cpp:464] Iteration 40, lr = 0.001
I0502 17:29:25.230723 2050462464 solver.cpp:189] Iteration 60, loss = 0.0647451
I0502 17:29:25.230768 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0647451 (* 1 = 0.0647451 loss)
I0502 17:29:25.230778 2050462464 solver.cpp:464] Iteration 60, lr = 0.001
^C
C02MX066FD58:rcc_net cusgadmin$ caffe train -solver solver_background.prototxt -weights bvlc_reference_rcnn_ilsvrc13.caffemodel 
I0502 17:31:53.733343 2050462464 caffe.cpp:117] Use CPU.
I0502 17:31:53.735167 2050462464 caffe.cpp:121] Starting Optimization
I0502 17:31:53.735364 2050462464 solver.cpp:32] Initializing solver from parameters: 
test_iter: 1000
test_interval: 1000
base_lr: 0.001
display: 20
max_iter: 10000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 100000
snapshot: 500
snapshot_prefix: "caffenet_train_background"
solver_mode: CPU
net: "train_val_background.prototxt"
I0502 17:31:53.735467 2050462464 solver.cpp:70] Creating training net from net file: train_val_background.prototxt
I0502 17:31:53.735997 2050462464 net.cpp:257] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0502 17:31:53.736037 2050462464 net.cpp:257] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0502 17:31:53.736083 2050462464 net.cpp:42] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "ImageData"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  image_data_param {
    source: "../train_list.txt"
    batch_size: 64
    shuffle: true
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8_VOC"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8_VOC"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  param {
    lr_mult: 20
    decay_mult: 0
  }
  inner_product_param {
    num_output: 6
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_VOC"
  bottom: "label"
  top: "loss"
}
I0502 17:31:53.736994 2050462464 layer_factory.hpp:74] Creating layer data
I0502 17:31:53.737025 2050462464 net.cpp:84] Creating Layer data
I0502 17:31:53.737036 2050462464 net.cpp:338] data -> data
I0502 17:31:53.737066 2050462464 net.cpp:338] data -> label
I0502 17:31:53.737099 2050462464 net.cpp:113] Setting up data
I0502 17:31:53.737128 2050462464 image_data_layer.cpp:36] Opening file ../train_list.txt
I0502 17:31:53.789082 2050462464 image_data_layer.cpp:46] Shuffling data
I0502 17:31:53.797718 2050462464 image_data_layer.cpp:51] A total of 32601 images.
I0502 17:31:53.799712 2050462464 image_data_layer.cpp:80] output data size: 64,3,227,227
I0502 17:31:53.821724 2050462464 net.cpp:120] Top shape: 64 3 227 227 (9893568)
I0502 17:31:53.821759 2050462464 net.cpp:120] Top shape: 64 (64)
I0502 17:31:53.821777 2050462464 layer_factory.hpp:74] Creating layer conv1
I0502 17:31:53.821815 2050462464 net.cpp:84] Creating Layer conv1
I0502 17:31:53.821827 2050462464 net.cpp:380] conv1 <- data
I0502 17:31:53.821841 2050462464 net.cpp:338] conv1 -> conv1
I0502 17:31:53.821857 2050462464 net.cpp:113] Setting up conv1
I0502 17:31:53.822726 2050462464 net.cpp:120] Top shape: 64 96 55 55 (18585600)
I0502 17:31:53.822749 2050462464 layer_factory.hpp:74] Creating layer relu1
I0502 17:31:53.822765 2050462464 net.cpp:84] Creating Layer relu1
I0502 17:31:53.822773 2050462464 net.cpp:380] relu1 <- conv1
I0502 17:31:53.822784 2050462464 net.cpp:327] relu1 -> conv1 (in-place)
I0502 17:31:53.822795 2050462464 net.cpp:113] Setting up relu1
I0502 17:31:53.822803 2050462464 net.cpp:120] Top shape: 64 96 55 55 (18585600)
I0502 17:31:53.822814 2050462464 layer_factory.hpp:74] Creating layer pool1
I0502 17:31:53.822829 2050462464 net.cpp:84] Creating Layer pool1
I0502 17:31:53.822839 2050462464 net.cpp:380] pool1 <- conv1
I0502 17:31:53.822849 2050462464 net.cpp:338] pool1 -> pool1
I0502 17:31:53.822866 2050462464 net.cpp:113] Setting up pool1
I0502 17:31:53.822902 2050462464 net.cpp:120] Top shape: 64 96 27 27 (4478976)
I0502 17:31:53.822918 2050462464 layer_factory.hpp:74] Creating layer norm1
I0502 17:31:53.822940 2050462464 net.cpp:84] Creating Layer norm1
I0502 17:31:53.822952 2050462464 net.cpp:380] norm1 <- pool1
I0502 17:31:53.822965 2050462464 net.cpp:338] norm1 -> norm1
I0502 17:31:53.822978 2050462464 net.cpp:113] Setting up norm1
I0502 17:31:53.822998 2050462464 net.cpp:120] Top shape: 64 96 27 27 (4478976)
I0502 17:31:53.823070 2050462464 layer_factory.hpp:74] Creating layer conv2
I0502 17:31:53.823091 2050462464 net.cpp:84] Creating Layer conv2
I0502 17:31:53.823101 2050462464 net.cpp:380] conv2 <- norm1
I0502 17:31:53.823114 2050462464 net.cpp:338] conv2 -> conv2
I0502 17:31:53.823129 2050462464 net.cpp:113] Setting up conv2
I0502 17:31:53.830519 2050462464 net.cpp:120] Top shape: 64 256 27 27 (11943936)
I0502 17:31:53.830560 2050462464 layer_factory.hpp:74] Creating layer relu2
I0502 17:31:53.830572 2050462464 net.cpp:84] Creating Layer relu2
I0502 17:31:53.830577 2050462464 net.cpp:380] relu2 <- conv2
I0502 17:31:53.830585 2050462464 net.cpp:327] relu2 -> conv2 (in-place)
I0502 17:31:53.830591 2050462464 net.cpp:113] Setting up relu2
I0502 17:31:53.830595 2050462464 net.cpp:120] Top shape: 64 256 27 27 (11943936)
I0502 17:31:53.830602 2050462464 layer_factory.hpp:74] Creating layer pool2
I0502 17:31:53.830608 2050462464 net.cpp:84] Creating Layer pool2
I0502 17:31:53.830612 2050462464 net.cpp:380] pool2 <- conv2
I0502 17:31:53.830618 2050462464 net.cpp:338] pool2 -> pool2
I0502 17:31:53.830624 2050462464 net.cpp:113] Setting up pool2
I0502 17:31:53.830631 2050462464 net.cpp:120] Top shape: 64 256 13 13 (2768896)
I0502 17:31:53.830638 2050462464 layer_factory.hpp:74] Creating layer norm2
I0502 17:31:53.830651 2050462464 net.cpp:84] Creating Layer norm2
I0502 17:31:53.830659 2050462464 net.cpp:380] norm2 <- pool2
I0502 17:31:53.830668 2050462464 net.cpp:338] norm2 -> norm2
I0502 17:31:53.830679 2050462464 net.cpp:113] Setting up norm2
I0502 17:31:53.830689 2050462464 net.cpp:120] Top shape: 64 256 13 13 (2768896)
I0502 17:31:53.830700 2050462464 layer_factory.hpp:74] Creating layer conv3
I0502 17:31:53.830711 2050462464 net.cpp:84] Creating Layer conv3
I0502 17:31:53.830718 2050462464 net.cpp:380] conv3 <- norm2
I0502 17:31:53.830729 2050462464 net.cpp:338] conv3 -> conv3
I0502 17:31:53.830746 2050462464 net.cpp:113] Setting up conv3
I0502 17:31:53.849853 2050462464 net.cpp:120] Top shape: 64 384 13 13 (4153344)
I0502 17:31:53.849889 2050462464 layer_factory.hpp:74] Creating layer relu3
I0502 17:31:53.849905 2050462464 net.cpp:84] Creating Layer relu3
I0502 17:31:53.849915 2050462464 net.cpp:380] relu3 <- conv3
I0502 17:31:53.849925 2050462464 net.cpp:327] relu3 -> conv3 (in-place)
I0502 17:31:53.849937 2050462464 net.cpp:113] Setting up relu3
I0502 17:31:53.849946 2050462464 net.cpp:120] Top shape: 64 384 13 13 (4153344)
I0502 17:31:53.849957 2050462464 layer_factory.hpp:74] Creating layer conv4
I0502 17:31:53.849970 2050462464 net.cpp:84] Creating Layer conv4
I0502 17:31:53.849979 2050462464 net.cpp:380] conv4 <- conv3
I0502 17:31:53.849997 2050462464 net.cpp:338] conv4 -> conv4
I0502 17:31:53.850011 2050462464 net.cpp:113] Setting up conv4
I0502 17:31:53.864258 2050462464 net.cpp:120] Top shape: 64 384 13 13 (4153344)
I0502 17:31:53.864291 2050462464 layer_factory.hpp:74] Creating layer relu4
I0502 17:31:53.864310 2050462464 net.cpp:84] Creating Layer relu4
I0502 17:31:53.864320 2050462464 net.cpp:380] relu4 <- conv4
I0502 17:31:53.864333 2050462464 net.cpp:327] relu4 -> conv4 (in-place)
I0502 17:31:53.864356 2050462464 net.cpp:113] Setting up relu4
I0502 17:31:53.864364 2050462464 net.cpp:120] Top shape: 64 384 13 13 (4153344)
I0502 17:31:53.864375 2050462464 layer_factory.hpp:74] Creating layer conv5
I0502 17:31:53.864390 2050462464 net.cpp:84] Creating Layer conv5
I0502 17:31:53.864399 2050462464 net.cpp:380] conv5 <- conv4
I0502 17:31:53.864411 2050462464 net.cpp:338] conv5 -> conv5
I0502 17:31:53.864426 2050462464 net.cpp:113] Setting up conv5
I0502 17:31:53.874091 2050462464 net.cpp:120] Top shape: 64 256 13 13 (2768896)
I0502 17:31:53.874124 2050462464 layer_factory.hpp:74] Creating layer relu5
I0502 17:31:53.874137 2050462464 net.cpp:84] Creating Layer relu5
I0502 17:31:53.874145 2050462464 net.cpp:380] relu5 <- conv5
I0502 17:31:53.874168 2050462464 net.cpp:327] relu5 -> conv5 (in-place)
I0502 17:31:53.874197 2050462464 net.cpp:113] Setting up relu5
I0502 17:31:53.874203 2050462464 net.cpp:120] Top shape: 64 256 13 13 (2768896)
I0502 17:31:53.874240 2050462464 layer_factory.hpp:74] Creating layer pool5
I0502 17:31:53.874264 2050462464 net.cpp:84] Creating Layer pool5
I0502 17:31:53.874272 2050462464 net.cpp:380] pool5 <- conv5
I0502 17:31:53.874284 2050462464 net.cpp:338] pool5 -> pool5
I0502 17:31:53.874297 2050462464 net.cpp:113] Setting up pool5
I0502 17:31:53.874310 2050462464 net.cpp:120] Top shape: 64 256 6 6 (589824)
I0502 17:31:53.874320 2050462464 layer_factory.hpp:74] Creating layer fc6
I0502 17:31:53.874336 2050462464 net.cpp:84] Creating Layer fc6
I0502 17:31:53.874343 2050462464 net.cpp:380] fc6 <- pool5
I0502 17:31:53.874356 2050462464 net.cpp:338] fc6 -> fc6
I0502 17:31:53.874367 2050462464 net.cpp:113] Setting up fc6
I0502 17:31:54.580421 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0502 17:31:54.580453 2050462464 layer_factory.hpp:74] Creating layer relu6
I0502 17:31:54.580466 2050462464 net.cpp:84] Creating Layer relu6
I0502 17:31:54.580473 2050462464 net.cpp:380] relu6 <- fc6
I0502 17:31:54.580483 2050462464 net.cpp:327] relu6 -> fc6 (in-place)
I0502 17:31:54.580497 2050462464 net.cpp:113] Setting up relu6
I0502 17:31:54.580505 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0502 17:31:54.580514 2050462464 layer_factory.hpp:74] Creating layer drop6
I0502 17:31:54.580523 2050462464 net.cpp:84] Creating Layer drop6
I0502 17:31:54.580530 2050462464 net.cpp:380] drop6 <- fc6
I0502 17:31:54.580538 2050462464 net.cpp:327] drop6 -> fc6 (in-place)
I0502 17:31:54.580543 2050462464 net.cpp:113] Setting up drop6
I0502 17:31:54.580569 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0502 17:31:54.580590 2050462464 layer_factory.hpp:74] Creating layer fc7
I0502 17:31:54.580605 2050462464 net.cpp:84] Creating Layer fc7
I0502 17:31:54.580612 2050462464 net.cpp:380] fc7 <- fc6
I0502 17:31:54.580626 2050462464 net.cpp:338] fc7 -> fc7
I0502 17:31:54.580641 2050462464 net.cpp:113] Setting up fc7
I0502 17:31:54.880962 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0502 17:31:54.881000 2050462464 layer_factory.hpp:74] Creating layer relu7
I0502 17:31:54.881016 2050462464 net.cpp:84] Creating Layer relu7
I0502 17:31:54.881024 2050462464 net.cpp:380] relu7 <- fc7
I0502 17:31:54.881052 2050462464 net.cpp:327] relu7 -> fc7 (in-place)
I0502 17:31:54.881077 2050462464 net.cpp:113] Setting up relu7
I0502 17:31:54.881088 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0502 17:31:54.881098 2050462464 layer_factory.hpp:74] Creating layer drop7
I0502 17:31:54.881111 2050462464 net.cpp:84] Creating Layer drop7
I0502 17:31:54.881119 2050462464 net.cpp:380] drop7 <- fc7
I0502 17:31:54.881129 2050462464 net.cpp:327] drop7 -> fc7 (in-place)
I0502 17:31:54.881141 2050462464 net.cpp:113] Setting up drop7
I0502 17:31:54.881150 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0502 17:31:54.881160 2050462464 layer_factory.hpp:74] Creating layer fc8_VOC
I0502 17:31:54.881175 2050462464 net.cpp:84] Creating Layer fc8_VOC
I0502 17:31:54.881183 2050462464 net.cpp:380] fc8_VOC <- fc7
I0502 17:31:54.881194 2050462464 net.cpp:338] fc8_VOC -> fc8_VOC
I0502 17:31:54.881208 2050462464 net.cpp:113] Setting up fc8_VOC
I0502 17:31:54.881755 2050462464 net.cpp:120] Top shape: 64 6 (384)
I0502 17:31:54.881781 2050462464 layer_factory.hpp:74] Creating layer loss
I0502 17:31:54.881798 2050462464 net.cpp:84] Creating Layer loss
I0502 17:31:54.881804 2050462464 net.cpp:380] loss <- fc8_VOC
I0502 17:31:54.881809 2050462464 net.cpp:380] loss <- label
I0502 17:31:54.881820 2050462464 net.cpp:338] loss -> loss
I0502 17:31:54.881829 2050462464 net.cpp:113] Setting up loss
I0502 17:31:54.881834 2050462464 layer_factory.hpp:74] Creating layer loss
I0502 17:31:54.881846 2050462464 net.cpp:120] Top shape: (1)
I0502 17:31:54.881855 2050462464 net.cpp:122]     with loss weight 1
I0502 17:31:54.881887 2050462464 net.cpp:167] loss needs backward computation.
I0502 17:31:54.881901 2050462464 net.cpp:167] fc8_VOC needs backward computation.
I0502 17:31:54.881911 2050462464 net.cpp:167] drop7 needs backward computation.
I0502 17:31:54.881949 2050462464 net.cpp:167] relu7 needs backward computation.
I0502 17:31:54.881978 2050462464 net.cpp:167] fc7 needs backward computation.
I0502 17:31:54.881989 2050462464 net.cpp:167] drop6 needs backward computation.
I0502 17:31:54.881995 2050462464 net.cpp:167] relu6 needs backward computation.
I0502 17:31:54.882001 2050462464 net.cpp:167] fc6 needs backward computation.
I0502 17:31:54.882025 2050462464 net.cpp:167] pool5 needs backward computation.
I0502 17:31:54.882033 2050462464 net.cpp:167] relu5 needs backward computation.
I0502 17:31:54.882041 2050462464 net.cpp:167] conv5 needs backward computation.
I0502 17:31:54.882047 2050462464 net.cpp:167] relu4 needs backward computation.
I0502 17:31:54.882055 2050462464 net.cpp:167] conv4 needs backward computation.
I0502 17:31:54.882060 2050462464 net.cpp:167] relu3 needs backward computation.
I0502 17:31:54.882066 2050462464 net.cpp:167] conv3 needs backward computation.
I0502 17:31:54.882072 2050462464 net.cpp:167] norm2 needs backward computation.
I0502 17:31:54.882078 2050462464 net.cpp:167] pool2 needs backward computation.
I0502 17:31:54.882086 2050462464 net.cpp:167] relu2 needs backward computation.
I0502 17:31:54.882092 2050462464 net.cpp:167] conv2 needs backward computation.
I0502 17:31:54.882100 2050462464 net.cpp:167] norm1 needs backward computation.
I0502 17:31:54.882107 2050462464 net.cpp:167] pool1 needs backward computation.
I0502 17:31:54.882113 2050462464 net.cpp:167] relu1 needs backward computation.
I0502 17:31:54.882120 2050462464 net.cpp:167] conv1 needs backward computation.
I0502 17:31:54.882124 2050462464 net.cpp:169] data does not need backward computation.
I0502 17:31:54.882128 2050462464 net.cpp:205] This network produces output loss
I0502 17:31:54.882144 2050462464 net.cpp:447] Collecting Learning Rate and Weight Decay.
I0502 17:31:54.882153 2050462464 net.cpp:217] Network initialization done.
I0502 17:31:54.882158 2050462464 net.cpp:218] Memory required for data: 439050756
I0502 17:31:54.882647 2050462464 solver.cpp:154] Creating test net (#0) specified by net file: train_val_background.prototxt
I0502 17:31:54.882711 2050462464 net.cpp:257] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0502 17:31:54.882745 2050462464 net.cpp:42] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "ImageData"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  image_data_param {
    source: "../test_list.txt"
    batch_size: 64
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8_VOC"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8_VOC"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  param {
    lr_mult: 20
    decay_mult: 0
  }
  inner_product_param {
    num_output: 6
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_VOC"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_VOC"
  bottom: "label"
  top: "loss"
}
I0502 17:31:54.883456 2050462464 layer_factory.hpp:74] Creating layer data
I0502 17:31:54.883479 2050462464 net.cpp:84] Creating Layer data
I0502 17:31:54.883491 2050462464 net.cpp:338] data -> data
I0502 17:31:54.883510 2050462464 net.cpp:338] data -> label
I0502 17:31:54.883524 2050462464 net.cpp:113] Setting up data
I0502 17:31:54.883533 2050462464 image_data_layer.cpp:36] Opening file ../test_list.txt
I0502 17:31:54.889894 2050462464 image_data_layer.cpp:51] A total of 3703 images.
I0502 17:31:54.892276 2050462464 image_data_layer.cpp:80] output data size: 64,3,227,227
I0502 17:31:54.913990 2050462464 net.cpp:120] Top shape: 64 3 227 227 (9893568)
I0502 17:31:54.914027 2050462464 net.cpp:120] Top shape: 64 (64)
I0502 17:31:54.914039 2050462464 layer_factory.hpp:74] Creating layer label_data_1_split
I0502 17:31:54.914142 2050462464 net.cpp:84] Creating Layer label_data_1_split
I0502 17:31:54.914157 2050462464 net.cpp:380] label_data_1_split <- label
I0502 17:31:54.914165 2050462464 net.cpp:338] label_data_1_split -> label_data_1_split_0
I0502 17:31:54.914175 2050462464 net.cpp:338] label_data_1_split -> label_data_1_split_1
I0502 17:31:54.914182 2050462464 net.cpp:113] Setting up label_data_1_split
I0502 17:31:54.914188 2050462464 net.cpp:120] Top shape: 64 (64)
I0502 17:31:54.914194 2050462464 net.cpp:120] Top shape: 64 (64)
I0502 17:31:54.914232 2050462464 layer_factory.hpp:74] Creating layer conv1
I0502 17:31:54.914258 2050462464 net.cpp:84] Creating Layer conv1
I0502 17:31:54.914268 2050462464 net.cpp:380] conv1 <- data
I0502 17:31:54.914281 2050462464 net.cpp:338] conv1 -> conv1
I0502 17:31:54.914291 2050462464 net.cpp:113] Setting up conv1
I0502 17:31:54.915078 2050462464 net.cpp:120] Top shape: 64 96 55 55 (18585600)
I0502 17:31:54.915191 2050462464 layer_factory.hpp:74] Creating layer relu1
I0502 17:31:54.915210 2050462464 net.cpp:84] Creating Layer relu1
I0502 17:31:54.915215 2050462464 net.cpp:380] relu1 <- conv1
I0502 17:31:54.915221 2050462464 net.cpp:327] relu1 -> conv1 (in-place)
I0502 17:31:54.915228 2050462464 net.cpp:113] Setting up relu1
I0502 17:31:54.915232 2050462464 net.cpp:120] Top shape: 64 96 55 55 (18585600)
I0502 17:31:54.915240 2050462464 layer_factory.hpp:74] Creating layer pool1
I0502 17:31:54.915251 2050462464 net.cpp:84] Creating Layer pool1
I0502 17:31:54.915256 2050462464 net.cpp:380] pool1 <- conv1
I0502 17:31:54.915262 2050462464 net.cpp:338] pool1 -> pool1
I0502 17:31:54.915269 2050462464 net.cpp:113] Setting up pool1
I0502 17:31:54.915277 2050462464 net.cpp:120] Top shape: 64 96 27 27 (4478976)
I0502 17:31:54.915282 2050462464 layer_factory.hpp:74] Creating layer norm1
I0502 17:31:54.915289 2050462464 net.cpp:84] Creating Layer norm1
I0502 17:31:54.915293 2050462464 net.cpp:380] norm1 <- pool1
I0502 17:31:54.915298 2050462464 net.cpp:338] norm1 -> norm1
I0502 17:31:54.915305 2050462464 net.cpp:113] Setting up norm1
I0502 17:31:54.915310 2050462464 net.cpp:120] Top shape: 64 96 27 27 (4478976)
I0502 17:31:54.915316 2050462464 layer_factory.hpp:74] Creating layer conv2
I0502 17:31:54.915323 2050462464 net.cpp:84] Creating Layer conv2
I0502 17:31:54.915330 2050462464 net.cpp:380] conv2 <- norm1
I0502 17:31:54.915369 2050462464 net.cpp:338] conv2 -> conv2
I0502 17:31:54.915392 2050462464 net.cpp:113] Setting up conv2
I0502 17:31:54.922431 2050462464 net.cpp:120] Top shape: 64 256 27 27 (11943936)
I0502 17:31:54.922464 2050462464 layer_factory.hpp:74] Creating layer relu2
I0502 17:31:54.922474 2050462464 net.cpp:84] Creating Layer relu2
I0502 17:31:54.922480 2050462464 net.cpp:380] relu2 <- conv2
I0502 17:31:54.922487 2050462464 net.cpp:327] relu2 -> conv2 (in-place)
I0502 17:31:54.922494 2050462464 net.cpp:113] Setting up relu2
I0502 17:31:54.922509 2050462464 net.cpp:120] Top shape: 64 256 27 27 (11943936)
I0502 17:31:54.922529 2050462464 layer_factory.hpp:74] Creating layer pool2
I0502 17:31:54.922545 2050462464 net.cpp:84] Creating Layer pool2
I0502 17:31:54.922551 2050462464 net.cpp:380] pool2 <- conv2
I0502 17:31:54.922559 2050462464 net.cpp:338] pool2 -> pool2
I0502 17:31:54.922566 2050462464 net.cpp:113] Setting up pool2
I0502 17:31:54.922574 2050462464 net.cpp:120] Top shape: 64 256 13 13 (2768896)
I0502 17:31:54.922580 2050462464 layer_factory.hpp:74] Creating layer norm2
I0502 17:31:54.922587 2050462464 net.cpp:84] Creating Layer norm2
I0502 17:31:54.922592 2050462464 net.cpp:380] norm2 <- pool2
I0502 17:31:54.922598 2050462464 net.cpp:338] norm2 -> norm2
I0502 17:31:54.922605 2050462464 net.cpp:113] Setting up norm2
I0502 17:31:54.922612 2050462464 net.cpp:120] Top shape: 64 256 13 13 (2768896)
I0502 17:31:54.922622 2050462464 layer_factory.hpp:74] Creating layer conv3
I0502 17:31:54.922636 2050462464 net.cpp:84] Creating Layer conv3
I0502 17:31:54.922641 2050462464 net.cpp:380] conv3 <- norm2
I0502 17:31:54.922652 2050462464 net.cpp:338] conv3 -> conv3
I0502 17:31:54.922660 2050462464 net.cpp:113] Setting up conv3
I0502 17:31:54.939443 2050462464 net.cpp:120] Top shape: 64 384 13 13 (4153344)
I0502 17:31:54.939471 2050462464 layer_factory.hpp:74] Creating layer relu3
I0502 17:31:54.939486 2050462464 net.cpp:84] Creating Layer relu3
I0502 17:31:54.939491 2050462464 net.cpp:380] relu3 <- conv3
I0502 17:31:54.939497 2050462464 net.cpp:327] relu3 -> conv3 (in-place)
I0502 17:31:54.939504 2050462464 net.cpp:113] Setting up relu3
I0502 17:31:54.939509 2050462464 net.cpp:120] Top shape: 64 384 13 13 (4153344)
I0502 17:31:54.939515 2050462464 layer_factory.hpp:74] Creating layer conv4
I0502 17:31:54.939546 2050462464 net.cpp:84] Creating Layer conv4
I0502 17:31:54.939561 2050462464 net.cpp:380] conv4 <- conv3
I0502 17:31:54.939575 2050462464 net.cpp:338] conv4 -> conv4
I0502 17:31:54.939610 2050462464 net.cpp:113] Setting up conv4
I0502 17:31:54.951156 2050462464 net.cpp:120] Top shape: 64 384 13 13 (4153344)
I0502 17:31:54.951218 2050462464 layer_factory.hpp:74] Creating layer relu4
I0502 17:31:54.951242 2050462464 net.cpp:84] Creating Layer relu4
I0502 17:31:54.951252 2050462464 net.cpp:380] relu4 <- conv4
I0502 17:31:54.951264 2050462464 net.cpp:327] relu4 -> conv4 (in-place)
I0502 17:31:54.951277 2050462464 net.cpp:113] Setting up relu4
I0502 17:31:54.951287 2050462464 net.cpp:120] Top shape: 64 384 13 13 (4153344)
I0502 17:31:54.951297 2050462464 layer_factory.hpp:74] Creating layer conv5
I0502 17:31:54.951311 2050462464 net.cpp:84] Creating Layer conv5
I0502 17:31:54.951319 2050462464 net.cpp:380] conv5 <- conv4
I0502 17:31:54.951361 2050462464 net.cpp:338] conv5 -> conv5
I0502 17:31:54.951380 2050462464 net.cpp:113] Setting up conv5
I0502 17:31:54.960311 2050462464 net.cpp:120] Top shape: 64 256 13 13 (2768896)
I0502 17:31:54.960347 2050462464 layer_factory.hpp:74] Creating layer relu5
I0502 17:31:54.960358 2050462464 net.cpp:84] Creating Layer relu5
I0502 17:31:54.960364 2050462464 net.cpp:380] relu5 <- conv5
I0502 17:31:54.960371 2050462464 net.cpp:327] relu5 -> conv5 (in-place)
I0502 17:31:54.960378 2050462464 net.cpp:113] Setting up relu5
I0502 17:31:54.960383 2050462464 net.cpp:120] Top shape: 64 256 13 13 (2768896)
I0502 17:31:54.960389 2050462464 layer_factory.hpp:74] Creating layer pool5
I0502 17:31:54.960433 2050462464 net.cpp:84] Creating Layer pool5
I0502 17:31:54.960441 2050462464 net.cpp:380] pool5 <- conv5
I0502 17:31:54.960451 2050462464 net.cpp:338] pool5 -> pool5
I0502 17:31:54.960463 2050462464 net.cpp:113] Setting up pool5
I0502 17:31:54.960470 2050462464 net.cpp:120] Top shape: 64 256 6 6 (589824)
I0502 17:31:54.960476 2050462464 layer_factory.hpp:74] Creating layer fc6
I0502 17:31:54.960484 2050462464 net.cpp:84] Creating Layer fc6
I0502 17:31:54.960489 2050462464 net.cpp:380] fc6 <- pool5
I0502 17:31:54.960494 2050462464 net.cpp:338] fc6 -> fc6
I0502 17:31:54.960501 2050462464 net.cpp:113] Setting up fc6
^C
C02MX066FD58:rcc_net cusgadmin$ caffe train -solver solver_background.prototxt -weights bvlc_reference_rcnn_ilsvrc13.caffemodel 
I0502 17:32:01.439545 2050462464 caffe.cpp:117] Use CPU.
I0502 17:32:01.440577 2050462464 caffe.cpp:121] Starting Optimization
I0502 17:32:01.440598 2050462464 solver.cpp:32] Initializing solver from parameters: 
test_iter: 1000
test_interval: 1000
base_lr: 0.001
display: 20
max_iter: 10000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 100000
snapshot: 500
snapshot_prefix: "caffenet_train_background"
solver_mode: CPU
net: "train_val_background.prototxt"
I0502 17:32:01.440687 2050462464 solver.cpp:70] Creating training net from net file: train_val_background.prototxt
I0502 17:32:01.441300 2050462464 net.cpp:257] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0502 17:32:01.441346 2050462464 net.cpp:257] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0502 17:32:01.441364 2050462464 net.cpp:42] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "ImageData"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  image_data_param {
    source: "../train_list.txt"
    batch_size: 64
    shuffle: true
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8_VOC"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8_VOC"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  param {
    lr_mult: 20
    decay_mult: 0
  }
  inner_product_param {
    num_output: 6
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_VOC"
  bottom: "label"
  top: "loss"
}
I0502 17:32:01.442083 2050462464 layer_factory.hpp:74] Creating layer data
I0502 17:32:01.442117 2050462464 net.cpp:84] Creating Layer data
I0502 17:32:01.442131 2050462464 net.cpp:338] data -> data
I0502 17:32:01.442163 2050462464 net.cpp:338] data -> label
I0502 17:32:01.442176 2050462464 net.cpp:113] Setting up data
I0502 17:32:01.442205 2050462464 image_data_layer.cpp:36] Opening file ../train_list.txt
I0502 17:32:01.491827 2050462464 image_data_layer.cpp:46] Shuffling data
I0502 17:32:01.501873 2050462464 image_data_layer.cpp:51] A total of 32601 images.
I0502 17:32:01.504477 2050462464 image_data_layer.cpp:80] output data size: 64,3,227,227
I0502 17:32:01.529039 2050462464 net.cpp:120] Top shape: 64 3 227 227 (9893568)
I0502 17:32:01.529101 2050462464 net.cpp:120] Top shape: 64 (64)
I0502 17:32:01.529115 2050462464 layer_factory.hpp:74] Creating layer conv1
I0502 17:32:01.529136 2050462464 net.cpp:84] Creating Layer conv1
I0502 17:32:01.529145 2050462464 net.cpp:380] conv1 <- data
I0502 17:32:01.529157 2050462464 net.cpp:338] conv1 -> conv1
I0502 17:32:01.529172 2050462464 net.cpp:113] Setting up conv1
I0502 17:32:01.530020 2050462464 net.cpp:120] Top shape: 64 96 55 55 (18585600)
I0502 17:32:01.530040 2050462464 layer_factory.hpp:74] Creating layer relu1
I0502 17:32:01.530053 2050462464 net.cpp:84] Creating Layer relu1
I0502 17:32:01.530061 2050462464 net.cpp:380] relu1 <- conv1
I0502 17:32:01.530068 2050462464 net.cpp:327] relu1 -> conv1 (in-place)
I0502 17:32:01.530077 2050462464 net.cpp:113] Setting up relu1
I0502 17:32:01.530084 2050462464 net.cpp:120] Top shape: 64 96 55 55 (18585600)
I0502 17:32:01.530092 2050462464 layer_factory.hpp:74] Creating layer pool1
I0502 17:32:01.530102 2050462464 net.cpp:84] Creating Layer pool1
I0502 17:32:01.530107 2050462464 net.cpp:380] pool1 <- conv1
I0502 17:32:01.530115 2050462464 net.cpp:338] pool1 -> pool1
I0502 17:32:01.530128 2050462464 net.cpp:113] Setting up pool1
I0502 17:32:01.530151 2050462464 net.cpp:120] Top shape: 64 96 27 27 (4478976)
I0502 17:32:01.530160 2050462464 layer_factory.hpp:74] Creating layer norm1
I0502 17:32:01.530174 2050462464 net.cpp:84] Creating Layer norm1
I0502 17:32:01.530180 2050462464 net.cpp:380] norm1 <- pool1
I0502 17:32:01.530189 2050462464 net.cpp:338] norm1 -> norm1
I0502 17:32:01.530227 2050462464 net.cpp:113] Setting up norm1
I0502 17:32:01.530251 2050462464 net.cpp:120] Top shape: 64 96 27 27 (4478976)
I0502 17:32:01.530310 2050462464 layer_factory.hpp:74] Creating layer conv2
I0502 17:32:01.530326 2050462464 net.cpp:84] Creating Layer conv2
I0502 17:32:01.530334 2050462464 net.cpp:380] conv2 <- norm1
I0502 17:32:01.530346 2050462464 net.cpp:338] conv2 -> conv2
I0502 17:32:01.530359 2050462464 net.cpp:113] Setting up conv2
I0502 17:32:01.537725 2050462464 net.cpp:120] Top shape: 64 256 27 27 (11943936)
I0502 17:32:01.537762 2050462464 layer_factory.hpp:74] Creating layer relu2
I0502 17:32:01.537778 2050462464 net.cpp:84] Creating Layer relu2
I0502 17:32:01.537789 2050462464 net.cpp:380] relu2 <- conv2
I0502 17:32:01.537807 2050462464 net.cpp:327] relu2 -> conv2 (in-place)
I0502 17:32:01.537829 2050462464 net.cpp:113] Setting up relu2
I0502 17:32:01.537840 2050462464 net.cpp:120] Top shape: 64 256 27 27 (11943936)
I0502 17:32:01.537853 2050462464 layer_factory.hpp:74] Creating layer pool2
I0502 17:32:01.537864 2050462464 net.cpp:84] Creating Layer pool2
I0502 17:32:01.537869 2050462464 net.cpp:380] pool2 <- conv2
I0502 17:32:01.537875 2050462464 net.cpp:338] pool2 -> pool2
I0502 17:32:01.537885 2050462464 net.cpp:113] Setting up pool2
I0502 17:32:01.537899 2050462464 net.cpp:120] Top shape: 64 256 13 13 (2768896)
I0502 17:32:01.537909 2050462464 layer_factory.hpp:74] Creating layer norm2
I0502 17:32:01.537925 2050462464 net.cpp:84] Creating Layer norm2
I0502 17:32:01.537955 2050462464 net.cpp:380] norm2 <- pool2
I0502 17:32:01.537971 2050462464 net.cpp:338] norm2 -> norm2
I0502 17:32:01.537984 2050462464 net.cpp:113] Setting up norm2
I0502 17:32:01.537997 2050462464 net.cpp:120] Top shape: 64 256 13 13 (2768896)
I0502 17:32:01.538008 2050462464 layer_factory.hpp:74] Creating layer conv3
I0502 17:32:01.538022 2050462464 net.cpp:84] Creating Layer conv3
I0502 17:32:01.538031 2050462464 net.cpp:380] conv3 <- norm2
I0502 17:32:01.538044 2050462464 net.cpp:338] conv3 -> conv3
I0502 17:32:01.538059 2050462464 net.cpp:113] Setting up conv3
I0502 17:32:01.559244 2050462464 net.cpp:120] Top shape: 64 384 13 13 (4153344)
I0502 17:32:01.559283 2050462464 layer_factory.hpp:74] Creating layer relu3
I0502 17:32:01.559298 2050462464 net.cpp:84] Creating Layer relu3
I0502 17:32:01.559309 2050462464 net.cpp:380] relu3 <- conv3
I0502 17:32:01.559319 2050462464 net.cpp:327] relu3 -> conv3 (in-place)
I0502 17:32:01.559331 2050462464 net.cpp:113] Setting up relu3
I0502 17:32:01.559340 2050462464 net.cpp:120] Top shape: 64 384 13 13 (4153344)
I0502 17:32:01.559350 2050462464 layer_factory.hpp:74] Creating layer conv4
I0502 17:32:01.559363 2050462464 net.cpp:84] Creating Layer conv4
I0502 17:32:01.559372 2050462464 net.cpp:380] conv4 <- conv3
I0502 17:32:01.559383 2050462464 net.cpp:338] conv4 -> conv4
I0502 17:32:01.559397 2050462464 net.cpp:113] Setting up conv4
I0502 17:32:01.574337 2050462464 net.cpp:120] Top shape: 64 384 13 13 (4153344)
I0502 17:32:01.574367 2050462464 layer_factory.hpp:74] Creating layer relu4
I0502 17:32:01.574379 2050462464 net.cpp:84] Creating Layer relu4
I0502 17:32:01.574388 2050462464 net.cpp:380] relu4 <- conv4
I0502 17:32:01.574396 2050462464 net.cpp:327] relu4 -> conv4 (in-place)
I0502 17:32:01.574406 2050462464 net.cpp:113] Setting up relu4
I0502 17:32:01.574414 2050462464 net.cpp:120] Top shape: 64 384 13 13 (4153344)
I0502 17:32:01.574446 2050462464 layer_factory.hpp:74] Creating layer conv5
I0502 17:32:01.574465 2050462464 net.cpp:84] Creating Layer conv5
I0502 17:32:01.574476 2050462464 net.cpp:380] conv5 <- conv4
I0502 17:32:01.574489 2050462464 net.cpp:338] conv5 -> conv5
I0502 17:32:01.574504 2050462464 net.cpp:113] Setting up conv5
I0502 17:32:01.584599 2050462464 net.cpp:120] Top shape: 64 256 13 13 (2768896)
I0502 17:32:01.584696 2050462464 layer_factory.hpp:74] Creating layer relu5
I0502 17:32:01.584712 2050462464 net.cpp:84] Creating Layer relu5
I0502 17:32:01.584719 2050462464 net.cpp:380] relu5 <- conv5
I0502 17:32:01.584727 2050462464 net.cpp:327] relu5 -> conv5 (in-place)
I0502 17:32:01.584734 2050462464 net.cpp:113] Setting up relu5
I0502 17:32:01.584739 2050462464 net.cpp:120] Top shape: 64 256 13 13 (2768896)
I0502 17:32:01.584780 2050462464 layer_factory.hpp:74] Creating layer pool5
I0502 17:32:01.584795 2050462464 net.cpp:84] Creating Layer pool5
I0502 17:32:01.584805 2050462464 net.cpp:380] pool5 <- conv5
I0502 17:32:01.584815 2050462464 net.cpp:338] pool5 -> pool5
I0502 17:32:01.584835 2050462464 net.cpp:113] Setting up pool5
I0502 17:32:01.584848 2050462464 net.cpp:120] Top shape: 64 256 6 6 (589824)
I0502 17:32:01.584861 2050462464 layer_factory.hpp:74] Creating layer fc6
I0502 17:32:01.584898 2050462464 net.cpp:84] Creating Layer fc6
I0502 17:32:01.584911 2050462464 net.cpp:380] fc6 <- pool5
I0502 17:32:01.584939 2050462464 net.cpp:338] fc6 -> fc6
I0502 17:32:01.584964 2050462464 net.cpp:113] Setting up fc6
I0502 17:32:02.341114 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0502 17:32:02.341153 2050462464 layer_factory.hpp:74] Creating layer relu6
I0502 17:32:02.341171 2050462464 net.cpp:84] Creating Layer relu6
I0502 17:32:02.341181 2050462464 net.cpp:380] relu6 <- fc6
I0502 17:32:02.341193 2050462464 net.cpp:327] relu6 -> fc6 (in-place)
I0502 17:32:02.341205 2050462464 net.cpp:113] Setting up relu6
I0502 17:32:02.341215 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0502 17:32:02.341225 2050462464 layer_factory.hpp:74] Creating layer drop6
I0502 17:32:02.341236 2050462464 net.cpp:84] Creating Layer drop6
I0502 17:32:02.341244 2050462464 net.cpp:380] drop6 <- fc6
I0502 17:32:02.341253 2050462464 net.cpp:327] drop6 -> fc6 (in-place)
I0502 17:32:02.341264 2050462464 net.cpp:113] Setting up drop6
I0502 17:32:02.341281 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0502 17:32:02.341291 2050462464 layer_factory.hpp:74] Creating layer fc7
I0502 17:32:02.341305 2050462464 net.cpp:84] Creating Layer fc7
I0502 17:32:02.341311 2050462464 net.cpp:380] fc7 <- fc6
I0502 17:32:02.341331 2050462464 net.cpp:338] fc7 -> fc7
I0502 17:32:02.341346 2050462464 net.cpp:113] Setting up fc7
I0502 17:32:02.679607 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0502 17:32:02.679651 2050462464 layer_factory.hpp:74] Creating layer relu7
I0502 17:32:02.679679 2050462464 net.cpp:84] Creating Layer relu7
I0502 17:32:02.679694 2050462464 net.cpp:380] relu7 <- fc7
I0502 17:32:02.679707 2050462464 net.cpp:327] relu7 -> fc7 (in-place)
I0502 17:32:02.679718 2050462464 net.cpp:113] Setting up relu7
I0502 17:32:02.679728 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0502 17:32:02.679738 2050462464 layer_factory.hpp:74] Creating layer drop7
I0502 17:32:02.679774 2050462464 net.cpp:84] Creating Layer drop7
I0502 17:32:02.679793 2050462464 net.cpp:380] drop7 <- fc7
I0502 17:32:02.679807 2050462464 net.cpp:327] drop7 -> fc7 (in-place)
I0502 17:32:02.679821 2050462464 net.cpp:113] Setting up drop7
I0502 17:32:02.679847 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0502 17:32:02.679860 2050462464 layer_factory.hpp:74] Creating layer fc8_VOC
I0502 17:32:02.679873 2050462464 net.cpp:84] Creating Layer fc8_VOC
I0502 17:32:02.679878 2050462464 net.cpp:380] fc8_VOC <- fc7
I0502 17:32:02.679886 2050462464 net.cpp:338] fc8_VOC -> fc8_VOC
I0502 17:32:02.679895 2050462464 net.cpp:113] Setting up fc8_VOC
I0502 17:32:02.680483 2050462464 net.cpp:120] Top shape: 64 6 (384)
I0502 17:32:02.680507 2050462464 layer_factory.hpp:74] Creating layer loss
I0502 17:32:02.680527 2050462464 net.cpp:84] Creating Layer loss
I0502 17:32:02.680538 2050462464 net.cpp:380] loss <- fc8_VOC
I0502 17:32:02.680548 2050462464 net.cpp:380] loss <- label
I0502 17:32:02.680559 2050462464 net.cpp:338] loss -> loss
I0502 17:32:02.680572 2050462464 net.cpp:113] Setting up loss
I0502 17:32:02.680583 2050462464 layer_factory.hpp:74] Creating layer loss
I0502 17:32:02.680605 2050462464 net.cpp:120] Top shape: (1)
I0502 17:32:02.680616 2050462464 net.cpp:122]     with loss weight 1
I0502 17:32:02.680661 2050462464 net.cpp:167] loss needs backward computation.
I0502 17:32:02.680675 2050462464 net.cpp:167] fc8_VOC needs backward computation.
I0502 17:32:02.680682 2050462464 net.cpp:167] drop7 needs backward computation.
I0502 17:32:02.680716 2050462464 net.cpp:167] relu7 needs backward computation.
I0502 17:32:02.680726 2050462464 net.cpp:167] fc7 needs backward computation.
I0502 17:32:02.680733 2050462464 net.cpp:167] drop6 needs backward computation.
I0502 17:32:02.680740 2050462464 net.cpp:167] relu6 needs backward computation.
I0502 17:32:02.680747 2050462464 net.cpp:167] fc6 needs backward computation.
I0502 17:32:02.680755 2050462464 net.cpp:167] pool5 needs backward computation.
I0502 17:32:02.680763 2050462464 net.cpp:167] relu5 needs backward computation.
I0502 17:32:02.680794 2050462464 net.cpp:167] conv5 needs backward computation.
I0502 17:32:02.680804 2050462464 net.cpp:167] relu4 needs backward computation.
I0502 17:32:02.680812 2050462464 net.cpp:167] conv4 needs backward computation.
I0502 17:32:02.680819 2050462464 net.cpp:167] relu3 needs backward computation.
I0502 17:32:02.680826 2050462464 net.cpp:167] conv3 needs backward computation.
I0502 17:32:02.680835 2050462464 net.cpp:167] norm2 needs backward computation.
I0502 17:32:02.680841 2050462464 net.cpp:167] pool2 needs backward computation.
I0502 17:32:02.680848 2050462464 net.cpp:167] relu2 needs backward computation.
I0502 17:32:02.680855 2050462464 net.cpp:167] conv2 needs backward computation.
I0502 17:32:02.680861 2050462464 net.cpp:167] norm1 needs backward computation.
I0502 17:32:02.680868 2050462464 net.cpp:167] pool1 needs backward computation.
I0502 17:32:02.680887 2050462464 net.cpp:167] relu1 needs backward computation.
I0502 17:32:02.680927 2050462464 net.cpp:167] conv1 needs backward computation.
I0502 17:32:02.680938 2050462464 net.cpp:169] data does not need backward computation.
I0502 17:32:02.680946 2050462464 net.cpp:205] This network produces output loss
I0502 17:32:02.680968 2050462464 net.cpp:447] Collecting Learning Rate and Weight Decay.
I0502 17:32:02.680981 2050462464 net.cpp:217] Network initialization done.
I0502 17:32:02.680989 2050462464 net.cpp:218] Memory required for data: 439050756
I0502 17:32:02.681623 2050462464 solver.cpp:154] Creating test net (#0) specified by net file: train_val_background.prototxt
I0502 17:32:02.681689 2050462464 net.cpp:257] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0502 17:32:02.681720 2050462464 net.cpp:42] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "ImageData"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  image_data_param {
    source: "../test_list.txt"
    batch_size: 64
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8_VOC"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8_VOC"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  param {
    lr_mult: 20
    decay_mult: 0
  }
  inner_product_param {
    num_output: 6
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_VOC"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_VOC"
  bottom: "label"
  top: "loss"
}
I0502 17:32:02.682256 2050462464 layer_factory.hpp:74] Creating layer data
I0502 17:32:02.682279 2050462464 net.cpp:84] Creating Layer data
I0502 17:32:02.682291 2050462464 net.cpp:338] data -> data
I0502 17:32:02.682307 2050462464 net.cpp:338] data -> label
I0502 17:32:02.682319 2050462464 net.cpp:113] Setting up data
I0502 17:32:02.682327 2050462464 image_data_layer.cpp:36] Opening file ../test_list.txt
I0502 17:32:02.689126 2050462464 image_data_layer.cpp:51] A total of 3703 images.
I0502 17:32:02.690974 2050462464 image_data_layer.cpp:80] output data size: 64,3,227,227
I0502 17:32:02.715842 2050462464 net.cpp:120] Top shape: 64 3 227 227 (9893568)
I0502 17:32:02.715878 2050462464 net.cpp:120] Top shape: 64 (64)
I0502 17:32:02.715909 2050462464 layer_factory.hpp:74] Creating layer label_data_1_split
I0502 17:32:02.715983 2050462464 net.cpp:84] Creating Layer label_data_1_split
I0502 17:32:02.716008 2050462464 net.cpp:380] label_data_1_split <- label
I0502 17:32:02.716029 2050462464 net.cpp:338] label_data_1_split -> label_data_1_split_0
I0502 17:32:02.716040 2050462464 net.cpp:338] label_data_1_split -> label_data_1_split_1
I0502 17:32:02.716050 2050462464 net.cpp:113] Setting up label_data_1_split
I0502 17:32:02.716059 2050462464 net.cpp:120] Top shape: 64 (64)
I0502 17:32:02.716065 2050462464 net.cpp:120] Top shape: 64 (64)
I0502 17:32:02.716073 2050462464 layer_factory.hpp:74] Creating layer conv1
I0502 17:32:02.716106 2050462464 net.cpp:84] Creating Layer conv1
I0502 17:32:02.716119 2050462464 net.cpp:380] conv1 <- data
I0502 17:32:02.716132 2050462464 net.cpp:338] conv1 -> conv1
I0502 17:32:02.716148 2050462464 net.cpp:113] Setting up conv1
I0502 17:32:02.716843 2050462464 net.cpp:120] Top shape: 64 96 55 55 (18585600)
I0502 17:32:02.716864 2050462464 layer_factory.hpp:74] Creating layer relu1
I0502 17:32:02.716877 2050462464 net.cpp:84] Creating Layer relu1
I0502 17:32:02.716886 2050462464 net.cpp:380] relu1 <- conv1
I0502 17:32:02.716897 2050462464 net.cpp:327] relu1 -> conv1 (in-place)
I0502 17:32:02.716909 2050462464 net.cpp:113] Setting up relu1
I0502 17:32:02.716918 2050462464 net.cpp:120] Top shape: 64 96 55 55 (18585600)
I0502 17:32:02.716954 2050462464 layer_factory.hpp:74] Creating layer pool1
I0502 17:32:02.716974 2050462464 net.cpp:84] Creating Layer pool1
I0502 17:32:02.716984 2050462464 net.cpp:380] pool1 <- conv1
I0502 17:32:02.717006 2050462464 net.cpp:338] pool1 -> pool1
I0502 17:32:02.717031 2050462464 net.cpp:113] Setting up pool1
I0502 17:32:02.717046 2050462464 net.cpp:120] Top shape: 64 96 27 27 (4478976)
I0502 17:32:02.717079 2050462464 layer_factory.hpp:74] Creating layer norm1
I0502 17:32:02.717097 2050462464 net.cpp:84] Creating Layer norm1
I0502 17:32:02.717104 2050462464 net.cpp:380] norm1 <- pool1
I0502 17:32:02.717114 2050462464 net.cpp:338] norm1 -> norm1
I0502 17:32:02.717185 2050462464 net.cpp:113] Setting up norm1
I0502 17:32:02.717202 2050462464 net.cpp:120] Top shape: 64 96 27 27 (4478976)
I0502 17:32:02.717214 2050462464 layer_factory.hpp:74] Creating layer conv2
I0502 17:32:02.717233 2050462464 net.cpp:84] Creating Layer conv2
I0502 17:32:02.717243 2050462464 net.cpp:380] conv2 <- norm1
I0502 17:32:02.717291 2050462464 net.cpp:338] conv2 -> conv2
I0502 17:32:02.717303 2050462464 net.cpp:113] Setting up conv2
I0502 17:32:02.722456 2050462464 net.cpp:120] Top shape: 64 256 27 27 (11943936)
I0502 17:32:02.722481 2050462464 layer_factory.hpp:74] Creating layer relu2
I0502 17:32:02.722491 2050462464 net.cpp:84] Creating Layer relu2
I0502 17:32:02.722496 2050462464 net.cpp:380] relu2 <- conv2
I0502 17:32:02.722503 2050462464 net.cpp:327] relu2 -> conv2 (in-place)
I0502 17:32:02.722511 2050462464 net.cpp:113] Setting up relu2
I0502 17:32:02.722515 2050462464 net.cpp:120] Top shape: 64 256 27 27 (11943936)
I0502 17:32:02.722522 2050462464 layer_factory.hpp:74] Creating layer pool2
I0502 17:32:02.722530 2050462464 net.cpp:84] Creating Layer pool2
I0502 17:32:02.722535 2050462464 net.cpp:380] pool2 <- conv2
I0502 17:32:02.722549 2050462464 net.cpp:338] pool2 -> pool2
I0502 17:32:02.722564 2050462464 net.cpp:113] Setting up pool2
I0502 17:32:02.722576 2050462464 net.cpp:120] Top shape: 64 256 13 13 (2768896)
I0502 17:32:02.722589 2050462464 layer_factory.hpp:74] Creating layer norm2
I0502 17:32:02.722599 2050462464 net.cpp:84] Creating Layer norm2
I0502 17:32:02.722602 2050462464 net.cpp:380] norm2 <- pool2
I0502 17:32:02.722630 2050462464 net.cpp:338] norm2 -> norm2
I0502 17:32:02.722652 2050462464 net.cpp:113] Setting up norm2
I0502 17:32:02.722666 2050462464 net.cpp:120] Top shape: 64 256 13 13 (2768896)
I0502 17:32:02.722678 2050462464 layer_factory.hpp:74] Creating layer conv3
I0502 17:32:02.722708 2050462464 net.cpp:84] Creating Layer conv3
I0502 17:32:02.722723 2050462464 net.cpp:380] conv3 <- norm2
I0502 17:32:02.722736 2050462464 net.cpp:338] conv3 -> conv3
I0502 17:32:02.722750 2050462464 net.cpp:113] Setting up conv3
I0502 17:32:02.739634 2050462464 net.cpp:120] Top shape: 64 384 13 13 (4153344)
I0502 17:32:02.739668 2050462464 layer_factory.hpp:74] Creating layer relu3
I0502 17:32:02.739707 2050462464 net.cpp:84] Creating Layer relu3
I0502 17:32:02.739728 2050462464 net.cpp:380] relu3 <- conv3
I0502 17:32:02.739765 2050462464 net.cpp:327] relu3 -> conv3 (in-place)
I0502 17:32:02.739789 2050462464 net.cpp:113] Setting up relu3
I0502 17:32:02.739799 2050462464 net.cpp:120] Top shape: 64 384 13 13 (4153344)
I0502 17:32:02.739809 2050462464 layer_factory.hpp:74] Creating layer conv4
I0502 17:32:02.739823 2050462464 net.cpp:84] Creating Layer conv4
I0502 17:32:02.739832 2050462464 net.cpp:380] conv4 <- conv3
I0502 17:32:02.739845 2050462464 net.cpp:338] conv4 -> conv4
I0502 17:32:02.739857 2050462464 net.cpp:113] Setting up conv4
I0502 17:32:02.754740 2050462464 net.cpp:120] Top shape: 64 384 13 13 (4153344)
I0502 17:32:02.754815 2050462464 layer_factory.hpp:74] Creating layer relu4
I0502 17:32:02.754837 2050462464 net.cpp:84] Creating Layer relu4
I0502 17:32:02.754848 2050462464 net.cpp:380] relu4 <- conv4
I0502 17:32:02.754860 2050462464 net.cpp:327] relu4 -> conv4 (in-place)
I0502 17:32:02.754873 2050462464 net.cpp:113] Setting up relu4
I0502 17:32:02.754881 2050462464 net.cpp:120] Top shape: 64 384 13 13 (4153344)
I0502 17:32:02.754899 2050462464 layer_factory.hpp:74] Creating layer conv5
I0502 17:32:02.754926 2050462464 net.cpp:84] Creating Layer conv5
I0502 17:32:02.754945 2050462464 net.cpp:380] conv5 <- conv4
I0502 17:32:02.754959 2050462464 net.cpp:338] conv5 -> conv5
I0502 17:32:02.754974 2050462464 net.cpp:113] Setting up conv5
I0502 17:32:02.765041 2050462464 net.cpp:120] Top shape: 64 256 13 13 (2768896)
I0502 17:32:02.765069 2050462464 layer_factory.hpp:74] Creating layer relu5
I0502 17:32:02.765082 2050462464 net.cpp:84] Creating Layer relu5
I0502 17:32:02.765091 2050462464 net.cpp:380] relu5 <- conv5
I0502 17:32:02.765103 2050462464 net.cpp:327] relu5 -> conv5 (in-place)
I0502 17:32:02.765115 2050462464 net.cpp:113] Setting up relu5
I0502 17:32:02.765123 2050462464 net.cpp:120] Top shape: 64 256 13 13 (2768896)
I0502 17:32:02.765135 2050462464 layer_factory.hpp:74] Creating layer pool5
I0502 17:32:02.765153 2050462464 net.cpp:84] Creating Layer pool5
I0502 17:32:02.765163 2050462464 net.cpp:380] pool5 <- conv5
I0502 17:32:02.765174 2050462464 net.cpp:338] pool5 -> pool5
I0502 17:32:02.765187 2050462464 net.cpp:113] Setting up pool5
I0502 17:32:02.765233 2050462464 net.cpp:120] Top shape: 64 256 6 6 (589824)
I0502 17:32:02.765246 2050462464 layer_factory.hpp:74] Creating layer fc6
I0502 17:32:02.765259 2050462464 net.cpp:84] Creating Layer fc6
I0502 17:32:02.765269 2050462464 net.cpp:380] fc6 <- pool5
I0502 17:32:02.765280 2050462464 net.cpp:338] fc6 -> fc6
I0502 17:32:02.765293 2050462464 net.cpp:113] Setting up fc6
I0502 17:32:03.518331 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0502 17:32:03.518409 2050462464 layer_factory.hpp:74] Creating layer relu6
I0502 17:32:03.518429 2050462464 net.cpp:84] Creating Layer relu6
I0502 17:32:03.518438 2050462464 net.cpp:380] relu6 <- fc6
I0502 17:32:03.518450 2050462464 net.cpp:327] relu6 -> fc6 (in-place)
I0502 17:32:03.518463 2050462464 net.cpp:113] Setting up relu6
I0502 17:32:03.518472 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0502 17:32:03.518514 2050462464 layer_factory.hpp:74] Creating layer drop6
I0502 17:32:03.518537 2050462464 net.cpp:84] Creating Layer drop6
I0502 17:32:03.518548 2050462464 net.cpp:380] drop6 <- fc6
I0502 17:32:03.518559 2050462464 net.cpp:327] drop6 -> fc6 (in-place)
I0502 17:32:03.518573 2050462464 net.cpp:113] Setting up drop6
I0502 17:32:03.518582 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0502 17:32:03.518592 2050462464 layer_factory.hpp:74] Creating layer fc7
I0502 17:32:03.518614 2050462464 net.cpp:84] Creating Layer fc7
I0502 17:32:03.518642 2050462464 net.cpp:380] fc7 <- fc6
I0502 17:32:03.518656 2050462464 net.cpp:338] fc7 -> fc7
I0502 17:32:03.518669 2050462464 net.cpp:113] Setting up fc7
I0502 17:32:03.865339 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0502 17:32:03.865386 2050462464 layer_factory.hpp:74] Creating layer relu7
I0502 17:32:03.865407 2050462464 net.cpp:84] Creating Layer relu7
I0502 17:32:03.865417 2050462464 net.cpp:380] relu7 <- fc7
I0502 17:32:03.865428 2050462464 net.cpp:327] relu7 -> fc7 (in-place)
I0502 17:32:03.865440 2050462464 net.cpp:113] Setting up relu7
I0502 17:32:03.865449 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0502 17:32:03.865459 2050462464 layer_factory.hpp:74] Creating layer drop7
I0502 17:32:03.865470 2050462464 net.cpp:84] Creating Layer drop7
I0502 17:32:03.865478 2050462464 net.cpp:380] drop7 <- fc7
I0502 17:32:03.865488 2050462464 net.cpp:327] drop7 -> fc7 (in-place)
I0502 17:32:03.865499 2050462464 net.cpp:113] Setting up drop7
I0502 17:32:03.865509 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0502 17:32:03.865520 2050462464 layer_factory.hpp:74] Creating layer fc8_VOC
I0502 17:32:03.865535 2050462464 net.cpp:84] Creating Layer fc8_VOC
I0502 17:32:03.865543 2050462464 net.cpp:380] fc8_VOC <- fc7
I0502 17:32:03.865556 2050462464 net.cpp:338] fc8_VOC -> fc8_VOC
I0502 17:32:03.865595 2050462464 net.cpp:113] Setting up fc8_VOC
I0502 17:32:03.866307 2050462464 net.cpp:120] Top shape: 64 6 (384)
I0502 17:32:03.866325 2050462464 layer_factory.hpp:74] Creating layer fc8_VOC_fc8_VOC_0_split
I0502 17:32:03.866338 2050462464 net.cpp:84] Creating Layer fc8_VOC_fc8_VOC_0_split
I0502 17:32:03.866348 2050462464 net.cpp:380] fc8_VOC_fc8_VOC_0_split <- fc8_VOC
I0502 17:32:03.866358 2050462464 net.cpp:338] fc8_VOC_fc8_VOC_0_split -> fc8_VOC_fc8_VOC_0_split_0
I0502 17:32:03.866374 2050462464 net.cpp:338] fc8_VOC_fc8_VOC_0_split -> fc8_VOC_fc8_VOC_0_split_1
I0502 17:32:03.866405 2050462464 net.cpp:113] Setting up fc8_VOC_fc8_VOC_0_split
I0502 17:32:03.866422 2050462464 net.cpp:120] Top shape: 64 6 (384)
I0502 17:32:03.866432 2050462464 net.cpp:120] Top shape: 64 6 (384)
I0502 17:32:03.866441 2050462464 layer_factory.hpp:74] Creating layer accuracy
I0502 17:32:03.866459 2050462464 net.cpp:84] Creating Layer accuracy
I0502 17:32:03.866468 2050462464 net.cpp:380] accuracy <- fc8_VOC_fc8_VOC_0_split_0
I0502 17:32:03.866477 2050462464 net.cpp:380] accuracy <- label_data_1_split_0
I0502 17:32:03.866507 2050462464 net.cpp:338] accuracy -> accuracy
I0502 17:32:03.866535 2050462464 net.cpp:113] Setting up accuracy
I0502 17:32:03.866549 2050462464 net.cpp:120] Top shape: (1)
I0502 17:32:03.866557 2050462464 layer_factory.hpp:74] Creating layer loss
I0502 17:32:03.866569 2050462464 net.cpp:84] Creating Layer loss
I0502 17:32:03.866577 2050462464 net.cpp:380] loss <- fc8_VOC_fc8_VOC_0_split_1
I0502 17:32:03.866586 2050462464 net.cpp:380] loss <- label_data_1_split_1
I0502 17:32:03.866597 2050462464 net.cpp:338] loss -> loss
I0502 17:32:03.866631 2050462464 net.cpp:113] Setting up loss
I0502 17:32:03.866653 2050462464 layer_factory.hpp:74] Creating layer loss
I0502 17:32:03.866678 2050462464 net.cpp:120] Top shape: (1)
I0502 17:32:03.866689 2050462464 net.cpp:122]     with loss weight 1
I0502 17:32:03.866703 2050462464 net.cpp:167] loss needs backward computation.
I0502 17:32:03.866711 2050462464 net.cpp:169] accuracy does not need backward computation.
I0502 17:32:03.866719 2050462464 net.cpp:167] fc8_VOC_fc8_VOC_0_split needs backward computation.
I0502 17:32:03.866726 2050462464 net.cpp:167] fc8_VOC needs backward computation.
I0502 17:32:03.866734 2050462464 net.cpp:167] drop7 needs backward computation.
I0502 17:32:03.866742 2050462464 net.cpp:167] relu7 needs backward computation.
I0502 17:32:03.866750 2050462464 net.cpp:167] fc7 needs backward computation.
I0502 17:32:03.866758 2050462464 net.cpp:167] drop6 needs backward computation.
I0502 17:32:03.866765 2050462464 net.cpp:167] relu6 needs backward computation.
I0502 17:32:03.866773 2050462464 net.cpp:167] fc6 needs backward computation.
I0502 17:32:03.866781 2050462464 net.cpp:167] pool5 needs backward computation.
I0502 17:32:03.866789 2050462464 net.cpp:167] relu5 needs backward computation.
I0502 17:32:03.866840 2050462464 net.cpp:167] conv5 needs backward computation.
I0502 17:32:03.866850 2050462464 net.cpp:167] relu4 needs backward computation.
I0502 17:32:03.866858 2050462464 net.cpp:167] conv4 needs backward computation.
I0502 17:32:03.866879 2050462464 net.cpp:167] relu3 needs backward computation.
I0502 17:32:03.866888 2050462464 net.cpp:167] conv3 needs backward computation.
I0502 17:32:03.866910 2050462464 net.cpp:167] norm2 needs backward computation.
I0502 17:32:03.866919 2050462464 net.cpp:167] pool2 needs backward computation.
I0502 17:32:03.866922 2050462464 net.cpp:167] relu2 needs backward computation.
I0502 17:32:03.866927 2050462464 net.cpp:167] conv2 needs backward computation.
I0502 17:32:03.866931 2050462464 net.cpp:167] norm1 needs backward computation.
I0502 17:32:03.866936 2050462464 net.cpp:167] pool1 needs backward computation.
I0502 17:32:03.866940 2050462464 net.cpp:167] relu1 needs backward computation.
I0502 17:32:03.866968 2050462464 net.cpp:167] conv1 needs backward computation.
I0502 17:32:03.866981 2050462464 net.cpp:169] label_data_1_split does not need backward computation.
I0502 17:32:03.866989 2050462464 net.cpp:169] data does not need backward computation.
I0502 17:32:03.866997 2050462464 net.cpp:205] This network produces output accuracy
I0502 17:32:03.867007 2050462464 net.cpp:205] This network produces output loss
I0502 17:32:03.867029 2050462464 net.cpp:447] Collecting Learning Rate and Weight Decay.
I0502 17:32:03.867043 2050462464 net.cpp:217] Network initialization done.
I0502 17:32:03.867049 2050462464 net.cpp:218] Memory required for data: 439054344
I0502 17:32:03.867298 2050462464 solver.cpp:42] Solver scaffolding done.
I0502 17:32:03.867388 2050462464 caffe.cpp:86] Finetuning from bvlc_reference_rcnn_ilsvrc13.caffemodel
E0502 17:32:04.605200 2050462464 upgrade_proto.cpp:618] Attempting to upgrade input file specified using deprecated V1LayerParameter: bvlc_reference_rcnn_ilsvrc13.caffemodel
I0502 17:32:05.015053 2050462464 upgrade_proto.cpp:626] Successfully upgraded file specified using deprecated V1LayerParameter
E0502 17:32:05.723994 2050462464 upgrade_proto.cpp:618] Attempting to upgrade input file specified using deprecated V1LayerParameter: bvlc_reference_rcnn_ilsvrc13.caffemodel
I0502 17:32:06.039270 2050462464 upgrade_proto.cpp:626] Successfully upgraded file specified using deprecated V1LayerParameter
I0502 17:32:06.158645 2050462464 solver.cpp:222] Solving CaffeNet
I0502 17:32:06.158676 2050462464 solver.cpp:223] Learning Rate Policy: step
I0502 17:32:06.158686 2050462464 solver.cpp:266] Iteration 0, Testing net (#0)
^C
C02MX066FD58:rcc_net cusgadmin$ clear

C02MX066FD58:rcc_net cusgadmin$ caffe train -solver solver_background.prototxt -weights bvlc_reference_rcnn_ilsvrc13.caffemodel 
I0502 17:32:41.759130 2050462464 caffe.cpp:117] Use CPU.
I0502 17:32:41.760625 2050462464 caffe.cpp:121] Starting Optimization
I0502 17:32:41.760640 2050462464 solver.cpp:32] Initializing solver from parameters: 
test_iter: 1000
test_interval: 1000
base_lr: 0.001
display: 20
max_iter: 10000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 100000
snapshot: 500
snapshot_prefix: "caffenet_train_background"
solver_mode: CPU
net: "train_val_background.prototxt"
I0502 17:32:41.760715 2050462464 solver.cpp:70] Creating training net from net file: train_val_background.prototxt
I0502 17:32:41.761044 2050462464 net.cpp:257] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0502 17:32:41.761070 2050462464 net.cpp:257] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0502 17:32:41.761078 2050462464 net.cpp:42] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "ImageData"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  image_data_param {
    source: "../train_list.txt"
    batch_size: 64
    shuffle: true
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8_VOC"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8_VOC"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  param {
    lr_mult: 20
    decay_mult: 0
  }
  inner_product_param {
    num_output: 6
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_VOC"
  bottom: "label"
  top: "loss"
}
I0502 17:32:41.761468 2050462464 layer_factory.hpp:74] Creating layer data
I0502 17:32:41.761492 2050462464 net.cpp:84] Creating Layer data
I0502 17:32:41.761500 2050462464 net.cpp:338] data -> data
I0502 17:32:41.761525 2050462464 net.cpp:338] data -> label
I0502 17:32:41.761533 2050462464 net.cpp:113] Setting up data
I0502 17:32:41.761546 2050462464 image_data_layer.cpp:36] Opening file ../train_list.txt
I0502 17:32:41.795171 2050462464 image_data_layer.cpp:46] Shuffling data
I0502 17:32:41.802644 2050462464 image_data_layer.cpp:51] A total of 32601 images.
I0502 17:32:41.804448 2050462464 image_data_layer.cpp:80] output data size: 64,3,227,227
I0502 17:32:41.824271 2050462464 net.cpp:120] Top shape: 64 3 227 227 (9893568)
I0502 17:32:41.824309 2050462464 net.cpp:120] Top shape: 64 (64)
I0502 17:32:41.824323 2050462464 layer_factory.hpp:74] Creating layer conv1
I0502 17:32:41.824345 2050462464 net.cpp:84] Creating Layer conv1
I0502 17:32:41.824354 2050462464 net.cpp:380] conv1 <- data
I0502 17:32:41.824368 2050462464 net.cpp:338] conv1 -> conv1
I0502 17:32:41.824389 2050462464 net.cpp:113] Setting up conv1
I0502 17:32:41.825146 2050462464 net.cpp:120] Top shape: 64 96 55 55 (18585600)
I0502 17:32:41.825170 2050462464 layer_factory.hpp:74] Creating layer relu1
I0502 17:32:41.825186 2050462464 net.cpp:84] Creating Layer relu1
I0502 17:32:41.825196 2050462464 net.cpp:380] relu1 <- conv1
I0502 17:32:41.825206 2050462464 net.cpp:327] relu1 -> conv1 (in-place)
I0502 17:32:41.825218 2050462464 net.cpp:113] Setting up relu1
I0502 17:32:41.825227 2050462464 net.cpp:120] Top shape: 64 96 55 55 (18585600)
I0502 17:32:41.825237 2050462464 layer_factory.hpp:74] Creating layer pool1
I0502 17:32:41.825248 2050462464 net.cpp:84] Creating Layer pool1
I0502 17:32:41.825256 2050462464 net.cpp:380] pool1 <- conv1
I0502 17:32:41.825268 2050462464 net.cpp:338] pool1 -> pool1
I0502 17:32:41.825283 2050462464 net.cpp:113] Setting up pool1
I0502 17:32:41.825309 2050462464 net.cpp:120] Top shape: 64 96 27 27 (4478976)
I0502 17:32:41.825321 2050462464 layer_factory.hpp:74] Creating layer norm1
I0502 17:32:41.825340 2050462464 net.cpp:84] Creating Layer norm1
I0502 17:32:41.825358 2050462464 net.cpp:380] norm1 <- pool1
I0502 17:32:41.825373 2050462464 net.cpp:338] norm1 -> norm1
I0502 17:32:41.825387 2050462464 net.cpp:113] Setting up norm1
I0502 17:32:41.825404 2050462464 net.cpp:120] Top shape: 64 96 27 27 (4478976)
I0502 17:32:41.825454 2050462464 layer_factory.hpp:74] Creating layer conv2
I0502 17:32:41.825470 2050462464 net.cpp:84] Creating Layer conv2
I0502 17:32:41.825479 2050462464 net.cpp:380] conv2 <- norm1
I0502 17:32:41.825492 2050462464 net.cpp:338] conv2 -> conv2
I0502 17:32:41.825506 2050462464 net.cpp:113] Setting up conv2
I0502 17:32:41.831504 2050462464 net.cpp:120] Top shape: 64 256 27 27 (11943936)
I0502 17:32:41.831533 2050462464 layer_factory.hpp:74] Creating layer relu2
I0502 17:32:41.831544 2050462464 net.cpp:84] Creating Layer relu2
I0502 17:32:41.831550 2050462464 net.cpp:380] relu2 <- conv2
I0502 17:32:41.831559 2050462464 net.cpp:327] relu2 -> conv2 (in-place)
I0502 17:32:41.831571 2050462464 net.cpp:113] Setting up relu2
I0502 17:32:41.831647 2050462464 net.cpp:120] Top shape: 64 256 27 27 (11943936)
I0502 17:32:41.831670 2050462464 layer_factory.hpp:74] Creating layer pool2
I0502 17:32:41.831684 2050462464 net.cpp:84] Creating Layer pool2
I0502 17:32:41.831694 2050462464 net.cpp:380] pool2 <- conv2
I0502 17:32:41.831707 2050462464 net.cpp:338] pool2 -> pool2
I0502 17:32:41.831722 2050462464 net.cpp:113] Setting up pool2
I0502 17:32:41.831735 2050462464 net.cpp:120] Top shape: 64 256 13 13 (2768896)
I0502 17:32:41.831748 2050462464 layer_factory.hpp:74] Creating layer norm2
I0502 17:32:41.831760 2050462464 net.cpp:84] Creating Layer norm2
I0502 17:32:41.831769 2050462464 net.cpp:380] norm2 <- pool2
I0502 17:32:41.831780 2050462464 net.cpp:338] norm2 -> norm2
I0502 17:32:41.831794 2050462464 net.cpp:113] Setting up norm2
I0502 17:32:41.831805 2050462464 net.cpp:120] Top shape: 64 256 13 13 (2768896)
I0502 17:32:41.831816 2050462464 layer_factory.hpp:74] Creating layer conv3
I0502 17:32:41.831835 2050462464 net.cpp:84] Creating Layer conv3
I0502 17:32:41.831845 2050462464 net.cpp:380] conv3 <- norm2
I0502 17:32:41.831857 2050462464 net.cpp:338] conv3 -> conv3
I0502 17:32:41.831873 2050462464 net.cpp:113] Setting up conv3
I0502 17:32:41.849298 2050462464 net.cpp:120] Top shape: 64 384 13 13 (4153344)
I0502 17:32:41.849323 2050462464 layer_factory.hpp:74] Creating layer relu3
I0502 17:32:41.849337 2050462464 net.cpp:84] Creating Layer relu3
I0502 17:32:41.849344 2050462464 net.cpp:380] relu3 <- conv3
I0502 17:32:41.849350 2050462464 net.cpp:327] relu3 -> conv3 (in-place)
I0502 17:32:41.849357 2050462464 net.cpp:113] Setting up relu3
I0502 17:32:41.849362 2050462464 net.cpp:120] Top shape: 64 384 13 13 (4153344)
I0502 17:32:41.849369 2050462464 layer_factory.hpp:74] Creating layer conv4
I0502 17:32:41.849376 2050462464 net.cpp:84] Creating Layer conv4
I0502 17:32:41.849381 2050462464 net.cpp:380] conv4 <- conv3
I0502 17:32:41.849387 2050462464 net.cpp:338] conv4 -> conv4
I0502 17:32:41.849395 2050462464 net.cpp:113] Setting up conv4
I0502 17:32:41.860359 2050462464 net.cpp:120] Top shape: 64 384 13 13 (4153344)
I0502 17:32:41.860385 2050462464 layer_factory.hpp:74] Creating layer relu4
I0502 17:32:41.860395 2050462464 net.cpp:84] Creating Layer relu4
I0502 17:32:41.860399 2050462464 net.cpp:380] relu4 <- conv4
I0502 17:32:41.860414 2050462464 net.cpp:327] relu4 -> conv4 (in-place)
I0502 17:32:41.860422 2050462464 net.cpp:113] Setting up relu4
I0502 17:32:41.860429 2050462464 net.cpp:120] Top shape: 64 384 13 13 (4153344)
I0502 17:32:41.860435 2050462464 layer_factory.hpp:74] Creating layer conv5
I0502 17:32:41.860445 2050462464 net.cpp:84] Creating Layer conv5
I0502 17:32:41.860450 2050462464 net.cpp:380] conv5 <- conv4
I0502 17:32:41.860457 2050462464 net.cpp:338] conv5 -> conv5
I0502 17:32:41.860466 2050462464 net.cpp:113] Setting up conv5
I0502 17:32:41.867825 2050462464 net.cpp:120] Top shape: 64 256 13 13 (2768896)
I0502 17:32:41.867853 2050462464 layer_factory.hpp:74] Creating layer relu5
I0502 17:32:41.867863 2050462464 net.cpp:84] Creating Layer relu5
I0502 17:32:41.867869 2050462464 net.cpp:380] relu5 <- conv5
I0502 17:32:41.867876 2050462464 net.cpp:327] relu5 -> conv5 (in-place)
I0502 17:32:41.867883 2050462464 net.cpp:113] Setting up relu5
I0502 17:32:41.867888 2050462464 net.cpp:120] Top shape: 64 256 13 13 (2768896)
I0502 17:32:41.867914 2050462464 layer_factory.hpp:74] Creating layer pool5
I0502 17:32:41.867923 2050462464 net.cpp:84] Creating Layer pool5
I0502 17:32:41.867928 2050462464 net.cpp:380] pool5 <- conv5
I0502 17:32:41.867933 2050462464 net.cpp:338] pool5 -> pool5
I0502 17:32:41.867940 2050462464 net.cpp:113] Setting up pool5
I0502 17:32:41.867947 2050462464 net.cpp:120] Top shape: 64 256 6 6 (589824)
I0502 17:32:41.867954 2050462464 layer_factory.hpp:74] Creating layer fc6
I0502 17:32:41.867965 2050462464 net.cpp:84] Creating Layer fc6
I0502 17:32:41.867970 2050462464 net.cpp:380] fc6 <- pool5
I0502 17:32:41.867977 2050462464 net.cpp:338] fc6 -> fc6
I0502 17:32:41.867985 2050462464 net.cpp:113] Setting up fc6
I0502 17:32:42.489469 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0502 17:32:42.489500 2050462464 layer_factory.hpp:74] Creating layer relu6
I0502 17:32:42.489516 2050462464 net.cpp:84] Creating Layer relu6
I0502 17:32:42.489524 2050462464 net.cpp:380] relu6 <- fc6
I0502 17:32:42.489536 2050462464 net.cpp:327] relu6 -> fc6 (in-place)
I0502 17:32:42.489548 2050462464 net.cpp:113] Setting up relu6
I0502 17:32:42.489557 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0502 17:32:42.489565 2050462464 layer_factory.hpp:74] Creating layer drop6
I0502 17:32:42.489575 2050462464 net.cpp:84] Creating Layer drop6
I0502 17:32:42.489581 2050462464 net.cpp:380] drop6 <- fc6
I0502 17:32:42.489596 2050462464 net.cpp:327] drop6 -> fc6 (in-place)
I0502 17:32:42.489608 2050462464 net.cpp:113] Setting up drop6
I0502 17:32:42.489625 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0502 17:32:42.489635 2050462464 layer_factory.hpp:74] Creating layer fc7
I0502 17:32:42.489646 2050462464 net.cpp:84] Creating Layer fc7
I0502 17:32:42.489650 2050462464 net.cpp:380] fc7 <- fc6
I0502 17:32:42.489660 2050462464 net.cpp:338] fc7 -> fc7
I0502 17:32:42.489671 2050462464 net.cpp:113] Setting up fc7
I0502 17:32:42.760328 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0502 17:32:42.760361 2050462464 layer_factory.hpp:74] Creating layer relu7
I0502 17:32:42.760373 2050462464 net.cpp:84] Creating Layer relu7
I0502 17:32:42.760380 2050462464 net.cpp:380] relu7 <- fc7
I0502 17:32:42.760386 2050462464 net.cpp:327] relu7 -> fc7 (in-place)
I0502 17:32:42.760396 2050462464 net.cpp:113] Setting up relu7
I0502 17:32:42.760401 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0502 17:32:42.760411 2050462464 layer_factory.hpp:74] Creating layer drop7
I0502 17:32:42.760422 2050462464 net.cpp:84] Creating Layer drop7
I0502 17:32:42.760432 2050462464 net.cpp:380] drop7 <- fc7
I0502 17:32:42.760442 2050462464 net.cpp:327] drop7 -> fc7 (in-place)
I0502 17:32:42.760454 2050462464 net.cpp:113] Setting up drop7
I0502 17:32:42.760465 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0502 17:32:42.760476 2050462464 layer_factory.hpp:74] Creating layer fc8_VOC
I0502 17:32:42.760490 2050462464 net.cpp:84] Creating Layer fc8_VOC
I0502 17:32:42.760500 2050462464 net.cpp:380] fc8_VOC <- fc7
I0502 17:32:42.760512 2050462464 net.cpp:338] fc8_VOC -> fc8_VOC
I0502 17:32:42.760529 2050462464 net.cpp:113] Setting up fc8_VOC
I0502 17:32:42.761054 2050462464 net.cpp:120] Top shape: 64 6 (384)
I0502 17:32:42.761070 2050462464 layer_factory.hpp:74] Creating layer loss
I0502 17:32:42.761086 2050462464 net.cpp:84] Creating Layer loss
I0502 17:32:42.761102 2050462464 net.cpp:380] loss <- fc8_VOC
I0502 17:32:42.761109 2050462464 net.cpp:380] loss <- label
I0502 17:32:42.761119 2050462464 net.cpp:338] loss -> loss
I0502 17:32:42.761132 2050462464 net.cpp:113] Setting up loss
I0502 17:32:42.761142 2050462464 layer_factory.hpp:74] Creating layer loss
I0502 17:32:42.761162 2050462464 net.cpp:120] Top shape: (1)
I0502 17:32:42.761169 2050462464 net.cpp:122]     with loss weight 1
I0502 17:32:42.761188 2050462464 net.cpp:167] loss needs backward computation.
I0502 17:32:42.761196 2050462464 net.cpp:167] fc8_VOC needs backward computation.
I0502 17:32:42.761204 2050462464 net.cpp:167] drop7 needs backward computation.
I0502 17:32:42.761255 2050462464 net.cpp:167] relu7 needs backward computation.
I0502 17:32:42.761263 2050462464 net.cpp:167] fc7 needs backward computation.
I0502 17:32:42.761272 2050462464 net.cpp:167] drop6 needs backward computation.
I0502 17:32:42.761281 2050462464 net.cpp:167] relu6 needs backward computation.
I0502 17:32:42.761287 2050462464 net.cpp:167] fc6 needs backward computation.
I0502 17:32:42.761296 2050462464 net.cpp:167] pool5 needs backward computation.
I0502 17:32:42.761301 2050462464 net.cpp:167] relu5 needs backward computation.
I0502 17:32:42.761308 2050462464 net.cpp:167] conv5 needs backward computation.
I0502 17:32:42.761317 2050462464 net.cpp:167] relu4 needs backward computation.
I0502 17:32:42.761323 2050462464 net.cpp:167] conv4 needs backward computation.
I0502 17:32:42.761332 2050462464 net.cpp:167] relu3 needs backward computation.
I0502 17:32:42.761339 2050462464 net.cpp:167] conv3 needs backward computation.
I0502 17:32:42.761348 2050462464 net.cpp:167] norm2 needs backward computation.
I0502 17:32:42.761355 2050462464 net.cpp:167] pool2 needs backward computation.
I0502 17:32:42.761363 2050462464 net.cpp:167] relu2 needs backward computation.
I0502 17:32:42.761371 2050462464 net.cpp:167] conv2 needs backward computation.
I0502 17:32:42.761379 2050462464 net.cpp:167] norm1 needs backward computation.
I0502 17:32:42.761386 2050462464 net.cpp:167] pool1 needs backward computation.
I0502 17:32:42.761391 2050462464 net.cpp:167] relu1 needs backward computation.
I0502 17:32:42.761395 2050462464 net.cpp:167] conv1 needs backward computation.
I0502 17:32:42.761400 2050462464 net.cpp:169] data does not need backward computation.
I0502 17:32:42.761404 2050462464 net.cpp:205] This network produces output loss
I0502 17:32:42.761417 2050462464 net.cpp:447] Collecting Learning Rate and Weight Decay.
I0502 17:32:42.761430 2050462464 net.cpp:217] Network initialization done.
I0502 17:32:42.761437 2050462464 net.cpp:218] Memory required for data: 439050756
I0502 17:32:42.761921 2050462464 solver.cpp:154] Creating test net (#0) specified by net file: train_val_background.prototxt
I0502 17:32:42.761986 2050462464 net.cpp:257] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0502 17:32:42.762020 2050462464 net.cpp:42] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "ImageData"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  image_data_param {
    source: "../test_list.txt"
    batch_size: 64
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8_VOC"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8_VOC"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  param {
    lr_mult: 20
    decay_mult: 0
  }
  inner_product_param {
    num_output: 6
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_VOC"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_VOC"
  bottom: "label"
  top: "loss"
}
I0502 17:32:42.763104 2050462464 layer_factory.hpp:74] Creating layer data
I0502 17:32:42.763131 2050462464 net.cpp:84] Creating Layer data
I0502 17:32:42.763142 2050462464 net.cpp:338] data -> data
I0502 17:32:42.763164 2050462464 net.cpp:338] data -> label
I0502 17:32:42.763177 2050462464 net.cpp:113] Setting up data
I0502 17:32:42.763187 2050462464 image_data_layer.cpp:36] Opening file ../test_list.txt
I0502 17:32:42.769119 2050462464 image_data_layer.cpp:51] A total of 3703 images.
I0502 17:32:42.770612 2050462464 image_data_layer.cpp:80] output data size: 64,3,227,227
I0502 17:32:42.790828 2050462464 net.cpp:120] Top shape: 64 3 227 227 (9893568)
I0502 17:32:42.790859 2050462464 net.cpp:120] Top shape: 64 (64)
I0502 17:32:42.790870 2050462464 layer_factory.hpp:74] Creating layer label_data_1_split
I0502 17:32:42.790915 2050462464 net.cpp:84] Creating Layer label_data_1_split
I0502 17:32:42.790925 2050462464 net.cpp:380] label_data_1_split <- label
I0502 17:32:42.790947 2050462464 net.cpp:338] label_data_1_split -> label_data_1_split_0
I0502 17:32:42.790978 2050462464 net.cpp:338] label_data_1_split -> label_data_1_split_1
I0502 17:32:42.790993 2050462464 net.cpp:113] Setting up label_data_1_split
I0502 17:32:42.791005 2050462464 net.cpp:120] Top shape: 64 (64)
I0502 17:32:42.791015 2050462464 net.cpp:120] Top shape: 64 (64)
I0502 17:32:42.791024 2050462464 layer_factory.hpp:74] Creating layer conv1
I0502 17:32:42.791041 2050462464 net.cpp:84] Creating Layer conv1
I0502 17:32:42.791050 2050462464 net.cpp:380] conv1 <- data
I0502 17:32:42.791069 2050462464 net.cpp:338] conv1 -> conv1
I0502 17:32:42.791085 2050462464 net.cpp:113] Setting up conv1
I0502 17:32:42.791854 2050462464 net.cpp:120] Top shape: 64 96 55 55 (18585600)
I0502 17:32:42.791877 2050462464 layer_factory.hpp:74] Creating layer relu1
I0502 17:32:42.791889 2050462464 net.cpp:84] Creating Layer relu1
I0502 17:32:42.791898 2050462464 net.cpp:380] relu1 <- conv1
I0502 17:32:42.791929 2050462464 net.cpp:327] relu1 -> conv1 (in-place)
I0502 17:32:42.791946 2050462464 net.cpp:113] Setting up relu1
I0502 17:32:42.791975 2050462464 net.cpp:120] Top shape: 64 96 55 55 (18585600)
I0502 17:32:42.791982 2050462464 layer_factory.hpp:74] Creating layer pool1
I0502 17:32:42.791990 2050462464 net.cpp:84] Creating Layer pool1
I0502 17:32:42.791996 2050462464 net.cpp:380] pool1 <- conv1
I0502 17:32:42.792007 2050462464 net.cpp:338] pool1 -> pool1
I0502 17:32:42.792016 2050462464 net.cpp:113] Setting up pool1
I0502 17:32:42.792023 2050462464 net.cpp:120] Top shape: 64 96 27 27 (4478976)
I0502 17:32:42.792032 2050462464 layer_factory.hpp:74] Creating layer norm1
I0502 17:32:42.792043 2050462464 net.cpp:84] Creating Layer norm1
I0502 17:32:42.792052 2050462464 net.cpp:380] norm1 <- pool1
I0502 17:32:42.792062 2050462464 net.cpp:338] norm1 -> norm1
I0502 17:32:42.792073 2050462464 net.cpp:113] Setting up norm1
I0502 17:32:42.792083 2050462464 net.cpp:120] Top shape: 64 96 27 27 (4478976)
I0502 17:32:42.792094 2050462464 layer_factory.hpp:74] Creating layer conv2
I0502 17:32:42.792109 2050462464 net.cpp:84] Creating Layer conv2
I0502 17:32:42.792116 2050462464 net.cpp:380] conv2 <- norm1
I0502 17:32:42.792129 2050462464 net.cpp:338] conv2 -> conv2
I0502 17:32:42.792142 2050462464 net.cpp:113] Setting up conv2
I0502 17:32:42.798202 2050462464 net.cpp:120] Top shape: 64 256 27 27 (11943936)
I0502 17:32:42.798249 2050462464 layer_factory.hpp:74] Creating layer relu2
I0502 17:32:42.798274 2050462464 net.cpp:84] Creating Layer relu2
I0502 17:32:42.798285 2050462464 net.cpp:380] relu2 <- conv2
I0502 17:32:42.798295 2050462464 net.cpp:327] relu2 -> conv2 (in-place)
I0502 17:32:42.798307 2050462464 net.cpp:113] Setting up relu2
I0502 17:32:42.798316 2050462464 net.cpp:120] Top shape: 64 256 27 27 (11943936)
I0502 17:32:42.798326 2050462464 layer_factory.hpp:74] Creating layer pool2
I0502 17:32:42.798338 2050462464 net.cpp:84] Creating Layer pool2
I0502 17:32:42.798346 2050462464 net.cpp:380] pool2 <- conv2
I0502 17:32:42.798357 2050462464 net.cpp:338] pool2 -> pool2
I0502 17:32:42.798367 2050462464 net.cpp:113] Setting up pool2
I0502 17:32:42.798378 2050462464 net.cpp:120] Top shape: 64 256 13 13 (2768896)
I0502 17:32:42.798388 2050462464 layer_factory.hpp:74] Creating layer norm2
I0502 17:32:42.798395 2050462464 net.cpp:84] Creating Layer norm2
I0502 17:32:42.798399 2050462464 net.cpp:380] norm2 <- pool2
I0502 17:32:42.798408 2050462464 net.cpp:338] norm2 -> norm2
I0502 17:32:42.798418 2050462464 net.cpp:113] Setting up norm2
I0502 17:32:42.798427 2050462464 net.cpp:120] Top shape: 64 256 13 13 (2768896)
I0502 17:32:42.798435 2050462464 layer_factory.hpp:74] Creating layer conv3
I0502 17:32:42.798451 2050462464 net.cpp:84] Creating Layer conv3
I0502 17:32:42.798460 2050462464 net.cpp:380] conv3 <- norm2
I0502 17:32:42.798471 2050462464 net.cpp:338] conv3 -> conv3
I0502 17:32:42.798485 2050462464 net.cpp:113] Setting up conv3
I0502 17:32:42.816985 2050462464 net.cpp:120] Top shape: 64 384 13 13 (4153344)
I0502 17:32:42.817013 2050462464 layer_factory.hpp:74] Creating layer relu3
I0502 17:32:42.817023 2050462464 net.cpp:84] Creating Layer relu3
I0502 17:32:42.817028 2050462464 net.cpp:380] relu3 <- conv3
I0502 17:32:42.817034 2050462464 net.cpp:327] relu3 -> conv3 (in-place)
I0502 17:32:42.817040 2050462464 net.cpp:113] Setting up relu3
I0502 17:32:42.817045 2050462464 net.cpp:120] Top shape: 64 384 13 13 (4153344)
I0502 17:32:42.817051 2050462464 layer_factory.hpp:74] Creating layer conv4
I0502 17:32:42.817059 2050462464 net.cpp:84] Creating Layer conv4
I0502 17:32:42.817064 2050462464 net.cpp:380] conv4 <- conv3
I0502 17:32:42.817070 2050462464 net.cpp:338] conv4 -> conv4
I0502 17:32:42.817077 2050462464 net.cpp:113] Setting up conv4
I0502 17:32:42.829071 2050462464 net.cpp:120] Top shape: 64 384 13 13 (4153344)
I0502 17:32:42.829097 2050462464 layer_factory.hpp:74] Creating layer relu4
I0502 17:32:42.829110 2050462464 net.cpp:84] Creating Layer relu4
I0502 17:32:42.829119 2050462464 net.cpp:380] relu4 <- conv4
I0502 17:32:42.829129 2050462464 net.cpp:327] relu4 -> conv4 (in-place)
I0502 17:32:42.829141 2050462464 net.cpp:113] Setting up relu4
I0502 17:32:42.829149 2050462464 net.cpp:120] Top shape: 64 384 13 13 (4153344)
I0502 17:32:42.829160 2050462464 layer_factory.hpp:74] Creating layer conv5
I0502 17:32:42.829174 2050462464 net.cpp:84] Creating Layer conv5
I0502 17:32:42.829181 2050462464 net.cpp:380] conv5 <- conv4
I0502 17:32:42.829192 2050462464 net.cpp:338] conv5 -> conv5
I0502 17:32:42.829205 2050462464 net.cpp:113] Setting up conv5
I0502 17:32:42.836642 2050462464 net.cpp:120] Top shape: 64 256 13 13 (2768896)
I0502 17:32:42.836666 2050462464 layer_factory.hpp:74] Creating layer relu5
I0502 17:32:42.836675 2050462464 net.cpp:84] Creating Layer relu5
I0502 17:32:42.836680 2050462464 net.cpp:380] relu5 <- conv5
I0502 17:32:42.836688 2050462464 net.cpp:327] relu5 -> conv5 (in-place)
I0502 17:32:42.836694 2050462464 net.cpp:113] Setting up relu5
I0502 17:32:42.836699 2050462464 net.cpp:120] Top shape: 64 256 13 13 (2768896)
I0502 17:32:42.836705 2050462464 layer_factory.hpp:74] Creating layer pool5
I0502 17:32:42.836714 2050462464 net.cpp:84] Creating Layer pool5
I0502 17:32:42.836717 2050462464 net.cpp:380] pool5 <- conv5
I0502 17:32:42.836724 2050462464 net.cpp:338] pool5 -> pool5
I0502 17:32:42.836732 2050462464 net.cpp:113] Setting up pool5
I0502 17:32:42.836745 2050462464 net.cpp:120] Top shape: 64 256 6 6 (589824)
I0502 17:32:42.836753 2050462464 layer_factory.hpp:74] Creating layer fc6
I0502 17:32:42.836761 2050462464 net.cpp:84] Creating Layer fc6
I0502 17:32:42.836766 2050462464 net.cpp:380] fc6 <- pool5
I0502 17:32:42.836771 2050462464 net.cpp:338] fc6 -> fc6
I0502 17:32:42.836778 2050462464 net.cpp:113] Setting up fc6
I0502 17:32:43.470657 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0502 17:32:43.470710 2050462464 layer_factory.hpp:74] Creating layer relu6
I0502 17:32:43.470727 2050462464 net.cpp:84] Creating Layer relu6
I0502 17:32:43.470746 2050462464 net.cpp:380] relu6 <- fc6
I0502 17:32:43.470757 2050462464 net.cpp:327] relu6 -> fc6 (in-place)
I0502 17:32:43.470767 2050462464 net.cpp:113] Setting up relu6
I0502 17:32:43.470774 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0502 17:32:43.470782 2050462464 layer_factory.hpp:74] Creating layer drop6
I0502 17:32:43.470793 2050462464 net.cpp:84] Creating Layer drop6
I0502 17:32:43.470798 2050462464 net.cpp:380] drop6 <- fc6
I0502 17:32:43.470806 2050462464 net.cpp:327] drop6 -> fc6 (in-place)
I0502 17:32:43.470815 2050462464 net.cpp:113] Setting up drop6
I0502 17:32:43.470824 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0502 17:32:43.470831 2050462464 layer_factory.hpp:74] Creating layer fc7
I0502 17:32:43.470871 2050462464 net.cpp:84] Creating Layer fc7
I0502 17:32:43.470877 2050462464 net.cpp:380] fc7 <- fc6
I0502 17:32:43.470885 2050462464 net.cpp:338] fc7 -> fc7
I0502 17:32:43.470893 2050462464 net.cpp:113] Setting up fc7
I0502 17:32:43.756098 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0502 17:32:43.756181 2050462464 layer_factory.hpp:74] Creating layer relu7
I0502 17:32:43.756206 2050462464 net.cpp:84] Creating Layer relu7
I0502 17:32:43.756248 2050462464 net.cpp:380] relu7 <- fc7
I0502 17:32:43.756264 2050462464 net.cpp:327] relu7 -> fc7 (in-place)
I0502 17:32:43.756276 2050462464 net.cpp:113] Setting up relu7
I0502 17:32:43.756284 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0502 17:32:43.756294 2050462464 layer_factory.hpp:74] Creating layer drop7
I0502 17:32:43.756305 2050462464 net.cpp:84] Creating Layer drop7
I0502 17:32:43.756312 2050462464 net.cpp:380] drop7 <- fc7
I0502 17:32:43.756322 2050462464 net.cpp:327] drop7 -> fc7 (in-place)
I0502 17:32:43.756332 2050462464 net.cpp:113] Setting up drop7
I0502 17:32:43.756341 2050462464 net.cpp:120] Top shape: 64 4096 (262144)
I0502 17:32:43.756350 2050462464 layer_factory.hpp:74] Creating layer fc8_VOC
I0502 17:32:43.756363 2050462464 net.cpp:84] Creating Layer fc8_VOC
I0502 17:32:43.756372 2050462464 net.cpp:380] fc8_VOC <- fc7
I0502 17:32:43.756431 2050462464 net.cpp:338] fc8_VOC -> fc8_VOC
I0502 17:32:43.756496 2050462464 net.cpp:113] Setting up fc8_VOC
I0502 17:32:43.757196 2050462464 net.cpp:120] Top shape: 64 6 (384)
I0502 17:32:43.757278 2050462464 layer_factory.hpp:74] Creating layer fc8_VOC_fc8_VOC_0_split
I0502 17:32:43.757294 2050462464 net.cpp:84] Creating Layer fc8_VOC_fc8_VOC_0_split
I0502 17:32:43.757303 2050462464 net.cpp:380] fc8_VOC_fc8_VOC_0_split <- fc8_VOC
I0502 17:32:43.757316 2050462464 net.cpp:338] fc8_VOC_fc8_VOC_0_split -> fc8_VOC_fc8_VOC_0_split_0
I0502 17:32:43.757333 2050462464 net.cpp:338] fc8_VOC_fc8_VOC_0_split -> fc8_VOC_fc8_VOC_0_split_1
I0502 17:32:43.757347 2050462464 net.cpp:113] Setting up fc8_VOC_fc8_VOC_0_split
I0502 17:32:43.757357 2050462464 net.cpp:120] Top shape: 64 6 (384)
I0502 17:32:43.757366 2050462464 net.cpp:120] Top shape: 64 6 (384)
I0502 17:32:43.757416 2050462464 layer_factory.hpp:74] Creating layer accuracy
I0502 17:32:43.757436 2050462464 net.cpp:84] Creating Layer accuracy
I0502 17:32:43.757447 2050462464 net.cpp:380] accuracy <- fc8_VOC_fc8_VOC_0_split_0
I0502 17:32:43.757464 2050462464 net.cpp:380] accuracy <- label_data_1_split_0
I0502 17:32:43.757477 2050462464 net.cpp:338] accuracy -> accuracy
I0502 17:32:43.757489 2050462464 net.cpp:113] Setting up accuracy
I0502 17:32:43.757499 2050462464 net.cpp:120] Top shape: (1)
I0502 17:32:43.757508 2050462464 layer_factory.hpp:74] Creating layer loss
I0502 17:32:43.757519 2050462464 net.cpp:84] Creating Layer loss
I0502 17:32:43.757527 2050462464 net.cpp:380] loss <- fc8_VOC_fc8_VOC_0_split_1
I0502 17:32:43.757536 2050462464 net.cpp:380] loss <- label_data_1_split_1
I0502 17:32:43.757560 2050462464 net.cpp:338] loss -> loss
I0502 17:32:43.757580 2050462464 net.cpp:113] Setting up loss
I0502 17:32:43.757604 2050462464 layer_factory.hpp:74] Creating layer loss
I0502 17:32:43.757637 2050462464 net.cpp:120] Top shape: (1)
I0502 17:32:43.757649 2050462464 net.cpp:122]     with loss weight 1
I0502 17:32:43.757663 2050462464 net.cpp:167] loss needs backward computation.
I0502 17:32:43.757671 2050462464 net.cpp:169] accuracy does not need backward computation.
I0502 17:32:43.757679 2050462464 net.cpp:167] fc8_VOC_fc8_VOC_0_split needs backward computation.
I0502 17:32:43.757688 2050462464 net.cpp:167] fc8_VOC needs backward computation.
I0502 17:32:43.757695 2050462464 net.cpp:167] drop7 needs backward computation.
I0502 17:32:43.757704 2050462464 net.cpp:167] relu7 needs backward computation.
I0502 17:32:43.757737 2050462464 net.cpp:167] fc7 needs backward computation.
I0502 17:32:43.757755 2050462464 net.cpp:167] drop6 needs backward computation.
I0502 17:32:43.757769 2050462464 net.cpp:167] relu6 needs backward computation.
I0502 17:32:43.757774 2050462464 net.cpp:167] fc6 needs backward computation.
I0502 17:32:43.757779 2050462464 net.cpp:167] pool5 needs backward computation.
I0502 17:32:43.757784 2050462464 net.cpp:167] relu5 needs backward computation.
I0502 17:32:43.757815 2050462464 net.cpp:167] conv5 needs backward computation.
I0502 17:32:43.757824 2050462464 net.cpp:167] relu4 needs backward computation.
I0502 17:32:43.757833 2050462464 net.cpp:167] conv4 needs backward computation.
I0502 17:32:43.757838 2050462464 net.cpp:167] relu3 needs backward computation.
I0502 17:32:43.757845 2050462464 net.cpp:167] conv3 needs backward computation.
I0502 17:32:43.757850 2050462464 net.cpp:167] norm2 needs backward computation.
I0502 17:32:43.757855 2050462464 net.cpp:167] pool2 needs backward computation.
I0502 17:32:43.757860 2050462464 net.cpp:167] relu2 needs backward computation.
I0502 17:32:43.757868 2050462464 net.cpp:167] conv2 needs backward computation.
I0502 17:32:43.757874 2050462464 net.cpp:167] norm1 needs backward computation.
I0502 17:32:43.757880 2050462464 net.cpp:167] pool1 needs backward computation.
I0502 17:32:43.757887 2050462464 net.cpp:167] relu1 needs backward computation.
I0502 17:32:43.757895 2050462464 net.cpp:167] conv1 needs backward computation.
I0502 17:32:43.757905 2050462464 net.cpp:169] label_data_1_split does not need backward computation.
I0502 17:32:43.757913 2050462464 net.cpp:169] data does not need backward computation.
I0502 17:32:43.757921 2050462464 net.cpp:205] This network produces output accuracy
I0502 17:32:43.757930 2050462464 net.cpp:205] This network produces output loss
I0502 17:32:43.757959 2050462464 net.cpp:447] Collecting Learning Rate and Weight Decay.
I0502 17:32:43.757972 2050462464 net.cpp:217] Network initialization done.
I0502 17:32:43.757979 2050462464 net.cpp:218] Memory required for data: 439054344
I0502 17:32:43.758168 2050462464 solver.cpp:42] Solver scaffolding done.
I0502 17:32:43.758311 2050462464 caffe.cpp:86] Finetuning from bvlc_reference_rcnn_ilsvrc13.caffemodel
E0502 17:32:44.230825 2050462464 upgrade_proto.cpp:618] Attempting to upgrade input file specified using deprecated V1LayerParameter: bvlc_reference_rcnn_ilsvrc13.caffemodel
I0502 17:32:44.575809 2050462464 upgrade_proto.cpp:626] Successfully upgraded file specified using deprecated V1LayerParameter
E0502 17:32:45.145794 2050462464 upgrade_proto.cpp:618] Attempting to upgrade input file specified using deprecated V1LayerParameter: bvlc_reference_rcnn_ilsvrc13.caffemodel
I0502 17:32:45.437284 2050462464 upgrade_proto.cpp:626] Successfully upgraded file specified using deprecated V1LayerParameter
I0502 17:32:45.554304 2050462464 solver.cpp:222] Solving CaffeNet
I0502 17:32:45.554337 2050462464 solver.cpp:223] Learning Rate Policy: step
I0502 17:32:45.554353 2050462464 solver.cpp:266] Iteration 0, Testing net (#0)
I0502 18:09:10.468407 2050462464 solver.cpp:315]     Test net output #0: accuracy = 0.177922
I0502 18:09:10.468785 2050462464 solver.cpp:315]     Test net output #1: loss = 1.78901 (* 1 = 1.78901 loss)
I0502 18:09:16.241178 2050462464 solver.cpp:189] Iteration 0, loss = 1.85038
I0502 18:09:16.241220 2050462464 solver.cpp:204]     Train net output #0: loss = 1.85038 (* 1 = 1.85038 loss)
I0502 18:09:16.241241 2050462464 solver.cpp:464] Iteration 0, lr = 0.001
I0502 18:11:01.087688 2050462464 solver.cpp:189] Iteration 20, loss = 0.224877
I0502 18:11:01.087736 2050462464 solver.cpp:204]     Train net output #0: loss = 0.224877 (* 1 = 0.224877 loss)
I0502 18:11:01.087746 2050462464 solver.cpp:464] Iteration 20, lr = 0.001
I0502 18:12:43.353890 2050462464 solver.cpp:189] Iteration 40, loss = 0.359199
I0502 18:12:43.353940 2050462464 solver.cpp:204]     Train net output #0: loss = 0.359199 (* 1 = 0.359199 loss)
I0502 18:12:43.353953 2050462464 solver.cpp:464] Iteration 40, lr = 0.001
I0502 18:14:26.757253 2050462464 solver.cpp:189] Iteration 60, loss = 0.134948
I0502 18:14:26.757302 2050462464 solver.cpp:204]     Train net output #0: loss = 0.134948 (* 1 = 0.134948 loss)
I0502 18:14:26.757318 2050462464 solver.cpp:464] Iteration 60, lr = 0.001
I0502 18:16:08.927139 2050462464 solver.cpp:189] Iteration 80, loss = 0.313918
I0502 18:16:08.927217 2050462464 solver.cpp:204]     Train net output #0: loss = 0.313918 (* 1 = 0.313918 loss)
I0502 18:16:08.927227 2050462464 solver.cpp:464] Iteration 80, lr = 0.001
I0502 18:17:52.313988 2050462464 solver.cpp:189] Iteration 100, loss = 0.149848
I0502 18:17:52.314041 2050462464 solver.cpp:204]     Train net output #0: loss = 0.149848 (* 1 = 0.149848 loss)
I0502 18:17:52.314069 2050462464 solver.cpp:464] Iteration 100, lr = 0.001
I0502 18:19:32.209561 2050462464 solver.cpp:189] Iteration 120, loss = 0.298938
I0502 18:19:32.209609 2050462464 solver.cpp:204]     Train net output #0: loss = 0.298938 (* 1 = 0.298938 loss)
I0502 18:19:32.209619 2050462464 solver.cpp:464] Iteration 120, lr = 0.001
I0502 18:21:15.845937 2050462464 solver.cpp:189] Iteration 140, loss = 0.251731
I0502 18:21:15.845983 2050462464 solver.cpp:204]     Train net output #0: loss = 0.251731 (* 1 = 0.251731 loss)
I0502 18:21:15.845993 2050462464 solver.cpp:464] Iteration 140, lr = 0.001
I0502 18:22:57.629355 2050462464 solver.cpp:189] Iteration 160, loss = 0.40438
I0502 18:22:57.629400 2050462464 solver.cpp:204]     Train net output #0: loss = 0.40438 (* 1 = 0.40438 loss)
I0502 18:22:57.629410 2050462464 solver.cpp:464] Iteration 160, lr = 0.001
I0502 18:24:41.355926 2050462464 solver.cpp:189] Iteration 180, loss = 0.1661
I0502 18:24:41.356220 2050462464 solver.cpp:204]     Train net output #0: loss = 0.1661 (* 1 = 0.1661 loss)
I0502 18:24:41.356233 2050462464 solver.cpp:464] Iteration 180, lr = 0.001
I0502 18:26:22.517740 2050462464 solver.cpp:189] Iteration 200, loss = 0.348071
I0502 18:26:22.517789 2050462464 solver.cpp:204]     Train net output #0: loss = 0.348071 (* 1 = 0.348071 loss)
I0502 18:26:22.517798 2050462464 solver.cpp:464] Iteration 200, lr = 0.001
I0502 18:28:05.578080 2050462464 solver.cpp:189] Iteration 220, loss = 0.325989
I0502 18:28:05.578136 2050462464 solver.cpp:204]     Train net output #0: loss = 0.325989 (* 1 = 0.325989 loss)
I0502 18:28:05.578152 2050462464 solver.cpp:464] Iteration 220, lr = 0.001
I0502 18:29:46.771296 2050462464 solver.cpp:189] Iteration 240, loss = 0.1915
I0502 18:29:46.771559 2050462464 solver.cpp:204]     Train net output #0: loss = 0.1915 (* 1 = 0.1915 loss)
I0502 18:29:46.771572 2050462464 solver.cpp:464] Iteration 240, lr = 0.001
I0502 18:31:30.870753 2050462464 solver.cpp:189] Iteration 260, loss = 0.291008
I0502 18:31:30.871026 2050462464 solver.cpp:204]     Train net output #0: loss = 0.291008 (* 1 = 0.291008 loss)
I0502 18:31:30.871043 2050462464 solver.cpp:464] Iteration 260, lr = 0.001
I0502 18:33:14.674557 2050462464 solver.cpp:189] Iteration 280, loss = 0.240502
I0502 18:33:14.674800 2050462464 solver.cpp:204]     Train net output #0: loss = 0.240502 (* 1 = 0.240502 loss)
I0502 18:33:14.674813 2050462464 solver.cpp:464] Iteration 280, lr = 0.001
I0502 18:34:54.516091 2050462464 solver.cpp:189] Iteration 300, loss = 0.214563
I0502 18:34:54.516139 2050462464 solver.cpp:204]     Train net output #0: loss = 0.214563 (* 1 = 0.214563 loss)
I0502 18:34:54.516149 2050462464 solver.cpp:464] Iteration 300, lr = 0.001
I0502 18:36:34.982415 2050462464 solver.cpp:189] Iteration 320, loss = 0.136705
I0502 18:36:34.982466 2050462464 solver.cpp:204]     Train net output #0: loss = 0.136705 (* 1 = 0.136705 loss)
I0502 18:36:34.982477 2050462464 solver.cpp:464] Iteration 320, lr = 0.001
I0502 18:38:12.830673 2050462464 solver.cpp:189] Iteration 340, loss = 0.060831
I0502 18:38:12.830724 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0608309 (* 1 = 0.0608309 loss)
I0502 18:38:12.830737 2050462464 solver.cpp:464] Iteration 340, lr = 0.001
I0502 18:39:50.644202 2050462464 solver.cpp:189] Iteration 360, loss = 0.118224
I0502 18:39:50.644248 2050462464 solver.cpp:204]     Train net output #0: loss = 0.118224 (* 1 = 0.118224 loss)
I0502 18:39:50.644258 2050462464 solver.cpp:464] Iteration 360, lr = 0.001
I0502 18:41:28.288624 2050462464 solver.cpp:189] Iteration 380, loss = 0.0592902
I0502 18:41:28.288671 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0592902 (* 1 = 0.0592902 loss)
I0502 18:41:28.288681 2050462464 solver.cpp:464] Iteration 380, lr = 0.001
I0502 18:43:04.940409 2050462464 solver.cpp:189] Iteration 400, loss = 0.298344
I0502 18:43:04.940469 2050462464 solver.cpp:204]     Train net output #0: loss = 0.298344 (* 1 = 0.298344 loss)
I0502 18:43:04.940479 2050462464 solver.cpp:464] Iteration 400, lr = 0.001
I0502 18:44:42.700829 2050462464 solver.cpp:189] Iteration 420, loss = 0.129057
I0502 18:44:42.700875 2050462464 solver.cpp:204]     Train net output #0: loss = 0.129057 (* 1 = 0.129057 loss)
I0502 18:44:42.700883 2050462464 solver.cpp:464] Iteration 420, lr = 0.001
I0502 18:46:22.515805 2050462464 solver.cpp:189] Iteration 440, loss = 0.36142
I0502 18:46:22.515930 2050462464 solver.cpp:204]     Train net output #0: loss = 0.36142 (* 1 = 0.36142 loss)
I0502 18:46:22.515944 2050462464 solver.cpp:464] Iteration 440, lr = 0.001
I0502 18:48:01.115205 2050462464 solver.cpp:189] Iteration 460, loss = 0.188072
I0502 18:48:01.115257 2050462464 solver.cpp:204]     Train net output #0: loss = 0.188072 (* 1 = 0.188072 loss)
I0502 18:48:01.115269 2050462464 solver.cpp:464] Iteration 460, lr = 0.001
I0502 18:49:39.376822 2050462464 solver.cpp:189] Iteration 480, loss = 0.138875
I0502 18:49:39.376865 2050462464 solver.cpp:204]     Train net output #0: loss = 0.138875 (* 1 = 0.138875 loss)
I0502 18:49:39.376875 2050462464 solver.cpp:464] Iteration 480, lr = 0.001
I0502 18:51:13.289445 2050462464 solver.cpp:334] Snapshotting to caffenet_train_background_iter_500.caffemodel
I0502 18:51:14.883227 2050462464 solver.cpp:342] Snapshotting solver state to caffenet_train_background_iter_500.solverstate
I0502 18:51:20.858403 2050462464 solver.cpp:189] Iteration 500, loss = 0.123763
I0502 18:51:20.858438 2050462464 solver.cpp:204]     Train net output #0: loss = 0.123763 (* 1 = 0.123763 loss)
I0502 18:51:20.858448 2050462464 solver.cpp:464] Iteration 500, lr = 0.001
I0502 18:52:59.240730 2050462464 solver.cpp:189] Iteration 520, loss = 0.0633292
I0502 18:52:59.240774 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0633291 (* 1 = 0.0633291 loss)
I0502 18:52:59.240784 2050462464 solver.cpp:464] Iteration 520, lr = 0.001
I0502 18:54:37.423255 2050462464 solver.cpp:189] Iteration 540, loss = 0.146624
I0502 18:54:37.423305 2050462464 solver.cpp:204]     Train net output #0: loss = 0.146624 (* 1 = 0.146624 loss)
I0502 18:54:37.423354 2050462464 solver.cpp:464] Iteration 540, lr = 0.001
I0502 18:56:15.908210 2050462464 solver.cpp:189] Iteration 560, loss = 0.128356
I0502 18:56:15.908257 2050462464 solver.cpp:204]     Train net output #0: loss = 0.128356 (* 1 = 0.128356 loss)
I0502 18:56:15.908267 2050462464 solver.cpp:464] Iteration 560, lr = 0.001
I0502 18:57:54.316897 2050462464 solver.cpp:189] Iteration 580, loss = 0.183151
I0502 18:57:54.316947 2050462464 solver.cpp:204]     Train net output #0: loss = 0.183151 (* 1 = 0.183151 loss)
I0502 18:57:54.316958 2050462464 solver.cpp:464] Iteration 580, lr = 0.001
I0502 18:59:32.237020 2050462464 solver.cpp:189] Iteration 600, loss = 0.126215
I0502 18:59:32.237071 2050462464 solver.cpp:204]     Train net output #0: loss = 0.126215 (* 1 = 0.126215 loss)
I0502 18:59:32.237085 2050462464 solver.cpp:464] Iteration 600, lr = 0.001
I0502 19:01:10.252801 2050462464 solver.cpp:189] Iteration 620, loss = 0.0491153
I0502 19:01:10.252845 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0491153 (* 1 = 0.0491153 loss)
I0502 19:01:10.252854 2050462464 solver.cpp:464] Iteration 620, lr = 0.001
I0502 19:02:47.929688 2050462464 solver.cpp:189] Iteration 640, loss = 0.0763443
I0502 19:02:47.929733 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0763442 (* 1 = 0.0763442 loss)
I0502 19:02:47.929743 2050462464 solver.cpp:464] Iteration 640, lr = 0.001
I0502 19:04:25.743453 2050462464 solver.cpp:189] Iteration 660, loss = 0.0749011
I0502 19:04:25.743496 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0749011 (* 1 = 0.0749011 loss)
I0502 19:04:25.743507 2050462464 solver.cpp:464] Iteration 660, lr = 0.001
I0502 19:06:04.398447 2050462464 solver.cpp:189] Iteration 680, loss = 0.028991
I0502 19:06:04.398737 2050462464 solver.cpp:204]     Train net output #0: loss = 0.028991 (* 1 = 0.028991 loss)
I0502 19:06:04.398751 2050462464 solver.cpp:464] Iteration 680, lr = 0.001
I0502 19:07:42.066196 2050462464 solver.cpp:189] Iteration 700, loss = 0.217958
I0502 19:07:42.066242 2050462464 solver.cpp:204]     Train net output #0: loss = 0.217958 (* 1 = 0.217958 loss)
I0502 19:07:42.066252 2050462464 solver.cpp:464] Iteration 700, lr = 0.001
I0502 19:09:20.688336 2050462464 solver.cpp:189] Iteration 720, loss = 0.162858
I0502 19:09:20.688385 2050462464 solver.cpp:204]     Train net output #0: loss = 0.162858 (* 1 = 0.162858 loss)
I0502 19:09:20.688395 2050462464 solver.cpp:464] Iteration 720, lr = 0.001
I0502 19:11:00.217676 2050462464 solver.cpp:189] Iteration 740, loss = 0.0703356
I0502 19:11:00.217722 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0703355 (* 1 = 0.0703355 loss)
I0502 19:11:00.217732 2050462464 solver.cpp:464] Iteration 740, lr = 0.001
I0502 19:12:38.745419 2050462464 solver.cpp:189] Iteration 760, loss = 0.263255
I0502 19:12:38.745467 2050462464 solver.cpp:204]     Train net output #0: loss = 0.263255 (* 1 = 0.263255 loss)
I0502 19:12:38.745477 2050462464 solver.cpp:464] Iteration 760, lr = 0.001
I0502 19:14:17.016226 2050462464 solver.cpp:189] Iteration 780, loss = 0.113691
I0502 19:14:17.016273 2050462464 solver.cpp:204]     Train net output #0: loss = 0.113691 (* 1 = 0.113691 loss)
I0502 19:14:17.016283 2050462464 solver.cpp:464] Iteration 780, lr = 0.001
I0502 19:15:54.827150 2050462464 solver.cpp:189] Iteration 800, loss = 0.137293
I0502 19:15:54.827203 2050462464 solver.cpp:204]     Train net output #0: loss = 0.137293 (* 1 = 0.137293 loss)
I0502 19:15:54.827216 2050462464 solver.cpp:464] Iteration 800, lr = 0.001
I0502 19:17:33.302721 2050462464 solver.cpp:189] Iteration 820, loss = 0.120539
I0502 19:17:33.302768 2050462464 solver.cpp:204]     Train net output #0: loss = 0.120539 (* 1 = 0.120539 loss)
I0502 19:17:33.302778 2050462464 solver.cpp:464] Iteration 820, lr = 0.001
I0502 19:19:12.213673 2050462464 solver.cpp:189] Iteration 840, loss = 0.0616443
I0502 19:19:12.213717 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0616443 (* 1 = 0.0616443 loss)
I0502 19:19:12.213727 2050462464 solver.cpp:464] Iteration 840, lr = 0.001
I0502 19:20:50.395627 2050462464 solver.cpp:189] Iteration 860, loss = 0.137223
I0502 19:20:50.395673 2050462464 solver.cpp:204]     Train net output #0: loss = 0.137223 (* 1 = 0.137223 loss)
I0502 19:20:50.395683 2050462464 solver.cpp:464] Iteration 860, lr = 0.001
I0502 19:22:29.320404 2050462464 solver.cpp:189] Iteration 880, loss = 0.140023
I0502 19:22:29.320453 2050462464 solver.cpp:204]     Train net output #0: loss = 0.140023 (* 1 = 0.140023 loss)
I0502 19:22:29.320467 2050462464 solver.cpp:464] Iteration 880, lr = 0.001
I0502 19:24:08.127560 2050462464 solver.cpp:189] Iteration 900, loss = 0.235001
I0502 19:24:08.127616 2050462464 solver.cpp:204]     Train net output #0: loss = 0.235001 (* 1 = 0.235001 loss)
I0502 19:24:08.127631 2050462464 solver.cpp:464] Iteration 900, lr = 0.001
I0502 19:25:46.526422 2050462464 solver.cpp:189] Iteration 920, loss = 0.19294
I0502 19:25:46.526504 2050462464 solver.cpp:204]     Train net output #0: loss = 0.19294 (* 1 = 0.19294 loss)
I0502 19:25:46.526520 2050462464 solver.cpp:464] Iteration 920, lr = 0.001
I0502 19:27:25.561796 2050462464 solver.cpp:189] Iteration 940, loss = 0.0930335
I0502 19:27:25.562240 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0930336 (* 1 = 0.0930336 loss)
I0502 19:27:25.562258 2050462464 solver.cpp:464] Iteration 940, lr = 0.001
I0502 19:29:04.255224 2050462464 solver.cpp:189] Iteration 960, loss = 0.251921
I0502 19:29:04.255271 2050462464 solver.cpp:204]     Train net output #0: loss = 0.251921 (* 1 = 0.251921 loss)
I0502 19:29:04.255280 2050462464 solver.cpp:464] Iteration 960, lr = 0.001
I0502 19:30:42.581634 2050462464 solver.cpp:189] Iteration 980, loss = 0.288991
I0502 19:30:42.581697 2050462464 solver.cpp:204]     Train net output #0: loss = 0.288991 (* 1 = 0.288991 loss)
I0502 19:30:42.581712 2050462464 solver.cpp:464] Iteration 980, lr = 0.001
I0502 19:32:17.010416 2050462464 solver.cpp:334] Snapshotting to caffenet_train_background_iter_1000.caffemodel
I0502 19:32:18.752444 2050462464 solver.cpp:342] Snapshotting solver state to caffenet_train_background_iter_1000.solverstate
I0502 19:32:20.163259 2050462464 solver.cpp:266] Iteration 1000, Testing net (#0)
I0502 20:04:58.539232 2050462464 solver.cpp:315]     Test net output #0: accuracy = 0.957766
I0502 20:04:58.539639 2050462464 solver.cpp:315]     Test net output #1: loss = 0.134262 (* 1 = 0.134262 loss)
I0502 20:05:03.479136 2050462464 solver.cpp:189] Iteration 1000, loss = 0.237426
I0502 20:05:03.479172 2050462464 solver.cpp:204]     Train net output #0: loss = 0.237426 (* 1 = 0.237426 loss)
I0502 20:05:03.479182 2050462464 solver.cpp:464] Iteration 1000, lr = 0.001
I0502 20:06:43.853883 2050462464 solver.cpp:189] Iteration 1020, loss = 0.0832768
I0502 20:06:43.853929 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0832768 (* 1 = 0.0832768 loss)
I0502 20:06:43.853940 2050462464 solver.cpp:464] Iteration 1020, lr = 0.001
I0502 20:08:23.635468 2050462464 solver.cpp:189] Iteration 1040, loss = 0.0332939
I0502 20:08:23.635516 2050462464 solver.cpp:204]     Train net output #0: loss = 0.033294 (* 1 = 0.033294 loss)
I0502 20:08:23.635525 2050462464 solver.cpp:464] Iteration 1040, lr = 0.001
I0502 20:10:03.402271 2050462464 solver.cpp:189] Iteration 1060, loss = 0.0317596
I0502 20:10:03.402324 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0317596 (* 1 = 0.0317596 loss)
I0502 20:10:03.402338 2050462464 solver.cpp:464] Iteration 1060, lr = 0.001
I0502 20:11:43.102921 2050462464 solver.cpp:189] Iteration 1080, loss = 0.190918
I0502 20:11:43.102967 2050462464 solver.cpp:204]     Train net output #0: loss = 0.190918 (* 1 = 0.190918 loss)
I0502 20:11:43.102977 2050462464 solver.cpp:464] Iteration 1080, lr = 0.001
I0502 20:13:23.227077 2050462464 solver.cpp:189] Iteration 1100, loss = 0.146333
I0502 20:13:23.227123 2050462464 solver.cpp:204]     Train net output #0: loss = 0.146333 (* 1 = 0.146333 loss)
I0502 20:13:23.227133 2050462464 solver.cpp:464] Iteration 1100, lr = 0.001
I0502 20:15:01.884376 2050462464 solver.cpp:189] Iteration 1120, loss = 0.0588711
I0502 20:15:01.884424 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0588711 (* 1 = 0.0588711 loss)
I0502 20:15:01.884435 2050462464 solver.cpp:464] Iteration 1120, lr = 0.001
I0502 20:16:40.378590 2050462464 solver.cpp:189] Iteration 1140, loss = 0.0457776
I0502 20:16:40.378646 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0457777 (* 1 = 0.0457777 loss)
I0502 20:16:40.378657 2050462464 solver.cpp:464] Iteration 1140, lr = 0.001
I0502 20:18:19.980056 2050462464 solver.cpp:189] Iteration 1160, loss = 0.208932
I0502 20:18:19.980104 2050462464 solver.cpp:204]     Train net output #0: loss = 0.208932 (* 1 = 0.208932 loss)
I0502 20:18:19.980114 2050462464 solver.cpp:464] Iteration 1160, lr = 0.001
I0502 20:19:58.454053 2050462464 solver.cpp:189] Iteration 1180, loss = 0.0820361
I0502 20:19:58.454102 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0820361 (* 1 = 0.0820361 loss)
I0502 20:19:58.454112 2050462464 solver.cpp:464] Iteration 1180, lr = 0.001
I0502 20:21:37.465229 2050462464 solver.cpp:189] Iteration 1200, loss = 0.0669426
I0502 20:21:37.465276 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0669426 (* 1 = 0.0669426 loss)
I0502 20:21:37.465288 2050462464 solver.cpp:464] Iteration 1200, lr = 0.001
I0502 20:23:16.644208 2050462464 solver.cpp:189] Iteration 1220, loss = 0.08196
I0502 20:23:16.644259 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0819601 (* 1 = 0.0819601 loss)
I0502 20:23:16.644270 2050462464 solver.cpp:464] Iteration 1220, lr = 0.001
I0502 20:24:55.130396 2050462464 solver.cpp:189] Iteration 1240, loss = 0.110455
I0502 20:24:55.130460 2050462464 solver.cpp:204]     Train net output #0: loss = 0.110455 (* 1 = 0.110455 loss)
I0502 20:24:55.130473 2050462464 solver.cpp:464] Iteration 1240, lr = 0.001
I0502 20:26:33.401625 2050462464 solver.cpp:189] Iteration 1260, loss = 0.23088
I0502 20:26:33.401681 2050462464 solver.cpp:204]     Train net output #0: loss = 0.23088 (* 1 = 0.23088 loss)
I0502 20:26:33.401690 2050462464 solver.cpp:464] Iteration 1260, lr = 0.001
I0502 20:28:12.226979 2050462464 solver.cpp:189] Iteration 1280, loss = 0.105276
I0502 20:28:12.227020 2050462464 solver.cpp:204]     Train net output #0: loss = 0.105276 (* 1 = 0.105276 loss)
I0502 20:28:12.227030 2050462464 solver.cpp:464] Iteration 1280, lr = 0.001
I0502 20:29:49.174737 2050462464 solver.cpp:189] Iteration 1300, loss = 0.101572
I0502 20:29:49.174785 2050462464 solver.cpp:204]     Train net output #0: loss = 0.101572 (* 1 = 0.101572 loss)
I0502 20:29:49.174794 2050462464 solver.cpp:464] Iteration 1300, lr = 0.001
I0502 20:31:24.789679 2050462464 solver.cpp:189] Iteration 1320, loss = 0.0956116
I0502 20:31:24.789726 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0956117 (* 1 = 0.0956117 loss)
I0502 20:31:24.789736 2050462464 solver.cpp:464] Iteration 1320, lr = 0.001
I0502 20:33:00.792325 2050462464 solver.cpp:189] Iteration 1340, loss = 0.0243186
I0502 20:33:00.792373 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0243186 (* 1 = 0.0243186 loss)
I0502 20:33:00.792388 2050462464 solver.cpp:464] Iteration 1340, lr = 0.001
I0502 20:34:35.201354 2050462464 solver.cpp:189] Iteration 1360, loss = 0.245934
I0502 20:34:35.201422 2050462464 solver.cpp:204]     Train net output #0: loss = 0.245934 (* 1 = 0.245934 loss)
I0502 20:34:35.201436 2050462464 solver.cpp:464] Iteration 1360, lr = 0.001
I0502 20:36:09.417464 2050462464 solver.cpp:189] Iteration 1380, loss = 0.0233203
I0502 20:36:09.417512 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0233203 (* 1 = 0.0233203 loss)
I0502 20:36:09.417523 2050462464 solver.cpp:464] Iteration 1380, lr = 0.001
I0502 20:37:43.567960 2050462464 solver.cpp:189] Iteration 1400, loss = 0.00774149
I0502 20:37:43.568011 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00774153 (* 1 = 0.00774153 loss)
I0502 20:37:43.568020 2050462464 solver.cpp:464] Iteration 1400, lr = 0.001
I0502 20:39:19.085994 2050462464 solver.cpp:189] Iteration 1420, loss = 0.0406763
I0502 20:39:19.086284 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0406763 (* 1 = 0.0406763 loss)
I0502 20:39:19.086297 2050462464 solver.cpp:464] Iteration 1420, lr = 0.001
I0502 20:40:52.962723 2050462464 solver.cpp:189] Iteration 1440, loss = 0.0980662
I0502 20:40:52.962771 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0980662 (* 1 = 0.0980662 loss)
I0502 20:40:52.962782 2050462464 solver.cpp:464] Iteration 1440, lr = 0.001
I0502 20:42:27.215034 2050462464 solver.cpp:189] Iteration 1460, loss = 0.114652
I0502 20:42:27.215087 2050462464 solver.cpp:204]     Train net output #0: loss = 0.114652 (* 1 = 0.114652 loss)
I0502 20:42:27.215096 2050462464 solver.cpp:464] Iteration 1460, lr = 0.001
I0502 20:44:00.245195 2050462464 solver.cpp:189] Iteration 1480, loss = 0.211149
I0502 20:44:00.245244 2050462464 solver.cpp:204]     Train net output #0: loss = 0.211149 (* 1 = 0.211149 loss)
I0502 20:44:00.245254 2050462464 solver.cpp:464] Iteration 1480, lr = 0.001
I0502 20:45:28.965735 2050462464 solver.cpp:334] Snapshotting to caffenet_train_background_iter_1500.caffemodel
I0502 20:45:30.672817 2050462464 solver.cpp:342] Snapshotting solver state to caffenet_train_background_iter_1500.solverstate
I0502 20:45:36.437427 2050462464 solver.cpp:189] Iteration 1500, loss = 0.0169303
I0502 20:45:36.437465 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0169304 (* 1 = 0.0169304 loss)
I0502 20:45:36.437480 2050462464 solver.cpp:464] Iteration 1500, lr = 0.001
I0502 20:47:11.040563 2050462464 solver.cpp:189] Iteration 1520, loss = 0.171103
I0502 20:47:11.040711 2050462464 solver.cpp:204]     Train net output #0: loss = 0.171103 (* 1 = 0.171103 loss)
I0502 20:47:11.040731 2050462464 solver.cpp:464] Iteration 1520, lr = 0.001
I0502 20:48:42.298580 2050462464 solver.cpp:189] Iteration 1540, loss = 0.044926
I0502 20:48:42.298651 2050462464 solver.cpp:204]     Train net output #0: loss = 0.044926 (* 1 = 0.044926 loss)
I0502 20:48:42.298661 2050462464 solver.cpp:464] Iteration 1540, lr = 0.001
I0502 20:50:04.939188 2050462464 solver.cpp:189] Iteration 1560, loss = 0.0242728
I0502 20:50:04.939244 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0242728 (* 1 = 0.0242728 loss)
I0502 20:50:04.939254 2050462464 solver.cpp:464] Iteration 1560, lr = 0.001
I0502 20:51:28.553434 2050462464 solver.cpp:189] Iteration 1580, loss = 0.145073
I0502 20:51:28.553490 2050462464 solver.cpp:204]     Train net output #0: loss = 0.145073 (* 1 = 0.145073 loss)
I0502 20:51:28.553499 2050462464 solver.cpp:464] Iteration 1580, lr = 0.001
I0502 20:52:51.104177 2050462464 solver.cpp:189] Iteration 1600, loss = 0.169171
I0502 20:52:51.104233 2050462464 solver.cpp:204]     Train net output #0: loss = 0.169171 (* 1 = 0.169171 loss)
I0502 20:52:51.104243 2050462464 solver.cpp:464] Iteration 1600, lr = 0.001
I0502 20:54:14.208164 2050462464 solver.cpp:189] Iteration 1620, loss = 0.0304572
I0502 20:54:14.208219 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0304572 (* 1 = 0.0304572 loss)
I0502 20:54:14.208227 2050462464 solver.cpp:464] Iteration 1620, lr = 0.001
I0502 20:55:37.184881 2050462464 solver.cpp:189] Iteration 1640, loss = 0.188804
I0502 20:55:37.184929 2050462464 solver.cpp:204]     Train net output #0: loss = 0.188804 (* 1 = 0.188804 loss)
I0502 20:55:37.184939 2050462464 solver.cpp:464] Iteration 1640, lr = 0.001
I0502 20:57:00.214916 2050462464 solver.cpp:189] Iteration 1660, loss = 0.138145
I0502 20:57:00.214972 2050462464 solver.cpp:204]     Train net output #0: loss = 0.138145 (* 1 = 0.138145 loss)
I0502 20:57:00.214980 2050462464 solver.cpp:464] Iteration 1660, lr = 0.001
I0502 20:58:23.380962 2050462464 solver.cpp:189] Iteration 1680, loss = 0.0425591
I0502 20:58:23.381017 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0425591 (* 1 = 0.0425591 loss)
I0502 20:58:23.381028 2050462464 solver.cpp:464] Iteration 1680, lr = 0.001
I0502 20:59:46.476775 2050462464 solver.cpp:189] Iteration 1700, loss = 0.0442964
I0502 20:59:46.476824 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0442964 (* 1 = 0.0442964 loss)
I0502 20:59:46.476833 2050462464 solver.cpp:464] Iteration 1700, lr = 0.001
I0502 21:01:09.481070 2050462464 solver.cpp:189] Iteration 1720, loss = 0.0132363
I0502 21:01:09.481120 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0132363 (* 1 = 0.0132363 loss)
I0502 21:01:09.481130 2050462464 solver.cpp:464] Iteration 1720, lr = 0.001
I0502 21:02:32.927819 2050462464 solver.cpp:189] Iteration 1740, loss = 0.150518
I0502 21:02:32.927875 2050462464 solver.cpp:204]     Train net output #0: loss = 0.150518 (* 1 = 0.150518 loss)
I0502 21:02:32.927883 2050462464 solver.cpp:464] Iteration 1740, lr = 0.001
I0502 21:03:55.845449 2050462464 solver.cpp:189] Iteration 1760, loss = 0.00875115
I0502 21:03:55.845501 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00875115 (* 1 = 0.00875115 loss)
I0502 21:03:55.845511 2050462464 solver.cpp:464] Iteration 1760, lr = 0.001
I0502 21:05:19.068640 2050462464 solver.cpp:189] Iteration 1780, loss = 0.00415348
I0502 21:05:19.068689 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00415347 (* 1 = 0.00415347 loss)
I0502 21:05:19.068699 2050462464 solver.cpp:464] Iteration 1780, lr = 0.001
I0502 21:06:42.109668 2050462464 solver.cpp:189] Iteration 1800, loss = 0.0476491
I0502 21:06:42.109719 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0476491 (* 1 = 0.0476491 loss)
I0502 21:06:42.109727 2050462464 solver.cpp:464] Iteration 1800, lr = 0.001
I0502 21:08:05.393674 2050462464 solver.cpp:189] Iteration 1820, loss = 0.164204
I0502 21:08:05.393730 2050462464 solver.cpp:204]     Train net output #0: loss = 0.164204 (* 1 = 0.164204 loss)
I0502 21:08:05.393739 2050462464 solver.cpp:464] Iteration 1820, lr = 0.001
I0502 21:09:28.287974 2050462464 solver.cpp:189] Iteration 1840, loss = 0.113424
I0502 21:09:28.288039 2050462464 solver.cpp:204]     Train net output #0: loss = 0.113424 (* 1 = 0.113424 loss)
I0502 21:09:28.288049 2050462464 solver.cpp:464] Iteration 1840, lr = 0.001
I0502 21:10:51.321579 2050462464 solver.cpp:189] Iteration 1860, loss = 0.133967
I0502 21:10:51.321631 2050462464 solver.cpp:204]     Train net output #0: loss = 0.133967 (* 1 = 0.133967 loss)
I0502 21:10:51.321640 2050462464 solver.cpp:464] Iteration 1860, lr = 0.001
I0502 21:12:14.376600 2050462464 solver.cpp:189] Iteration 1880, loss = 0.021683
I0502 21:12:14.376651 2050462464 solver.cpp:204]     Train net output #0: loss = 0.021683 (* 1 = 0.021683 loss)
I0502 21:12:14.376660 2050462464 solver.cpp:464] Iteration 1880, lr = 0.001
I0502 21:13:37.066095 2050462464 solver.cpp:189] Iteration 1900, loss = 0.12315
I0502 21:13:37.066148 2050462464 solver.cpp:204]     Train net output #0: loss = 0.12315 (* 1 = 0.12315 loss)
I0502 21:13:37.066156 2050462464 solver.cpp:464] Iteration 1900, lr = 0.001
I0502 21:14:59.927353 2050462464 solver.cpp:189] Iteration 1920, loss = 0.14237
I0502 21:14:59.927408 2050462464 solver.cpp:204]     Train net output #0: loss = 0.14237 (* 1 = 0.14237 loss)
I0502 21:14:59.927418 2050462464 solver.cpp:464] Iteration 1920, lr = 0.001
I0502 21:16:23.143009 2050462464 solver.cpp:189] Iteration 1940, loss = 0.0202138
I0502 21:16:23.143060 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0202138 (* 1 = 0.0202138 loss)
I0502 21:16:23.143070 2050462464 solver.cpp:464] Iteration 1940, lr = 0.001
I0502 21:17:46.235507 2050462464 solver.cpp:189] Iteration 1960, loss = 0.00880835
I0502 21:17:46.235558 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00880834 (* 1 = 0.00880834 loss)
I0502 21:17:46.235568 2050462464 solver.cpp:464] Iteration 1960, lr = 0.001
I0502 21:19:09.041281 2050462464 solver.cpp:189] Iteration 1980, loss = 0.080405
I0502 21:19:09.041337 2050462464 solver.cpp:204]     Train net output #0: loss = 0.080405 (* 1 = 0.080405 loss)
I0502 21:19:09.041347 2050462464 solver.cpp:464] Iteration 1980, lr = 0.001
I0502 21:20:28.420053 2050462464 solver.cpp:334] Snapshotting to caffenet_train_background_iter_2000.caffemodel
I0502 21:20:29.990774 2050462464 solver.cpp:342] Snapshotting solver state to caffenet_train_background_iter_2000.solverstate
I0502 21:20:31.204084 2050462464 solver.cpp:266] Iteration 2000, Testing net (#0)
I0502 21:48:02.709605 2050462464 solver.cpp:315]     Test net output #0: accuracy = 0.966297
I0502 21:48:02.710441 2050462464 solver.cpp:315]     Test net output #1: loss = 0.104098 (* 1 = 0.104098 loss)
I0502 21:48:06.623776 2050462464 solver.cpp:189] Iteration 2000, loss = 0.0449403
I0502 21:48:06.623811 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0449403 (* 1 = 0.0449403 loss)
I0502 21:48:06.623819 2050462464 solver.cpp:464] Iteration 2000, lr = 0.001
I0502 21:49:29.588068 2050462464 solver.cpp:189] Iteration 2020, loss = 0.0697617
I0502 21:49:29.588116 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0697617 (* 1 = 0.0697617 loss)
I0502 21:49:29.588126 2050462464 solver.cpp:464] Iteration 2020, lr = 0.001
I0502 21:50:52.507474 2050462464 solver.cpp:189] Iteration 2040, loss = 0.0537334
I0502 21:50:52.507524 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0537334 (* 1 = 0.0537334 loss)
I0502 21:50:52.507532 2050462464 solver.cpp:464] Iteration 2040, lr = 0.001
I0502 21:52:15.714354 2050462464 solver.cpp:189] Iteration 2060, loss = 0.131734
I0502 21:52:15.714404 2050462464 solver.cpp:204]     Train net output #0: loss = 0.131734 (* 1 = 0.131734 loss)
I0502 21:52:15.714413 2050462464 solver.cpp:464] Iteration 2060, lr = 0.001
I0502 21:53:38.583822 2050462464 solver.cpp:189] Iteration 2080, loss = 0.100788
I0502 21:53:38.583878 2050462464 solver.cpp:204]     Train net output #0: loss = 0.100788 (* 1 = 0.100788 loss)
I0502 21:53:38.583886 2050462464 solver.cpp:464] Iteration 2080, lr = 0.001
I0502 21:55:01.150615 2050462464 solver.cpp:189] Iteration 2100, loss = 0.0154012
I0502 21:55:01.150692 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0154012 (* 1 = 0.0154012 loss)
I0502 21:55:01.150702 2050462464 solver.cpp:464] Iteration 2100, lr = 0.001
I0502 21:56:24.379467 2050462464 solver.cpp:189] Iteration 2120, loss = 0.126998
I0502 21:56:24.379521 2050462464 solver.cpp:204]     Train net output #0: loss = 0.126998 (* 1 = 0.126998 loss)
I0502 21:56:24.379531 2050462464 solver.cpp:464] Iteration 2120, lr = 0.001
I0502 21:57:47.430019 2050462464 solver.cpp:189] Iteration 2140, loss = 0.0678845
I0502 21:57:47.430066 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0678845 (* 1 = 0.0678845 loss)
I0502 21:57:47.430076 2050462464 solver.cpp:464] Iteration 2140, lr = 0.001
I0502 21:59:10.833665 2050462464 solver.cpp:189] Iteration 2160, loss = 0.112353
I0502 21:59:10.833716 2050462464 solver.cpp:204]     Train net output #0: loss = 0.112353 (* 1 = 0.112353 loss)
I0502 21:59:10.833725 2050462464 solver.cpp:464] Iteration 2160, lr = 0.001
I0502 22:00:33.847257 2050462464 solver.cpp:189] Iteration 2180, loss = 0.0961517
I0502 22:00:33.847314 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0961517 (* 1 = 0.0961517 loss)
I0502 22:00:33.847323 2050462464 solver.cpp:464] Iteration 2180, lr = 0.001
I0502 22:01:57.082273 2050462464 solver.cpp:189] Iteration 2200, loss = 0.174818
I0502 22:01:57.082324 2050462464 solver.cpp:204]     Train net output #0: loss = 0.174818 (* 1 = 0.174818 loss)
I0502 22:01:57.082332 2050462464 solver.cpp:464] Iteration 2200, lr = 0.001
I0502 22:03:20.268728 2050462464 solver.cpp:189] Iteration 2220, loss = 0.0641523
I0502 22:03:20.268784 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0641523 (* 1 = 0.0641523 loss)
I0502 22:03:20.268795 2050462464 solver.cpp:464] Iteration 2220, lr = 0.001
I0502 22:04:43.130079 2050462464 solver.cpp:189] Iteration 2240, loss = 0.043619
I0502 22:04:43.130132 2050462464 solver.cpp:204]     Train net output #0: loss = 0.043619 (* 1 = 0.043619 loss)
I0502 22:04:43.130141 2050462464 solver.cpp:464] Iteration 2240, lr = 0.001
I0502 22:06:06.366014 2050462464 solver.cpp:189] Iteration 2260, loss = 0.0618592
I0502 22:06:06.366067 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0618592 (* 1 = 0.0618592 loss)
I0502 22:06:06.366077 2050462464 solver.cpp:464] Iteration 2260, lr = 0.001
I0502 22:07:29.424056 2050462464 solver.cpp:189] Iteration 2280, loss = 0.0254541
I0502 22:07:29.424110 2050462464 solver.cpp:204]     Train net output #0: loss = 0.025454 (* 1 = 0.025454 loss)
I0502 22:07:29.424120 2050462464 solver.cpp:464] Iteration 2280, lr = 0.001
I0502 22:08:52.494665 2050462464 solver.cpp:189] Iteration 2300, loss = 0.138705
I0502 22:08:52.494719 2050462464 solver.cpp:204]     Train net output #0: loss = 0.138705 (* 1 = 0.138705 loss)
I0502 22:08:52.494729 2050462464 solver.cpp:464] Iteration 2300, lr = 0.001
I0502 22:10:15.711812 2050462464 solver.cpp:189] Iteration 2320, loss = 0.0705482
I0502 22:10:15.711866 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0705481 (* 1 = 0.0705481 loss)
I0502 22:10:15.711876 2050462464 solver.cpp:464] Iteration 2320, lr = 0.001
I0502 22:11:38.613194 2050462464 solver.cpp:189] Iteration 2340, loss = 0.0678405
I0502 22:11:38.613250 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0678404 (* 1 = 0.0678404 loss)
I0502 22:11:38.613260 2050462464 solver.cpp:464] Iteration 2340, lr = 0.001
I0502 22:13:01.654428 2050462464 solver.cpp:189] Iteration 2360, loss = 0.0392053
I0502 22:13:01.654481 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0392052 (* 1 = 0.0392052 loss)
I0502 22:13:01.654491 2050462464 solver.cpp:464] Iteration 2360, lr = 0.001
I0502 22:14:24.766206 2050462464 solver.cpp:189] Iteration 2380, loss = 0.163143
I0502 22:14:24.766254 2050462464 solver.cpp:204]     Train net output #0: loss = 0.163142 (* 1 = 0.163142 loss)
I0502 22:14:24.766263 2050462464 solver.cpp:464] Iteration 2380, lr = 0.001
I0502 22:15:47.261575 2050462464 solver.cpp:189] Iteration 2400, loss = 0.0204439
I0502 22:15:47.261667 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0204438 (* 1 = 0.0204438 loss)
I0502 22:15:47.261683 2050462464 solver.cpp:464] Iteration 2400, lr = 0.001
I0502 22:17:10.603647 2050462464 solver.cpp:189] Iteration 2420, loss = 0.12979
I0502 22:17:10.603703 2050462464 solver.cpp:204]     Train net output #0: loss = 0.12979 (* 1 = 0.12979 loss)
I0502 22:17:10.603713 2050462464 solver.cpp:464] Iteration 2420, lr = 0.001
I0502 22:18:33.410760 2050462464 solver.cpp:189] Iteration 2440, loss = 0.0479036
I0502 22:18:33.410819 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0479035 (* 1 = 0.0479035 loss)
I0502 22:18:33.410828 2050462464 solver.cpp:464] Iteration 2440, lr = 0.001
I0502 22:19:56.309345 2050462464 solver.cpp:189] Iteration 2460, loss = 0.0077314
I0502 22:19:56.309401 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00773132 (* 1 = 0.00773132 loss)
I0502 22:19:56.309411 2050462464 solver.cpp:464] Iteration 2460, lr = 0.001
I0502 22:21:19.503816 2050462464 solver.cpp:189] Iteration 2480, loss = 0.00552277
I0502 22:21:19.503862 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00552268 (* 1 = 0.00552268 loss)
I0502 22:21:19.503871 2050462464 solver.cpp:464] Iteration 2480, lr = 0.001
I0502 22:22:38.939831 2050462464 solver.cpp:334] Snapshotting to caffenet_train_background_iter_2500.caffemodel
I0502 22:22:40.546063 2050462464 solver.cpp:342] Snapshotting solver state to caffenet_train_background_iter_2500.solverstate
I0502 22:22:45.692451 2050462464 solver.cpp:189] Iteration 2500, loss = 0.0130888
I0502 22:22:45.692492 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0130888 (* 1 = 0.0130888 loss)
I0502 22:22:45.692502 2050462464 solver.cpp:464] Iteration 2500, lr = 0.001
I0502 22:24:09.069133 2050462464 solver.cpp:189] Iteration 2520, loss = 0.155843
I0502 22:24:09.069190 2050462464 solver.cpp:204]     Train net output #0: loss = 0.155843 (* 1 = 0.155843 loss)
I0502 22:24:09.069200 2050462464 solver.cpp:464] Iteration 2520, lr = 0.001
I0502 22:25:32.085608 2050462464 solver.cpp:189] Iteration 2540, loss = 0.030003
I0502 22:25:32.085659 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0300029 (* 1 = 0.0300029 loss)
I0502 22:25:32.085669 2050462464 solver.cpp:464] Iteration 2540, lr = 0.001
I0502 22:26:54.719521 2050462464 solver.cpp:189] Iteration 2560, loss = 0.0332129
I0502 22:26:54.719574 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0332128 (* 1 = 0.0332128 loss)
I0502 22:26:54.719585 2050462464 solver.cpp:464] Iteration 2560, lr = 0.001
I0502 22:28:17.780992 2050462464 solver.cpp:189] Iteration 2580, loss = 0.00741068
I0502 22:28:17.781049 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00741057 (* 1 = 0.00741057 loss)
I0502 22:28:17.781059 2050462464 solver.cpp:464] Iteration 2580, lr = 0.001
I0502 22:29:40.709269 2050462464 solver.cpp:189] Iteration 2600, loss = 0.0277271
I0502 22:29:40.709321 2050462464 solver.cpp:204]     Train net output #0: loss = 0.027727 (* 1 = 0.027727 loss)
I0502 22:29:40.709331 2050462464 solver.cpp:464] Iteration 2600, lr = 0.001
I0502 22:31:04.025158 2050462464 solver.cpp:189] Iteration 2620, loss = 0.0739842
I0502 22:31:04.025209 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0739841 (* 1 = 0.0739841 loss)
I0502 22:31:04.025219 2050462464 solver.cpp:464] Iteration 2620, lr = 0.001
I0502 22:32:26.893573 2050462464 solver.cpp:189] Iteration 2640, loss = 0.0110769
I0502 22:32:26.893631 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0110768 (* 1 = 0.0110768 loss)
I0502 22:32:26.893641 2050462464 solver.cpp:464] Iteration 2640, lr = 0.001
I0502 22:33:49.490756 2050462464 solver.cpp:189] Iteration 2660, loss = 0.00984343
I0502 22:33:49.490808 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00984335 (* 1 = 0.00984335 loss)
I0502 22:33:49.490818 2050462464 solver.cpp:464] Iteration 2660, lr = 0.001
I0502 22:35:12.298940 2050462464 solver.cpp:189] Iteration 2680, loss = 0.059794
I0502 22:35:12.299010 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0597939 (* 1 = 0.0597939 loss)
I0502 22:35:12.299020 2050462464 solver.cpp:464] Iteration 2680, lr = 0.001
I0502 22:36:35.418885 2050462464 solver.cpp:189] Iteration 2700, loss = 0.0470155
I0502 22:36:35.418937 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0470154 (* 1 = 0.0470154 loss)
I0502 22:36:35.418947 2050462464 solver.cpp:464] Iteration 2700, lr = 0.001
I0502 22:37:58.165889 2050462464 solver.cpp:189] Iteration 2720, loss = 0.0384163
I0502 22:37:58.165942 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0384162 (* 1 = 0.0384162 loss)
I0502 22:37:58.165952 2050462464 solver.cpp:464] Iteration 2720, lr = 0.001
I0502 22:39:21.216156 2050462464 solver.cpp:189] Iteration 2740, loss = 0.0319282
I0502 22:39:21.216210 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0319281 (* 1 = 0.0319281 loss)
I0502 22:39:21.216220 2050462464 solver.cpp:464] Iteration 2740, lr = 0.001
I0502 22:40:43.958652 2050462464 solver.cpp:189] Iteration 2760, loss = 0.0835643
I0502 22:40:43.958708 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0835642 (* 1 = 0.0835642 loss)
I0502 22:40:43.958717 2050462464 solver.cpp:464] Iteration 2760, lr = 0.001
I0502 22:42:07.283190 2050462464 solver.cpp:189] Iteration 2780, loss = 0.054327
I0502 22:42:07.283239 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0543269 (* 1 = 0.0543269 loss)
I0502 22:42:07.283249 2050462464 solver.cpp:464] Iteration 2780, lr = 0.001
I0502 22:43:30.211354 2050462464 solver.cpp:189] Iteration 2800, loss = 0.0190437
I0502 22:43:30.211410 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0190437 (* 1 = 0.0190437 loss)
I0502 22:43:30.211421 2050462464 solver.cpp:464] Iteration 2800, lr = 0.001
I0502 22:44:53.094387 2050462464 solver.cpp:189] Iteration 2820, loss = 0.00659186
I0502 22:44:53.094444 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00659177 (* 1 = 0.00659177 loss)
I0502 22:44:53.094455 2050462464 solver.cpp:464] Iteration 2820, lr = 0.001
I0502 22:46:16.040570 2050462464 solver.cpp:189] Iteration 2840, loss = 0.0100674
I0502 22:46:16.040619 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0100673 (* 1 = 0.0100673 loss)
I0502 22:46:16.040629 2050462464 solver.cpp:464] Iteration 2840, lr = 0.001
I0502 22:47:39.325080 2050462464 solver.cpp:189] Iteration 2860, loss = 0.102721
I0502 22:47:39.325134 2050462464 solver.cpp:204]     Train net output #0: loss = 0.102721 (* 1 = 0.102721 loss)
I0502 22:47:39.325142 2050462464 solver.cpp:464] Iteration 2860, lr = 0.001
I0502 22:49:02.206645 2050462464 solver.cpp:189] Iteration 2880, loss = 0.0238921
I0502 22:49:02.210273 2050462464 solver.cpp:204]     Train net output #0: loss = 0.023892 (* 1 = 0.023892 loss)
I0502 22:49:02.210309 2050462464 solver.cpp:464] Iteration 2880, lr = 0.001
I0502 22:50:25.119634 2050462464 solver.cpp:189] Iteration 2900, loss = 0.0410385
I0502 22:50:25.119689 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0410384 (* 1 = 0.0410384 loss)
I0502 22:50:25.119699 2050462464 solver.cpp:464] Iteration 2900, lr = 0.001
I0502 22:51:47.923424 2050462464 solver.cpp:189] Iteration 2920, loss = 0.0108593
I0502 22:51:47.923482 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0108592 (* 1 = 0.0108592 loss)
I0502 22:51:47.923492 2050462464 solver.cpp:464] Iteration 2920, lr = 0.001
I0502 22:53:10.927242 2050462464 solver.cpp:189] Iteration 2940, loss = 0.0672435
I0502 22:53:10.927300 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0672434 (* 1 = 0.0672434 loss)
I0502 22:53:10.927309 2050462464 solver.cpp:464] Iteration 2940, lr = 0.001
I0502 22:54:33.917100 2050462464 solver.cpp:189] Iteration 2960, loss = 0.0372576
I0502 22:54:33.917150 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0372576 (* 1 = 0.0372576 loss)
I0502 22:54:33.917160 2050462464 solver.cpp:464] Iteration 2960, lr = 0.001
I0502 22:55:56.705646 2050462464 solver.cpp:189] Iteration 2980, loss = 0.0785081
I0502 22:55:56.705718 2050462464 solver.cpp:204]     Train net output #0: loss = 0.078508 (* 1 = 0.078508 loss)
I0502 22:55:56.705728 2050462464 solver.cpp:464] Iteration 2980, lr = 0.001
I0502 22:57:16.154518 2050462464 solver.cpp:334] Snapshotting to caffenet_train_background_iter_3000.caffemodel
I0502 22:57:17.712859 2050462464 solver.cpp:342] Snapshotting solver state to caffenet_train_background_iter_3000.solverstate
I0502 22:57:18.931089 2050462464 solver.cpp:266] Iteration 3000, Testing net (#0)
I0502 23:24:49.406291 2050462464 solver.cpp:315]     Test net output #0: accuracy = 0.974984
I0502 23:24:49.406333 2050462464 solver.cpp:315]     Test net output #1: loss = 0.0981063 (* 1 = 0.0981063 loss)
I0502 23:24:53.318511 2050462464 solver.cpp:189] Iteration 3000, loss = 0.0219322
I0502 23:24:53.318552 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0219321 (* 1 = 0.0219321 loss)
I0502 23:24:53.318562 2050462464 solver.cpp:464] Iteration 3000, lr = 0.001
I0502 23:26:16.552817 2050462464 solver.cpp:189] Iteration 3020, loss = 0.0338215
I0502 23:26:16.552866 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0338214 (* 1 = 0.0338214 loss)
I0502 23:26:16.552876 2050462464 solver.cpp:464] Iteration 3020, lr = 0.001
I0502 23:27:39.520952 2050462464 solver.cpp:189] Iteration 3040, loss = 0.069683
I0502 23:27:39.521002 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0696829 (* 1 = 0.0696829 loss)
I0502 23:27:39.521011 2050462464 solver.cpp:464] Iteration 3040, lr = 0.001
I0502 23:29:02.881602 2050462464 solver.cpp:189] Iteration 3060, loss = 0.039291
I0502 23:29:02.881649 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0392909 (* 1 = 0.0392909 loss)
I0502 23:29:02.881659 2050462464 solver.cpp:464] Iteration 3060, lr = 0.001
I0502 23:30:25.907512 2050462464 solver.cpp:189] Iteration 3080, loss = 0.0585718
I0502 23:30:25.907562 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0585717 (* 1 = 0.0585717 loss)
I0502 23:30:25.907572 2050462464 solver.cpp:464] Iteration 3080, lr = 0.001
I0502 23:31:48.952780 2050462464 solver.cpp:189] Iteration 3100, loss = 0.0107253
I0502 23:31:48.952833 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0107252 (* 1 = 0.0107252 loss)
I0502 23:31:48.952843 2050462464 solver.cpp:464] Iteration 3100, lr = 0.001
I0502 23:33:12.104094 2050462464 solver.cpp:189] Iteration 3120, loss = 0.0202163
I0502 23:33:12.104151 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0202163 (* 1 = 0.0202163 loss)
I0502 23:33:12.104161 2050462464 solver.cpp:464] Iteration 3120, lr = 0.001
I0502 23:34:35.320210 2050462464 solver.cpp:189] Iteration 3140, loss = 0.0418621
I0502 23:34:35.320269 2050462464 solver.cpp:204]     Train net output #0: loss = 0.041862 (* 1 = 0.041862 loss)
I0502 23:34:35.320279 2050462464 solver.cpp:464] Iteration 3140, lr = 0.001
I0502 23:35:58.379317 2050462464 solver.cpp:189] Iteration 3160, loss = 0.006219
I0502 23:35:58.379374 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00621893 (* 1 = 0.00621893 loss)
I0502 23:35:58.379384 2050462464 solver.cpp:464] Iteration 3160, lr = 0.001
I0502 23:37:21.794991 2050462464 solver.cpp:189] Iteration 3180, loss = 0.0115075
I0502 23:37:21.795045 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0115074 (* 1 = 0.0115074 loss)
I0502 23:37:21.795056 2050462464 solver.cpp:464] Iteration 3180, lr = 0.001
I0502 23:38:45.137809 2050462464 solver.cpp:189] Iteration 3200, loss = 0.0558174
I0502 23:38:45.137871 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0558173 (* 1 = 0.0558173 loss)
I0502 23:38:45.137881 2050462464 solver.cpp:464] Iteration 3200, lr = 0.001
I0502 23:40:08.261085 2050462464 solver.cpp:189] Iteration 3220, loss = 0.0259746
I0502 23:40:08.261142 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0259745 (* 1 = 0.0259745 loss)
I0502 23:40:08.261152 2050462464 solver.cpp:464] Iteration 3220, lr = 0.001
I0502 23:41:31.677953 2050462464 solver.cpp:189] Iteration 3240, loss = 0.0975653
I0502 23:41:31.677999 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0975653 (* 1 = 0.0975653 loss)
I0502 23:41:31.678009 2050462464 solver.cpp:464] Iteration 3240, lr = 0.001
I0502 23:42:54.444572 2050462464 solver.cpp:189] Iteration 3260, loss = 0.00338425
I0502 23:42:54.444651 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00338418 (* 1 = 0.00338418 loss)
I0502 23:42:54.444663 2050462464 solver.cpp:464] Iteration 3260, lr = 0.001
I0502 23:44:17.608687 2050462464 solver.cpp:189] Iteration 3280, loss = 0.0563155
I0502 23:44:17.608746 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0563155 (* 1 = 0.0563155 loss)
I0502 23:44:17.608755 2050462464 solver.cpp:464] Iteration 3280, lr = 0.001
I0502 23:45:40.560235 2050462464 solver.cpp:189] Iteration 3300, loss = 0.0197643
I0502 23:45:40.560292 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0197642 (* 1 = 0.0197642 loss)
I0502 23:45:40.560302 2050462464 solver.cpp:464] Iteration 3300, lr = 0.001
I0502 23:47:03.726320 2050462464 solver.cpp:189] Iteration 3320, loss = 0.0152639
I0502 23:47:03.726375 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0152638 (* 1 = 0.0152638 loss)
I0502 23:47:03.726385 2050462464 solver.cpp:464] Iteration 3320, lr = 0.001
I0502 23:48:26.556871 2050462464 solver.cpp:189] Iteration 3340, loss = 0.0206541
I0502 23:48:26.556943 2050462464 solver.cpp:204]     Train net output #0: loss = 0.020654 (* 1 = 0.020654 loss)
I0502 23:48:26.556952 2050462464 solver.cpp:464] Iteration 3340, lr = 0.001
I0502 23:49:49.713657 2050462464 solver.cpp:189] Iteration 3360, loss = 0.00150597
I0502 23:49:49.713716 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00150592 (* 1 = 0.00150592 loss)
I0502 23:49:49.713727 2050462464 solver.cpp:464] Iteration 3360, lr = 0.001
I0502 23:51:12.852140 2050462464 solver.cpp:189] Iteration 3380, loss = 0.0469301
I0502 23:51:12.852195 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0469301 (* 1 = 0.0469301 loss)
I0502 23:51:12.852205 2050462464 solver.cpp:464] Iteration 3380, lr = 0.001
I0502 23:52:35.699188 2050462464 solver.cpp:189] Iteration 3400, loss = 0.0646484
I0502 23:52:35.699234 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0646483 (* 1 = 0.0646483 loss)
I0502 23:52:35.699244 2050462464 solver.cpp:464] Iteration 3400, lr = 0.001
I0502 23:53:58.813552 2050462464 solver.cpp:189] Iteration 3420, loss = 0.0300941
I0502 23:53:58.813608 2050462464 solver.cpp:204]     Train net output #0: loss = 0.030094 (* 1 = 0.030094 loss)
I0502 23:53:58.813617 2050462464 solver.cpp:464] Iteration 3420, lr = 0.001
I0502 23:55:22.119882 2050462464 solver.cpp:189] Iteration 3440, loss = 0.00281821
I0502 23:55:22.119937 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00281815 (* 1 = 0.00281815 loss)
I0502 23:55:22.119947 2050462464 solver.cpp:464] Iteration 3440, lr = 0.001
I0502 23:56:45.352167 2050462464 solver.cpp:189] Iteration 3460, loss = 0.0343182
I0502 23:56:45.352221 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0343181 (* 1 = 0.0343181 loss)
I0502 23:56:45.352231 2050462464 solver.cpp:464] Iteration 3460, lr = 0.001
I0502 23:58:08.118671 2050462464 solver.cpp:189] Iteration 3480, loss = 0.025877
I0502 23:58:08.118727 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0258769 (* 1 = 0.0258769 loss)
I0502 23:58:08.118737 2050462464 solver.cpp:464] Iteration 3480, lr = 0.001
I0502 23:59:27.347348 2050462464 solver.cpp:334] Snapshotting to caffenet_train_background_iter_3500.caffemodel
I0502 23:59:28.950990 2050462464 solver.cpp:342] Snapshotting solver state to caffenet_train_background_iter_3500.solverstate
I0502 23:59:34.143894 2050462464 solver.cpp:189] Iteration 3500, loss = 0.00343406
I0502 23:59:34.143936 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00343398 (* 1 = 0.00343398 loss)
I0502 23:59:34.143946 2050462464 solver.cpp:464] Iteration 3500, lr = 0.001
I0503 00:00:57.195132 2050462464 solver.cpp:189] Iteration 3520, loss = 0.00305336
I0503 00:00:57.195901 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00305328 (* 1 = 0.00305328 loss)
I0503 00:00:57.195914 2050462464 solver.cpp:464] Iteration 3520, lr = 0.001
I0503 00:02:20.561975 2050462464 solver.cpp:189] Iteration 3540, loss = 0.0954027
I0503 00:02:20.562047 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0954027 (* 1 = 0.0954027 loss)
I0503 00:02:20.562058 2050462464 solver.cpp:464] Iteration 3540, lr = 0.001
I0503 00:03:43.642433 2050462464 solver.cpp:189] Iteration 3560, loss = 0.0104929
I0503 00:03:43.642490 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0104928 (* 1 = 0.0104928 loss)
I0503 00:03:43.642501 2050462464 solver.cpp:464] Iteration 3560, lr = 0.001
I0503 00:05:06.962820 2050462464 solver.cpp:189] Iteration 3580, loss = 0.00677591
I0503 00:05:06.962877 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0067758 (* 1 = 0.0067758 loss)
I0503 00:05:06.962887 2050462464 solver.cpp:464] Iteration 3580, lr = 0.001
I0503 00:06:29.685533 2050462464 solver.cpp:189] Iteration 3600, loss = 0.00181412
I0503 00:06:29.685580 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00181401 (* 1 = 0.00181401 loss)
I0503 00:06:29.685590 2050462464 solver.cpp:464] Iteration 3600, lr = 0.001
I0503 00:07:52.355592 2050462464 solver.cpp:189] Iteration 3620, loss = 0.0538655
I0503 00:07:52.355648 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0538654 (* 1 = 0.0538654 loss)
I0503 00:07:52.355659 2050462464 solver.cpp:464] Iteration 3620, lr = 0.001
I0503 00:09:15.636557 2050462464 solver.cpp:189] Iteration 3640, loss = 0.0331556
I0503 00:09:15.636612 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0331555 (* 1 = 0.0331555 loss)
I0503 00:09:15.636622 2050462464 solver.cpp:464] Iteration 3640, lr = 0.001
I0503 00:10:38.629489 2050462464 solver.cpp:189] Iteration 3660, loss = 0.0108101
I0503 00:10:38.629537 2050462464 solver.cpp:204]     Train net output #0: loss = 0.01081 (* 1 = 0.01081 loss)
I0503 00:10:38.629547 2050462464 solver.cpp:464] Iteration 3660, lr = 0.001
I0503 00:12:01.492836 2050462464 solver.cpp:189] Iteration 3680, loss = 0.137984
I0503 00:12:01.492892 2050462464 solver.cpp:204]     Train net output #0: loss = 0.137984 (* 1 = 0.137984 loss)
I0503 00:12:01.492900 2050462464 solver.cpp:464] Iteration 3680, lr = 0.001
I0503 00:13:24.785171 2050462464 solver.cpp:189] Iteration 3700, loss = 0.0129806
I0503 00:13:24.785225 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0129805 (* 1 = 0.0129805 loss)
I0503 00:13:24.785235 2050462464 solver.cpp:464] Iteration 3700, lr = 0.001
I0503 00:14:47.476846 2050462464 solver.cpp:189] Iteration 3720, loss = 0.00210395
I0503 00:14:47.476899 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00210383 (* 1 = 0.00210383 loss)
I0503 00:14:47.476909 2050462464 solver.cpp:464] Iteration 3720, lr = 0.001
I0503 00:16:12.129706 2050462464 solver.cpp:189] Iteration 3740, loss = 0.030846
I0503 00:16:12.130024 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0308459 (* 1 = 0.0308459 loss)
I0503 00:16:12.130038 2050462464 solver.cpp:464] Iteration 3740, lr = 0.001
I0503 00:17:35.208917 2050462464 solver.cpp:189] Iteration 3760, loss = 0.00667368
I0503 00:17:35.208973 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00667354 (* 1 = 0.00667354 loss)
I0503 00:17:35.208983 2050462464 solver.cpp:464] Iteration 3760, lr = 0.001
I0503 00:18:58.282295 2050462464 solver.cpp:189] Iteration 3780, loss = 0.0732887
I0503 00:18:58.282344 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0732886 (* 1 = 0.0732886 loss)
I0503 00:18:58.282353 2050462464 solver.cpp:464] Iteration 3780, lr = 0.001
I0503 00:20:21.403595 2050462464 solver.cpp:189] Iteration 3800, loss = 0.0191969
I0503 00:20:21.403645 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0191967 (* 1 = 0.0191967 loss)
I0503 00:20:21.403655 2050462464 solver.cpp:464] Iteration 3800, lr = 0.001
I0503 00:21:44.439754 2050462464 solver.cpp:189] Iteration 3820, loss = 0.00688172
I0503 00:21:44.439805 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0068816 (* 1 = 0.0068816 loss)
I0503 00:21:44.439815 2050462464 solver.cpp:464] Iteration 3820, lr = 0.001
I0503 00:23:07.527328 2050462464 solver.cpp:189] Iteration 3840, loss = 0.0473653
I0503 00:23:07.527405 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0473652 (* 1 = 0.0473652 loss)
I0503 00:23:07.527415 2050462464 solver.cpp:464] Iteration 3840, lr = 0.001
I0503 00:24:30.530601 2050462464 solver.cpp:189] Iteration 3860, loss = 0.0151651
I0503 00:24:30.530645 2050462464 solver.cpp:204]     Train net output #0: loss = 0.015165 (* 1 = 0.015165 loss)
I0503 00:24:30.530655 2050462464 solver.cpp:464] Iteration 3860, lr = 0.001
I0503 00:25:53.255763 2050462464 solver.cpp:189] Iteration 3880, loss = 0.0537118
I0503 00:25:53.255817 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0537117 (* 1 = 0.0537117 loss)
I0503 00:25:53.255826 2050462464 solver.cpp:464] Iteration 3880, lr = 0.001
I0503 00:27:16.386639 2050462464 solver.cpp:189] Iteration 3900, loss = 0.018063
I0503 00:27:16.386687 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0180629 (* 1 = 0.0180629 loss)
I0503 00:27:16.386698 2050462464 solver.cpp:464] Iteration 3900, lr = 0.001
I0503 00:28:39.358325 2050462464 solver.cpp:189] Iteration 3920, loss = 0.0254497
I0503 00:28:39.358372 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0254496 (* 1 = 0.0254496 loss)
I0503 00:28:39.358382 2050462464 solver.cpp:464] Iteration 3920, lr = 0.001
I0503 00:30:02.050844 2050462464 solver.cpp:189] Iteration 3940, loss = 0.00534545
I0503 00:30:02.050895 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00534534 (* 1 = 0.00534534 loss)
I0503 00:30:02.050905 2050462464 solver.cpp:464] Iteration 3940, lr = 0.001
I0503 00:31:24.936549 2050462464 solver.cpp:189] Iteration 3960, loss = 0.00851219
I0503 00:31:24.936599 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00851209 (* 1 = 0.00851209 loss)
I0503 00:31:24.936607 2050462464 solver.cpp:464] Iteration 3960, lr = 0.001
I0503 00:32:48.084131 2050462464 solver.cpp:189] Iteration 3980, loss = 0.0172838
I0503 00:32:48.084183 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0172837 (* 1 = 0.0172837 loss)
I0503 00:32:48.084193 2050462464 solver.cpp:464] Iteration 3980, lr = 0.001
I0503 00:34:07.528347 2050462464 solver.cpp:334] Snapshotting to caffenet_train_background_iter_4000.caffemodel
I0503 00:34:09.269176 2050462464 solver.cpp:342] Snapshotting solver state to caffenet_train_background_iter_4000.solverstate
I0503 00:34:10.518714 2050462464 solver.cpp:266] Iteration 4000, Testing net (#0)
I0503 01:01:46.692735 2050462464 solver.cpp:315]     Test net output #0: accuracy = 0.970312
I0503 01:01:46.692873 2050462464 solver.cpp:315]     Test net output #1: loss = 0.109164 (* 1 = 0.109164 loss)
I0503 01:01:50.573094 2050462464 solver.cpp:189] Iteration 4000, loss = 0.00165342
I0503 01:01:50.573137 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00165331 (* 1 = 0.00165331 loss)
I0503 01:01:50.573145 2050462464 solver.cpp:464] Iteration 4000, lr = 0.001
I0503 01:03:13.974386 2050462464 solver.cpp:189] Iteration 4020, loss = 0.00841328
I0503 01:03:13.974441 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00841317 (* 1 = 0.00841317 loss)
I0503 01:03:13.974450 2050462464 solver.cpp:464] Iteration 4020, lr = 0.001
I0503 01:04:37.174275 2050462464 solver.cpp:189] Iteration 4040, loss = 0.00623882
I0503 01:04:37.174331 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00623872 (* 1 = 0.00623872 loss)
I0503 01:04:37.174341 2050462464 solver.cpp:464] Iteration 4040, lr = 0.001
I0503 01:06:00.091469 2050462464 solver.cpp:189] Iteration 4060, loss = 0.0149207
I0503 01:06:00.091521 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0149206 (* 1 = 0.0149206 loss)
I0503 01:06:00.091531 2050462464 solver.cpp:464] Iteration 4060, lr = 0.001
I0503 01:07:23.181192 2050462464 solver.cpp:189] Iteration 4080, loss = 0.00279272
I0503 01:07:23.181246 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0027926 (* 1 = 0.0027926 loss)
I0503 01:07:23.181255 2050462464 solver.cpp:464] Iteration 4080, lr = 0.001
I0503 01:08:46.097759 2050462464 solver.cpp:189] Iteration 4100, loss = 0.0415163
I0503 01:08:46.097841 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0415162 (* 1 = 0.0415162 loss)
I0503 01:08:46.097852 2050462464 solver.cpp:464] Iteration 4100, lr = 0.001
I0503 01:10:09.217784 2050462464 solver.cpp:189] Iteration 4120, loss = 0.0592963
I0503 01:10:09.217844 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0592962 (* 1 = 0.0592962 loss)
I0503 01:10:09.217854 2050462464 solver.cpp:464] Iteration 4120, lr = 0.001
I0503 01:11:32.221235 2050462464 solver.cpp:189] Iteration 4140, loss = 0.00199792
I0503 01:11:32.221292 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00199783 (* 1 = 0.00199783 loss)
I0503 01:11:32.221302 2050462464 solver.cpp:464] Iteration 4140, lr = 0.001
I0503 01:12:55.083034 2050462464 solver.cpp:189] Iteration 4160, loss = 0.0164666
I0503 01:12:55.083081 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0164665 (* 1 = 0.0164665 loss)
I0503 01:12:55.083091 2050462464 solver.cpp:464] Iteration 4160, lr = 0.001
I0503 01:14:18.328593 2050462464 solver.cpp:189] Iteration 4180, loss = 0.0591859
I0503 01:14:18.328654 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0591858 (* 1 = 0.0591858 loss)
I0503 01:14:18.328665 2050462464 solver.cpp:464] Iteration 4180, lr = 0.001
I0503 01:15:41.175585 2050462464 solver.cpp:189] Iteration 4200, loss = 0.00973608
I0503 01:15:41.175648 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00973599 (* 1 = 0.00973599 loss)
I0503 01:15:41.175659 2050462464 solver.cpp:464] Iteration 4200, lr = 0.001
I0503 01:17:04.401113 2050462464 solver.cpp:189] Iteration 4220, loss = 0.0156271
I0503 01:17:04.401160 2050462464 solver.cpp:204]     Train net output #0: loss = 0.015627 (* 1 = 0.015627 loss)
I0503 01:17:04.401168 2050462464 solver.cpp:464] Iteration 4220, lr = 0.001
I0503 01:18:27.340714 2050462464 solver.cpp:189] Iteration 4240, loss = 0.0554328
I0503 01:18:27.340767 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0554327 (* 1 = 0.0554327 loss)
I0503 01:18:27.340777 2050462464 solver.cpp:464] Iteration 4240, lr = 0.001
I0503 01:19:50.111129 2050462464 solver.cpp:189] Iteration 4260, loss = 0.0370316
I0503 01:19:50.111186 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0370315 (* 1 = 0.0370315 loss)
I0503 01:19:50.111196 2050462464 solver.cpp:464] Iteration 4260, lr = 0.001
I0503 01:21:13.258879 2050462464 solver.cpp:189] Iteration 4280, loss = 0.00134483
I0503 01:21:13.258936 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00134472 (* 1 = 0.00134472 loss)
I0503 01:21:13.258946 2050462464 solver.cpp:464] Iteration 4280, lr = 0.001
I0503 01:22:35.970850 2050462464 solver.cpp:189] Iteration 4300, loss = 0.00667434
I0503 01:22:35.970906 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00667423 (* 1 = 0.00667423 loss)
I0503 01:22:35.970916 2050462464 solver.cpp:464] Iteration 4300, lr = 0.001
I0503 01:23:58.962944 2050462464 solver.cpp:189] Iteration 4320, loss = 0.0496533
I0503 01:23:58.962990 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0496532 (* 1 = 0.0496532 loss)
I0503 01:23:58.962999 2050462464 solver.cpp:464] Iteration 4320, lr = 0.001
I0503 01:25:22.081079 2050462464 solver.cpp:189] Iteration 4340, loss = 0.0242657
I0503 01:25:22.081135 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0242656 (* 1 = 0.0242656 loss)
I0503 01:25:22.081146 2050462464 solver.cpp:464] Iteration 4340, lr = 0.001
I0503 01:26:44.549840 2050462464 solver.cpp:189] Iteration 4360, loss = 0.0403955
I0503 01:26:44.549886 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0403954 (* 1 = 0.0403954 loss)
I0503 01:26:44.549896 2050462464 solver.cpp:464] Iteration 4360, lr = 0.001
I0503 01:28:07.505982 2050462464 solver.cpp:189] Iteration 4380, loss = 0.0984319
I0503 01:28:07.506037 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0984318 (* 1 = 0.0984318 loss)
I0503 01:28:07.506047 2050462464 solver.cpp:464] Iteration 4380, lr = 0.001
I0503 01:29:30.054277 2050462464 solver.cpp:189] Iteration 4400, loss = 0.023604
I0503 01:29:30.054345 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0236039 (* 1 = 0.0236039 loss)
I0503 01:29:30.054357 2050462464 solver.cpp:464] Iteration 4400, lr = 0.001
I0503 01:30:52.797844 2050462464 solver.cpp:189] Iteration 4420, loss = 0.0248
I0503 01:30:52.797891 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0247999 (* 1 = 0.0247999 loss)
I0503 01:30:52.797902 2050462464 solver.cpp:464] Iteration 4420, lr = 0.001
I0503 01:32:15.982000 2050462464 solver.cpp:189] Iteration 4440, loss = 0.00763929
I0503 01:32:15.982058 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00763917 (* 1 = 0.00763917 loss)
I0503 01:32:15.982067 2050462464 solver.cpp:464] Iteration 4440, lr = 0.001
I0503 01:33:39.043730 2050462464 solver.cpp:189] Iteration 4460, loss = 0.0416746
I0503 01:33:39.043777 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0416745 (* 1 = 0.0416745 loss)
I0503 01:33:39.043787 2050462464 solver.cpp:464] Iteration 4460, lr = 0.001
I0503 01:35:01.573726 2050462464 solver.cpp:189] Iteration 4480, loss = 0.0095599
I0503 01:35:01.573781 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0095598 (* 1 = 0.0095598 loss)
I0503 01:35:01.573791 2050462464 solver.cpp:464] Iteration 4480, lr = 0.001
I0503 01:36:21.296010 2050462464 solver.cpp:334] Snapshotting to caffenet_train_background_iter_4500.caffemodel
I0503 01:36:23.190927 2050462464 solver.cpp:342] Snapshotting solver state to caffenet_train_background_iter_4500.solverstate
I0503 01:36:28.648294 2050462464 solver.cpp:189] Iteration 4500, loss = 0.0853437
I0503 01:36:28.648332 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0853436 (* 1 = 0.0853436 loss)
I0503 01:36:28.648342 2050462464 solver.cpp:464] Iteration 4500, lr = 0.001
I0503 01:37:54.691088 2050462464 solver.cpp:189] Iteration 4520, loss = 0.00826793
I0503 01:37:54.691143 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00826782 (* 1 = 0.00826782 loss)
I0503 01:37:54.691154 2050462464 solver.cpp:464] Iteration 4520, lr = 0.001
I0503 01:39:17.579399 2050462464 solver.cpp:189] Iteration 4540, loss = 0.0220981
I0503 01:39:17.579447 2050462464 solver.cpp:204]     Train net output #0: loss = 0.022098 (* 1 = 0.022098 loss)
I0503 01:39:17.579457 2050462464 solver.cpp:464] Iteration 4540, lr = 0.001
I0503 01:40:40.278992 2050462464 solver.cpp:189] Iteration 4560, loss = 0.00101793
I0503 01:40:40.279047 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00101782 (* 1 = 0.00101782 loss)
I0503 01:40:40.279057 2050462464 solver.cpp:464] Iteration 4560, lr = 0.001
I0503 01:42:03.427743 2050462464 solver.cpp:189] Iteration 4580, loss = 0.017884
I0503 01:42:03.427808 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0178839 (* 1 = 0.0178839 loss)
I0503 01:42:03.427822 2050462464 solver.cpp:464] Iteration 4580, lr = 0.001
I0503 01:43:26.226312 2050462464 solver.cpp:189] Iteration 4600, loss = 0.0116307
I0503 01:43:26.226361 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0116306 (* 1 = 0.0116306 loss)
I0503 01:43:26.226371 2050462464 solver.cpp:464] Iteration 4600, lr = 0.001
I0503 01:44:48.831742 2050462464 solver.cpp:189] Iteration 4620, loss = 0.00575511
I0503 01:44:48.831787 2050462464 solver.cpp:204]     Train net output #0: loss = 0.005755 (* 1 = 0.005755 loss)
I0503 01:44:48.831796 2050462464 solver.cpp:464] Iteration 4620, lr = 0.001
I0503 01:46:11.959091 2050462464 solver.cpp:189] Iteration 4640, loss = 0.00191129
I0503 01:46:11.959143 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00191119 (* 1 = 0.00191119 loss)
I0503 01:46:11.959153 2050462464 solver.cpp:464] Iteration 4640, lr = 0.001
I0503 01:47:34.901371 2050462464 solver.cpp:189] Iteration 4660, loss = 0.000763293
I0503 01:47:34.901417 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000763188 (* 1 = 0.000763188 loss)
I0503 01:47:34.901427 2050462464 solver.cpp:464] Iteration 4660, lr = 0.001
I0503 01:48:57.659459 2050462464 solver.cpp:189] Iteration 4680, loss = 0.00181224
I0503 01:48:57.659524 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00181214 (* 1 = 0.00181214 loss)
I0503 01:48:57.659535 2050462464 solver.cpp:464] Iteration 4680, lr = 0.001
I0503 01:50:20.846606 2050462464 solver.cpp:189] Iteration 4700, loss = 0.00754179
I0503 01:50:20.846662 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00754169 (* 1 = 0.00754169 loss)
I0503 01:50:20.846673 2050462464 solver.cpp:464] Iteration 4700, lr = 0.001
I0503 01:51:43.866436 2050462464 solver.cpp:189] Iteration 4720, loss = 0.0142519
I0503 01:51:43.866484 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0142518 (* 1 = 0.0142518 loss)
I0503 01:51:43.866495 2050462464 solver.cpp:464] Iteration 4720, lr = 0.001
I0503 01:53:07.100173 2050462464 solver.cpp:189] Iteration 4740, loss = 0.0411745
I0503 01:53:07.100222 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0411744 (* 1 = 0.0411744 loss)
I0503 01:53:07.100231 2050462464 solver.cpp:464] Iteration 4740, lr = 0.001
I0503 01:54:29.909266 2050462464 solver.cpp:189] Iteration 4760, loss = 0.1073
I0503 01:54:29.909315 2050462464 solver.cpp:204]     Train net output #0: loss = 0.107299 (* 1 = 0.107299 loss)
I0503 01:54:29.909325 2050462464 solver.cpp:464] Iteration 4760, lr = 0.001
I0503 01:55:52.466382 2050462464 solver.cpp:189] Iteration 4780, loss = 0.0246341
I0503 01:55:52.466437 2050462464 solver.cpp:204]     Train net output #0: loss = 0.024634 (* 1 = 0.024634 loss)
I0503 01:55:52.466447 2050462464 solver.cpp:464] Iteration 4780, lr = 0.001
I0503 01:57:15.403846 2050462464 solver.cpp:189] Iteration 4800, loss = 0.00631163
I0503 01:57:15.403893 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00631153 (* 1 = 0.00631153 loss)
I0503 01:57:15.403903 2050462464 solver.cpp:464] Iteration 4800, lr = 0.001
I0503 01:58:38.109887 2050462464 solver.cpp:189] Iteration 4820, loss = 0.00291582
I0503 01:58:38.109941 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00291572 (* 1 = 0.00291572 loss)
I0503 01:58:38.109951 2050462464 solver.cpp:464] Iteration 4820, lr = 0.001
I0503 02:00:00.938930 2050462464 solver.cpp:189] Iteration 4840, loss = 0.00665997
I0503 02:00:00.938979 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00665988 (* 1 = 0.00665988 loss)
I0503 02:00:00.938989 2050462464 solver.cpp:464] Iteration 4840, lr = 0.001
I0503 02:01:24.567807 2050462464 solver.cpp:189] Iteration 4860, loss = 0.0104517
I0503 02:01:24.567854 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0104516 (* 1 = 0.0104516 loss)
I0503 02:01:24.567864 2050462464 solver.cpp:464] Iteration 4860, lr = 0.001
I0503 02:02:47.333197 2050462464 solver.cpp:189] Iteration 4880, loss = 0.000914956
I0503 02:02:47.333246 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000914872 (* 1 = 0.000914872 loss)
I0503 02:02:47.333257 2050462464 solver.cpp:464] Iteration 4880, lr = 0.001
I0503 02:04:10.317224 2050462464 solver.cpp:189] Iteration 4900, loss = 0.0795023
I0503 02:04:10.317280 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0795022 (* 1 = 0.0795022 loss)
I0503 02:04:10.317289 2050462464 solver.cpp:464] Iteration 4900, lr = 0.001
I0503 02:05:33.558567 2050462464 solver.cpp:189] Iteration 4920, loss = 0.0235939
I0503 02:05:33.558615 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0235939 (* 1 = 0.0235939 loss)
I0503 02:05:33.558625 2050462464 solver.cpp:464] Iteration 4920, lr = 0.001
I0503 02:06:56.421257 2050462464 solver.cpp:189] Iteration 4940, loss = 0.00348935
I0503 02:06:56.421311 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00348929 (* 1 = 0.00348929 loss)
I0503 02:06:56.421321 2050462464 solver.cpp:464] Iteration 4940, lr = 0.001
I0503 02:08:19.624706 2050462464 solver.cpp:189] Iteration 4960, loss = 0.0361545
I0503 02:08:19.624763 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0361545 (* 1 = 0.0361545 loss)
I0503 02:08:19.624773 2050462464 solver.cpp:464] Iteration 4960, lr = 0.001
I0503 02:09:42.477819 2050462464 solver.cpp:189] Iteration 4980, loss = 0.0703688
I0503 02:09:42.477896 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0703687 (* 1 = 0.0703687 loss)
I0503 02:09:42.477907 2050462464 solver.cpp:464] Iteration 4980, lr = 0.001
I0503 02:11:01.804517 2050462464 solver.cpp:334] Snapshotting to caffenet_train_background_iter_5000.caffemodel
I0503 02:11:03.394834 2050462464 solver.cpp:342] Snapshotting solver state to caffenet_train_background_iter_5000.solverstate
I0503 02:11:04.784703 2050462464 solver.cpp:266] Iteration 5000, Testing net (#0)
I0503 02:38:35.440716 2050462464 solver.cpp:315]     Test net output #0: accuracy = 0.971094
I0503 02:38:35.440768 2050462464 solver.cpp:315]     Test net output #1: loss = 0.118707 (* 1 = 0.118707 loss)
I0503 02:38:39.399469 2050462464 solver.cpp:189] Iteration 5000, loss = 0.0478157
I0503 02:38:39.399510 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0478157 (* 1 = 0.0478157 loss)
I0503 02:38:39.399519 2050462464 solver.cpp:464] Iteration 5000, lr = 0.001
I0503 02:40:02.319054 2050462464 solver.cpp:189] Iteration 5020, loss = 0.0307701
I0503 02:40:02.319108 2050462464 solver.cpp:204]     Train net output #0: loss = 0.03077 (* 1 = 0.03077 loss)
I0503 02:40:02.319116 2050462464 solver.cpp:464] Iteration 5020, lr = 0.001
I0503 02:41:25.133376 2050462464 solver.cpp:189] Iteration 5040, loss = 0.00325206
I0503 02:41:25.133424 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00325199 (* 1 = 0.00325199 loss)
I0503 02:41:25.133433 2050462464 solver.cpp:464] Iteration 5040, lr = 0.001
I0503 02:42:48.132239 2050462464 solver.cpp:189] Iteration 5060, loss = 0.0200945
I0503 02:42:48.132293 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0200945 (* 1 = 0.0200945 loss)
I0503 02:42:48.132303 2050462464 solver.cpp:464] Iteration 5060, lr = 0.001
I0503 02:44:11.478248 2050462464 solver.cpp:189] Iteration 5080, loss = 0.0741542
I0503 02:44:11.478302 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0741542 (* 1 = 0.0741542 loss)
I0503 02:44:11.478312 2050462464 solver.cpp:464] Iteration 5080, lr = 0.001
I0503 02:45:34.025157 2050462464 solver.cpp:189] Iteration 5100, loss = 0.00311369
I0503 02:45:34.025214 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00311363 (* 1 = 0.00311363 loss)
I0503 02:45:34.025224 2050462464 solver.cpp:464] Iteration 5100, lr = 0.001
I0503 02:46:56.969457 2050462464 solver.cpp:189] Iteration 5120, loss = 0.00877447
I0503 02:46:56.969512 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00877441 (* 1 = 0.00877441 loss)
I0503 02:46:56.969522 2050462464 solver.cpp:464] Iteration 5120, lr = 0.001
I0503 02:48:20.144701 2050462464 solver.cpp:189] Iteration 5140, loss = 0.0121534
I0503 02:48:20.144759 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0121533 (* 1 = 0.0121533 loss)
I0503 02:48:20.144770 2050462464 solver.cpp:464] Iteration 5140, lr = 0.001
I0503 02:49:42.940610 2050462464 solver.cpp:189] Iteration 5160, loss = 0.0327338
I0503 02:49:42.940666 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0327337 (* 1 = 0.0327337 loss)
I0503 02:49:42.940676 2050462464 solver.cpp:464] Iteration 5160, lr = 0.001
I0503 02:51:05.737396 2050462464 solver.cpp:189] Iteration 5180, loss = 0.00117696
I0503 02:51:05.737445 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0011769 (* 1 = 0.0011769 loss)
I0503 02:51:05.737455 2050462464 solver.cpp:464] Iteration 5180, lr = 0.001
I0503 02:52:28.558053 2050462464 solver.cpp:189] Iteration 5200, loss = 0.00517373
I0503 02:52:28.558109 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00517367 (* 1 = 0.00517367 loss)
I0503 02:52:28.558117 2050462464 solver.cpp:464] Iteration 5200, lr = 0.001
I0503 02:53:51.274919 2050462464 solver.cpp:189] Iteration 5220, loss = 0.00240559
I0503 02:53:51.274976 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00240552 (* 1 = 0.00240552 loss)
I0503 02:53:51.274984 2050462464 solver.cpp:464] Iteration 5220, lr = 0.001
I0503 02:55:14.306089 2050462464 solver.cpp:189] Iteration 5240, loss = 0.0116958
I0503 02:55:14.306165 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0116957 (* 1 = 0.0116957 loss)
I0503 02:55:14.306175 2050462464 solver.cpp:464] Iteration 5240, lr = 0.001
I0503 02:56:37.279230 2050462464 solver.cpp:189] Iteration 5260, loss = 0.00966524
I0503 02:56:37.279284 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00966518 (* 1 = 0.00966518 loss)
I0503 02:56:37.279294 2050462464 solver.cpp:464] Iteration 5260, lr = 0.001
I0503 02:58:00.157666 2050462464 solver.cpp:189] Iteration 5280, loss = 0.00406267
I0503 02:58:00.157721 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00406262 (* 1 = 0.00406262 loss)
I0503 02:58:00.157730 2050462464 solver.cpp:464] Iteration 5280, lr = 0.001
I0503 02:59:23.015262 2050462464 solver.cpp:189] Iteration 5300, loss = 0.0206983
I0503 02:59:23.015317 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0206983 (* 1 = 0.0206983 loss)
I0503 02:59:23.015327 2050462464 solver.cpp:464] Iteration 5300, lr = 0.001
I0503 03:00:45.557138 2050462464 solver.cpp:189] Iteration 5320, loss = 0.0474139
I0503 03:00:45.557195 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0474138 (* 1 = 0.0474138 loss)
I0503 03:00:45.557205 2050462464 solver.cpp:464] Iteration 5320, lr = 0.001
I0503 03:02:08.918058 2050462464 solver.cpp:189] Iteration 5340, loss = 0.0025897
I0503 03:02:08.918115 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00258964 (* 1 = 0.00258964 loss)
I0503 03:02:08.918125 2050462464 solver.cpp:464] Iteration 5340, lr = 0.001
I0503 03:03:31.888531 2050462464 solver.cpp:189] Iteration 5360, loss = 0.028798
I0503 03:03:31.888586 2050462464 solver.cpp:204]     Train net output #0: loss = 0.028798 (* 1 = 0.028798 loss)
I0503 03:03:31.888597 2050462464 solver.cpp:464] Iteration 5360, lr = 0.001
I0503 03:04:54.739835 2050462464 solver.cpp:189] Iteration 5380, loss = 0.035414
I0503 03:04:54.739892 2050462464 solver.cpp:204]     Train net output #0: loss = 0.035414 (* 1 = 0.035414 loss)
I0503 03:04:54.739900 2050462464 solver.cpp:464] Iteration 5380, lr = 0.001
I0503 03:06:17.695634 2050462464 solver.cpp:189] Iteration 5400, loss = 0.00576671
I0503 03:06:17.695690 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00576665 (* 1 = 0.00576665 loss)
I0503 03:06:17.695699 2050462464 solver.cpp:464] Iteration 5400, lr = 0.001
I0503 03:07:40.611565 2050462464 solver.cpp:189] Iteration 5420, loss = 0.00807661
I0503 03:07:40.611614 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00807655 (* 1 = 0.00807655 loss)
I0503 03:07:40.611624 2050462464 solver.cpp:464] Iteration 5420, lr = 0.001
I0503 03:09:03.201596 2050462464 solver.cpp:189] Iteration 5440, loss = 0.0134307
I0503 03:09:03.201653 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0134306 (* 1 = 0.0134306 loss)
I0503 03:09:03.201663 2050462464 solver.cpp:464] Iteration 5440, lr = 0.001
I0503 03:10:25.953199 2050462464 solver.cpp:189] Iteration 5460, loss = 0.00174254
I0503 03:10:25.953254 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00174248 (* 1 = 0.00174248 loss)
I0503 03:10:25.953264 2050462464 solver.cpp:464] Iteration 5460, lr = 0.001
I0503 03:11:48.971006 2050462464 solver.cpp:189] Iteration 5480, loss = 0.00570349
I0503 03:11:48.971060 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00570343 (* 1 = 0.00570343 loss)
I0503 03:11:48.971070 2050462464 solver.cpp:464] Iteration 5480, lr = 0.001
I0503 03:13:08.462394 2050462464 solver.cpp:334] Snapshotting to caffenet_train_background_iter_5500.caffemodel
I0503 03:13:10.033645 2050462464 solver.cpp:342] Snapshotting solver state to caffenet_train_background_iter_5500.solverstate
I0503 03:13:15.299871 2050462464 solver.cpp:189] Iteration 5500, loss = 0.0513757
I0503 03:13:15.299907 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0513756 (* 1 = 0.0513756 loss)
I0503 03:13:15.299917 2050462464 solver.cpp:464] Iteration 5500, lr = 0.001
I0503 03:14:38.214814 2050462464 solver.cpp:189] Iteration 5520, loss = 0.00523783
I0503 03:14:38.214892 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00523778 (* 1 = 0.00523778 loss)
I0503 03:14:38.214903 2050462464 solver.cpp:464] Iteration 5520, lr = 0.001
I0503 03:16:01.161134 2050462464 solver.cpp:189] Iteration 5540, loss = 0.0170683
I0503 03:16:01.161188 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0170683 (* 1 = 0.0170683 loss)
I0503 03:16:01.161198 2050462464 solver.cpp:464] Iteration 5540, lr = 0.001
I0503 03:17:24.406302 2050462464 solver.cpp:189] Iteration 5560, loss = 0.0486642
I0503 03:17:24.406355 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0486641 (* 1 = 0.0486641 loss)
I0503 03:17:24.406365 2050462464 solver.cpp:464] Iteration 5560, lr = 0.001
I0503 03:18:47.166100 2050462464 solver.cpp:189] Iteration 5580, loss = 0.0429723
I0503 03:18:47.166242 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0429722 (* 1 = 0.0429722 loss)
I0503 03:18:47.166255 2050462464 solver.cpp:464] Iteration 5580, lr = 0.001
I0503 03:20:10.459614 2050462464 solver.cpp:189] Iteration 5600, loss = 0.0408537
I0503 03:20:10.459666 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0408537 (* 1 = 0.0408537 loss)
I0503 03:20:10.459676 2050462464 solver.cpp:464] Iteration 5600, lr = 0.001
I0503 03:21:33.104253 2050462464 solver.cpp:189] Iteration 5620, loss = 0.00342338
I0503 03:21:33.104307 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00342333 (* 1 = 0.00342333 loss)
I0503 03:21:33.104317 2050462464 solver.cpp:464] Iteration 5620, lr = 0.001
I0503 03:22:55.947552 2050462464 solver.cpp:189] Iteration 5640, loss = 0.000491489
I0503 03:22:55.947635 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000491435 (* 1 = 0.000491435 loss)
I0503 03:22:55.947646 2050462464 solver.cpp:464] Iteration 5640, lr = 0.001
I0503 03:24:19.496510 2050462464 solver.cpp:189] Iteration 5660, loss = 0.00111017
I0503 03:24:19.496567 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00111011 (* 1 = 0.00111011 loss)
I0503 03:24:19.496577 2050462464 solver.cpp:464] Iteration 5660, lr = 0.001
I0503 03:25:42.457159 2050462464 solver.cpp:189] Iteration 5680, loss = 0.00275822
I0503 03:25:42.457207 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00275817 (* 1 = 0.00275817 loss)
I0503 03:25:42.457217 2050462464 solver.cpp:464] Iteration 5680, lr = 0.001
I0503 03:27:05.211801 2050462464 solver.cpp:189] Iteration 5700, loss = 0.00240734
I0503 03:27:05.211856 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00240728 (* 1 = 0.00240728 loss)
I0503 03:27:05.211866 2050462464 solver.cpp:464] Iteration 5700, lr = 0.001
I0503 03:28:28.008445 2050462464 solver.cpp:189] Iteration 5720, loss = 0.000726093
I0503 03:28:28.008496 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000726027 (* 1 = 0.000726027 loss)
I0503 03:28:28.008505 2050462464 solver.cpp:464] Iteration 5720, lr = 0.001
I0503 03:29:50.740070 2050462464 solver.cpp:189] Iteration 5740, loss = 0.000968195
I0503 03:29:50.740113 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000968134 (* 1 = 0.000968134 loss)
I0503 03:29:50.740123 2050462464 solver.cpp:464] Iteration 5740, lr = 0.001
I0503 03:31:13.588721 2050462464 solver.cpp:189] Iteration 5760, loss = 0.000148721
I0503 03:31:13.588767 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000148666 (* 1 = 0.000148666 loss)
I0503 03:31:13.588776 2050462464 solver.cpp:464] Iteration 5760, lr = 0.001
I0503 03:32:36.426142 2050462464 solver.cpp:189] Iteration 5780, loss = 0.00720591
I0503 03:32:36.426190 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00720585 (* 1 = 0.00720585 loss)
I0503 03:32:36.426199 2050462464 solver.cpp:464] Iteration 5780, lr = 0.001
I0503 03:33:59.268230 2050462464 solver.cpp:189] Iteration 5800, loss = 0.00765675
I0503 03:33:59.268286 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0076567 (* 1 = 0.0076567 loss)
I0503 03:33:59.268296 2050462464 solver.cpp:464] Iteration 5800, lr = 0.001
I0503 03:35:22.271425 2050462464 solver.cpp:189] Iteration 5820, loss = 0.0132232
I0503 03:35:22.271502 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0132232 (* 1 = 0.0132232 loss)
I0503 03:35:22.271512 2050462464 solver.cpp:464] Iteration 5820, lr = 0.001
I0503 03:36:45.491274 2050462464 solver.cpp:189] Iteration 5840, loss = 0.0248778
I0503 03:36:45.491330 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0248778 (* 1 = 0.0248778 loss)
I0503 03:36:45.491340 2050462464 solver.cpp:464] Iteration 5840, lr = 0.001
I0503 03:38:08.615164 2050462464 solver.cpp:189] Iteration 5860, loss = 0.00296749
I0503 03:38:08.615214 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00296744 (* 1 = 0.00296744 loss)
I0503 03:38:08.615224 2050462464 solver.cpp:464] Iteration 5860, lr = 0.001
I0503 03:39:31.277606 2050462464 solver.cpp:189] Iteration 5880, loss = 0.00429273
I0503 03:39:31.277660 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00429268 (* 1 = 0.00429268 loss)
I0503 03:39:31.277670 2050462464 solver.cpp:464] Iteration 5880, lr = 0.001
I0503 03:40:54.130538 2050462464 solver.cpp:189] Iteration 5900, loss = 0.0107238
I0503 03:40:54.130589 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0107238 (* 1 = 0.0107238 loss)
I0503 03:40:54.130599 2050462464 solver.cpp:464] Iteration 5900, lr = 0.001
I0503 03:42:17.402361 2050462464 solver.cpp:189] Iteration 5920, loss = 0.00101663
I0503 03:42:17.402406 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00101658 (* 1 = 0.00101658 loss)
I0503 03:42:17.402416 2050462464 solver.cpp:464] Iteration 5920, lr = 0.001
I0503 03:43:40.215814 2050462464 solver.cpp:189] Iteration 5940, loss = 0.0187485
I0503 03:43:40.215869 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0187485 (* 1 = 0.0187485 loss)
I0503 03:43:40.215879 2050462464 solver.cpp:464] Iteration 5940, lr = 0.001
I0503 03:45:03.365288 2050462464 solver.cpp:189] Iteration 5960, loss = 0.0188617
I0503 03:45:03.365344 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0188616 (* 1 = 0.0188616 loss)
I0503 03:45:03.365353 2050462464 solver.cpp:464] Iteration 5960, lr = 0.001
I0503 03:46:26.834532 2050462464 solver.cpp:189] Iteration 5980, loss = 0.0249819
I0503 03:46:26.834589 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0249818 (* 1 = 0.0249818 loss)
I0503 03:46:26.834599 2050462464 solver.cpp:464] Iteration 5980, lr = 0.001
I0503 03:47:46.188861 2050462464 solver.cpp:334] Snapshotting to caffenet_train_background_iter_6000.caffemodel
I0503 03:47:47.747514 2050462464 solver.cpp:342] Snapshotting solver state to caffenet_train_background_iter_6000.solverstate
I0503 03:47:49.001273 2050462464 solver.cpp:266] Iteration 6000, Testing net (#0)
I0503 04:15:18.526648 2050462464 solver.cpp:315]     Test net output #0: accuracy = 0.971953
I0503 04:15:18.527542 2050462464 solver.cpp:315]     Test net output #1: loss = 0.118883 (* 1 = 0.118883 loss)
I0503 04:15:22.453770 2050462464 solver.cpp:189] Iteration 6000, loss = 0.00221767
I0503 04:15:22.453811 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00221761 (* 1 = 0.00221761 loss)
I0503 04:15:22.453821 2050462464 solver.cpp:464] Iteration 6000, lr = 0.001
I0503 04:16:45.574848 2050462464 solver.cpp:189] Iteration 6020, loss = 0.0034902
I0503 04:16:45.574903 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00349014 (* 1 = 0.00349014 loss)
I0503 04:16:45.574914 2050462464 solver.cpp:464] Iteration 6020, lr = 0.001
I0503 04:18:08.676307 2050462464 solver.cpp:189] Iteration 6040, loss = 0.00798863
I0503 04:18:08.676367 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00798857 (* 1 = 0.00798857 loss)
I0503 04:18:08.676378 2050462464 solver.cpp:464] Iteration 6040, lr = 0.001
I0503 04:19:31.276306 2050462464 solver.cpp:189] Iteration 6060, loss = 0.00735867
I0503 04:19:31.276361 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00735861 (* 1 = 0.00735861 loss)
I0503 04:19:31.276371 2050462464 solver.cpp:464] Iteration 6060, lr = 0.001
I0503 04:20:54.208986 2050462464 solver.cpp:189] Iteration 6080, loss = 0.000912104
I0503 04:20:54.209061 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000912037 (* 1 = 0.000912037 loss)
I0503 04:20:54.209072 2050462464 solver.cpp:464] Iteration 6080, lr = 0.001
I0503 04:22:17.316169 2050462464 solver.cpp:189] Iteration 6100, loss = 0.00484143
I0503 04:22:17.316225 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00484136 (* 1 = 0.00484136 loss)
I0503 04:22:17.316234 2050462464 solver.cpp:464] Iteration 6100, lr = 0.001
I0503 04:23:40.499487 2050462464 solver.cpp:189] Iteration 6120, loss = 7.89701e-05
I0503 04:23:40.499536 2050462464 solver.cpp:204]     Train net output #0: loss = 7.89017e-05 (* 1 = 7.89017e-05 loss)
I0503 04:23:40.499547 2050462464 solver.cpp:464] Iteration 6120, lr = 0.001
I0503 04:25:03.521121 2050462464 solver.cpp:189] Iteration 6140, loss = 0.00218198
I0503 04:25:03.521178 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00218193 (* 1 = 0.00218193 loss)
I0503 04:25:03.521188 2050462464 solver.cpp:464] Iteration 6140, lr = 0.001
I0503 04:26:26.467635 2050462464 solver.cpp:189] Iteration 6160, loss = 0.0195983
I0503 04:26:26.467684 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0195982 (* 1 = 0.0195982 loss)
I0503 04:26:26.467694 2050462464 solver.cpp:464] Iteration 6160, lr = 0.001
I0503 04:27:49.250396 2050462464 solver.cpp:189] Iteration 6180, loss = 0.00882723
I0503 04:27:49.250452 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00882717 (* 1 = 0.00882717 loss)
I0503 04:27:49.250460 2050462464 solver.cpp:464] Iteration 6180, lr = 0.001
I0503 04:29:12.314453 2050462464 solver.cpp:189] Iteration 6200, loss = 0.0680341
I0503 04:29:12.314512 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0680341 (* 1 = 0.0680341 loss)
I0503 04:29:12.314523 2050462464 solver.cpp:464] Iteration 6200, lr = 0.001
I0503 04:30:35.001919 2050462464 solver.cpp:189] Iteration 6220, loss = 0.000337684
I0503 04:30:35.001972 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000337628 (* 1 = 0.000337628 loss)
I0503 04:30:35.001982 2050462464 solver.cpp:464] Iteration 6220, lr = 0.001
I0503 04:31:57.974725 2050462464 solver.cpp:189] Iteration 6240, loss = 0.000763197
I0503 04:31:57.974771 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000763141 (* 1 = 0.000763141 loss)
I0503 04:31:57.974781 2050462464 solver.cpp:464] Iteration 6240, lr = 0.001
I0503 04:33:21.249205 2050462464 solver.cpp:189] Iteration 6260, loss = 0.00613794
I0503 04:33:21.249261 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00613788 (* 1 = 0.00613788 loss)
I0503 04:33:21.249271 2050462464 solver.cpp:464] Iteration 6260, lr = 0.001
I0503 04:34:44.310629 2050462464 solver.cpp:189] Iteration 6280, loss = 0.0160258
I0503 04:34:44.310679 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0160257 (* 1 = 0.0160257 loss)
I0503 04:34:44.310689 2050462464 solver.cpp:464] Iteration 6280, lr = 0.001
I0503 04:36:07.301205 2050462464 solver.cpp:189] Iteration 6300, loss = 0.018508
I0503 04:36:07.301257 2050462464 solver.cpp:204]     Train net output #0: loss = 0.018508 (* 1 = 0.018508 loss)
I0503 04:36:07.301267 2050462464 solver.cpp:464] Iteration 6300, lr = 0.001
I0503 04:37:30.545821 2050462464 solver.cpp:189] Iteration 6320, loss = 0.024869
I0503 04:37:30.545866 2050462464 solver.cpp:204]     Train net output #0: loss = 0.024869 (* 1 = 0.024869 loss)
I0503 04:37:30.545876 2050462464 solver.cpp:464] Iteration 6320, lr = 0.001
I0503 04:38:53.461508 2050462464 solver.cpp:189] Iteration 6340, loss = 0.00241773
I0503 04:38:53.461562 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00241767 (* 1 = 0.00241767 loss)
I0503 04:38:53.461572 2050462464 solver.cpp:464] Iteration 6340, lr = 0.001
I0503 04:40:16.277220 2050462464 solver.cpp:189] Iteration 6360, loss = 0.0050236
I0503 04:40:16.277277 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00502354 (* 1 = 0.00502354 loss)
I0503 04:40:16.277287 2050462464 solver.cpp:464] Iteration 6360, lr = 0.001
I0503 04:41:39.131593 2050462464 solver.cpp:189] Iteration 6380, loss = 0.00247258
I0503 04:41:39.131661 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00247251 (* 1 = 0.00247251 loss)
I0503 04:41:39.131671 2050462464 solver.cpp:464] Iteration 6380, lr = 0.001
I0503 04:43:02.102686 2050462464 solver.cpp:189] Iteration 6400, loss = 0.00449801
I0503 04:43:02.102740 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00449794 (* 1 = 0.00449794 loss)
I0503 04:43:02.102749 2050462464 solver.cpp:464] Iteration 6400, lr = 0.001
I0503 04:44:25.299499 2050462464 solver.cpp:189] Iteration 6420, loss = 0.0548107
I0503 04:44:25.299553 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0548106 (* 1 = 0.0548106 loss)
I0503 04:44:25.299563 2050462464 solver.cpp:464] Iteration 6420, lr = 0.001
I0503 04:45:48.154573 2050462464 solver.cpp:189] Iteration 6440, loss = 0.00915493
I0503 04:45:48.154623 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00915486 (* 1 = 0.00915486 loss)
I0503 04:45:48.154633 2050462464 solver.cpp:464] Iteration 6440, lr = 0.001
I0503 04:47:11.245576 2050462464 solver.cpp:189] Iteration 6460, loss = 0.000259176
I0503 04:47:11.245631 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000259102 (* 1 = 0.000259102 loss)
I0503 04:47:11.245641 2050462464 solver.cpp:464] Iteration 6460, lr = 0.001
I0503 04:48:33.999575 2050462464 solver.cpp:189] Iteration 6480, loss = 0.0024834
I0503 04:48:33.999631 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00248332 (* 1 = 0.00248332 loss)
I0503 04:48:33.999644 2050462464 solver.cpp:464] Iteration 6480, lr = 0.001
I0503 04:49:53.272186 2050462464 solver.cpp:334] Snapshotting to caffenet_train_background_iter_6500.caffemodel
I0503 04:49:54.848485 2050462464 solver.cpp:342] Snapshotting solver state to caffenet_train_background_iter_6500.solverstate
I0503 04:50:00.024745 2050462464 solver.cpp:189] Iteration 6500, loss = 0.00194708
I0503 04:50:00.024786 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00194701 (* 1 = 0.00194701 loss)
I0503 04:50:00.024796 2050462464 solver.cpp:464] Iteration 6500, lr = 0.001
I0503 04:51:22.863723 2050462464 solver.cpp:189] Iteration 6520, loss = 0.000204576
I0503 04:51:22.863772 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000204497 (* 1 = 0.000204497 loss)
I0503 04:51:22.863782 2050462464 solver.cpp:464] Iteration 6520, lr = 0.001
I0503 04:52:45.640290 2050462464 solver.cpp:189] Iteration 6540, loss = 0.00364044
I0503 04:52:45.640347 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00364036 (* 1 = 0.00364036 loss)
I0503 04:52:45.640357 2050462464 solver.cpp:464] Iteration 6540, lr = 0.001
I0503 04:54:08.825018 2050462464 solver.cpp:189] Iteration 6560, loss = 0.0113248
I0503 04:54:08.825069 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0113247 (* 1 = 0.0113247 loss)
I0503 04:54:08.825079 2050462464 solver.cpp:464] Iteration 6560, lr = 0.001
I0503 04:55:31.756549 2050462464 solver.cpp:189] Iteration 6580, loss = 0.0212901
I0503 04:55:31.756681 2050462464 solver.cpp:204]     Train net output #0: loss = 0.02129 (* 1 = 0.02129 loss)
I0503 04:55:31.756692 2050462464 solver.cpp:464] Iteration 6580, lr = 0.001
I0503 04:56:55.011916 2050462464 solver.cpp:189] Iteration 6600, loss = 0.00515464
I0503 04:56:55.011970 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00515457 (* 1 = 0.00515457 loss)
I0503 04:56:55.011979 2050462464 solver.cpp:464] Iteration 6600, lr = 0.001
I0503 04:58:17.831293 2050462464 solver.cpp:189] Iteration 6620, loss = 0.0637296
I0503 04:58:17.831343 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0637295 (* 1 = 0.0637295 loss)
I0503 04:58:17.831353 2050462464 solver.cpp:464] Iteration 6620, lr = 0.001
I0503 04:59:40.744323 2050462464 solver.cpp:189] Iteration 6640, loss = 0.00737328
I0503 04:59:40.744369 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00737321 (* 1 = 0.00737321 loss)
I0503 04:59:40.744384 2050462464 solver.cpp:464] Iteration 6640, lr = 0.001
I0503 05:01:03.405117 2050462464 solver.cpp:189] Iteration 6660, loss = 0.0196757
I0503 05:01:03.405196 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0196756 (* 1 = 0.0196756 loss)
I0503 05:01:03.405206 2050462464 solver.cpp:464] Iteration 6660, lr = 0.001
I0503 05:02:26.696627 2050462464 solver.cpp:189] Iteration 6680, loss = 0.0010479
I0503 05:02:26.696683 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00104784 (* 1 = 0.00104784 loss)
I0503 05:02:26.696693 2050462464 solver.cpp:464] Iteration 6680, lr = 0.001
I0503 05:03:49.337529 2050462464 solver.cpp:189] Iteration 6700, loss = 0.00189363
I0503 05:03:49.337581 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00189357 (* 1 = 0.00189357 loss)
I0503 05:03:49.337591 2050462464 solver.cpp:464] Iteration 6700, lr = 0.001
I0503 05:05:12.516564 2050462464 solver.cpp:189] Iteration 6720, loss = 0.00747587
I0503 05:05:12.516613 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0074758 (* 1 = 0.0074758 loss)
I0503 05:05:12.516621 2050462464 solver.cpp:464] Iteration 6720, lr = 0.001
I0503 05:06:35.509029 2050462464 solver.cpp:189] Iteration 6740, loss = 0.0212271
I0503 05:06:35.509075 2050462464 solver.cpp:204]     Train net output #0: loss = 0.021227 (* 1 = 0.021227 loss)
I0503 05:06:35.509084 2050462464 solver.cpp:464] Iteration 6740, lr = 0.001
I0503 05:07:58.373941 2050462464 solver.cpp:189] Iteration 6760, loss = 0.0197514
I0503 05:07:58.373998 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0197513 (* 1 = 0.0197513 loss)
I0503 05:07:58.374008 2050462464 solver.cpp:464] Iteration 6760, lr = 0.001
I0503 05:09:21.565044 2050462464 solver.cpp:189] Iteration 6780, loss = 0.00361523
I0503 05:09:21.565093 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00361516 (* 1 = 0.00361516 loss)
I0503 05:09:21.565103 2050462464 solver.cpp:464] Iteration 6780, lr = 0.001
I0503 05:10:44.362269 2050462464 solver.cpp:189] Iteration 6800, loss = 0.0064397
I0503 05:10:44.362325 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00643963 (* 1 = 0.00643963 loss)
I0503 05:10:44.362335 2050462464 solver.cpp:464] Iteration 6800, lr = 0.001
I0503 05:12:07.589491 2050462464 solver.cpp:189] Iteration 6820, loss = 0.0376852
I0503 05:12:07.589550 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0376851 (* 1 = 0.0376851 loss)
I0503 05:12:07.589558 2050462464 solver.cpp:464] Iteration 6820, lr = 0.001
I0503 05:13:30.257607 2050462464 solver.cpp:189] Iteration 6840, loss = 0.00138356
I0503 05:13:30.257655 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00138349 (* 1 = 0.00138349 loss)
I0503 05:13:30.257666 2050462464 solver.cpp:464] Iteration 6840, lr = 0.001
I0503 05:14:53.367632 2050462464 solver.cpp:189] Iteration 6860, loss = 5.08622e-05
I0503 05:14:53.367687 2050462464 solver.cpp:204]     Train net output #0: loss = 5.07951e-05 (* 1 = 5.07951e-05 loss)
I0503 05:14:53.367697 2050462464 solver.cpp:464] Iteration 6860, lr = 0.001
I0503 05:16:16.321003 2050462464 solver.cpp:189] Iteration 6880, loss = 0.0149109
I0503 05:16:16.321059 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0149108 (* 1 = 0.0149108 loss)
I0503 05:16:16.321069 2050462464 solver.cpp:464] Iteration 6880, lr = 0.001
I0503 05:17:39.437634 2050462464 solver.cpp:189] Iteration 6900, loss = 0.00356832
I0503 05:17:39.437690 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00356825 (* 1 = 0.00356825 loss)
I0503 05:17:39.437701 2050462464 solver.cpp:464] Iteration 6900, lr = 0.001
I0503 05:19:02.382971 2050462464 solver.cpp:189] Iteration 6920, loss = 0.0330401
I0503 05:19:02.383034 2050462464 solver.cpp:204]     Train net output #0: loss = 0.03304 (* 1 = 0.03304 loss)
I0503 05:19:02.383044 2050462464 solver.cpp:464] Iteration 6920, lr = 0.001
I0503 05:20:25.328622 2050462464 solver.cpp:189] Iteration 6940, loss = 0.017811
I0503 05:20:25.328673 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0178109 (* 1 = 0.0178109 loss)
I0503 05:20:25.328683 2050462464 solver.cpp:464] Iteration 6940, lr = 0.001
I0503 05:21:48.033522 2050462464 solver.cpp:189] Iteration 6960, loss = 0.0266081
I0503 05:21:48.033610 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0266081 (* 1 = 0.0266081 loss)
I0503 05:21:48.033620 2050462464 solver.cpp:464] Iteration 6960, lr = 0.001
I0503 05:23:11.065791 2050462464 solver.cpp:189] Iteration 6980, loss = 0.000398798
I0503 05:23:11.065842 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00039873 (* 1 = 0.00039873 loss)
I0503 05:23:11.065851 2050462464 solver.cpp:464] Iteration 6980, lr = 0.001
I0503 05:24:30.399363 2050462464 solver.cpp:334] Snapshotting to caffenet_train_background_iter_7000.caffemodel
I0503 05:24:31.950922 2050462464 solver.cpp:342] Snapshotting solver state to caffenet_train_background_iter_7000.solverstate
I0503 05:24:33.191772 2050462464 solver.cpp:266] Iteration 7000, Testing net (#0)
I0503 05:52:01.899441 2050462464 solver.cpp:315]     Test net output #0: accuracy = 0.969813
I0503 05:52:01.899492 2050462464 solver.cpp:315]     Test net output #1: loss = 0.102035 (* 1 = 0.102035 loss)
I0503 05:52:05.991312 2050462464 solver.cpp:189] Iteration 7000, loss = 0.0034897
I0503 05:52:05.991351 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00348962 (* 1 = 0.00348962 loss)
I0503 05:52:05.991360 2050462464 solver.cpp:464] Iteration 7000, lr = 0.001
I0503 05:53:28.906975 2050462464 solver.cpp:189] Iteration 7020, loss = 0.0107199
I0503 05:53:28.907030 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0107198 (* 1 = 0.0107198 loss)
I0503 05:53:28.907040 2050462464 solver.cpp:464] Iteration 7020, lr = 0.001
I0503 05:54:51.650771 2050462464 solver.cpp:189] Iteration 7040, loss = 0.000799331
I0503 05:54:51.650825 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000799251 (* 1 = 0.000799251 loss)
I0503 05:54:51.650835 2050462464 solver.cpp:464] Iteration 7040, lr = 0.001
I0503 05:56:14.788214 2050462464 solver.cpp:189] Iteration 7060, loss = 0.00190095
I0503 05:56:14.788357 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00190087 (* 1 = 0.00190087 loss)
I0503 05:56:14.788368 2050462464 solver.cpp:464] Iteration 7060, lr = 0.001
I0503 05:57:37.850057 2050462464 solver.cpp:189] Iteration 7080, loss = 0.0427309
I0503 05:57:37.850105 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0427309 (* 1 = 0.0427309 loss)
I0503 05:57:37.850114 2050462464 solver.cpp:464] Iteration 7080, lr = 0.001
I0503 05:59:00.768120 2050462464 solver.cpp:189] Iteration 7100, loss = 0.00161145
I0503 05:59:00.768172 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00161136 (* 1 = 0.00161136 loss)
I0503 05:59:00.768182 2050462464 solver.cpp:464] Iteration 7100, lr = 0.001
I0503 06:00:23.826524 2050462464 solver.cpp:189] Iteration 7120, loss = 0.00193483
I0503 06:00:23.826578 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00193475 (* 1 = 0.00193475 loss)
I0503 06:00:23.826588 2050462464 solver.cpp:464] Iteration 7120, lr = 0.001
I0503 06:01:46.892017 2050462464 solver.cpp:189] Iteration 7140, loss = 0.0108162
I0503 06:01:46.892076 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0108161 (* 1 = 0.0108161 loss)
I0503 06:01:46.892086 2050462464 solver.cpp:464] Iteration 7140, lr = 0.001
I0503 06:03:09.986088 2050462464 solver.cpp:189] Iteration 7160, loss = 0.0121899
I0503 06:03:09.986142 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0121898 (* 1 = 0.0121898 loss)
I0503 06:03:09.986151 2050462464 solver.cpp:464] Iteration 7160, lr = 0.001
I0503 06:04:33.000197 2050462464 solver.cpp:189] Iteration 7180, loss = 0.0652927
I0503 06:04:33.000252 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0652926 (* 1 = 0.0652926 loss)
I0503 06:04:33.000262 2050462464 solver.cpp:464] Iteration 7180, lr = 0.001
I0503 06:05:55.943353 2050462464 solver.cpp:189] Iteration 7200, loss = 0.00619019
I0503 06:05:55.943402 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00619012 (* 1 = 0.00619012 loss)
I0503 06:05:55.943411 2050462464 solver.cpp:464] Iteration 7200, lr = 0.001
I0503 06:07:18.798019 2050462464 solver.cpp:189] Iteration 7220, loss = 0.0010463
I0503 06:07:18.798094 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00104623 (* 1 = 0.00104623 loss)
I0503 06:07:18.798104 2050462464 solver.cpp:464] Iteration 7220, lr = 0.001
I0503 06:08:41.961647 2050462464 solver.cpp:189] Iteration 7240, loss = 0.00240959
I0503 06:08:41.961699 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00240951 (* 1 = 0.00240951 loss)
I0503 06:08:41.961709 2050462464 solver.cpp:464] Iteration 7240, lr = 0.001
I0503 06:10:05.075598 2050462464 solver.cpp:189] Iteration 7260, loss = 0.00160803
I0503 06:10:05.075646 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00160795 (* 1 = 0.00160795 loss)
I0503 06:10:05.075656 2050462464 solver.cpp:464] Iteration 7260, lr = 0.001
I0503 06:11:28.289574 2050462464 solver.cpp:189] Iteration 7280, loss = 0.00160305
I0503 06:11:28.289621 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00160298 (* 1 = 0.00160298 loss)
I0503 06:11:28.289631 2050462464 solver.cpp:464] Iteration 7280, lr = 0.001
I0503 06:12:51.091027 2050462464 solver.cpp:189] Iteration 7300, loss = 0.00803316
I0503 06:12:51.091085 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00803309 (* 1 = 0.00803309 loss)
I0503 06:12:51.091095 2050462464 solver.cpp:464] Iteration 7300, lr = 0.001
I0503 06:14:14.091545 2050462464 solver.cpp:189] Iteration 7320, loss = 0.00386915
I0503 06:14:14.091600 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00386907 (* 1 = 0.00386907 loss)
I0503 06:14:14.091610 2050462464 solver.cpp:464] Iteration 7320, lr = 0.001
I0503 06:15:37.169486 2050462464 solver.cpp:189] Iteration 7340, loss = 0.000678295
I0503 06:15:37.169543 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000678217 (* 1 = 0.000678217 loss)
I0503 06:15:37.169553 2050462464 solver.cpp:464] Iteration 7340, lr = 0.001
I0503 06:17:00.709360 2050462464 solver.cpp:189] Iteration 7360, loss = 0.00112793
I0503 06:17:00.709416 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00112785 (* 1 = 0.00112785 loss)
I0503 06:17:00.709426 2050462464 solver.cpp:464] Iteration 7360, lr = 0.001
I0503 06:18:23.552368 2050462464 solver.cpp:189] Iteration 7380, loss = 0.0116161
I0503 06:18:23.552424 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0116161 (* 1 = 0.0116161 loss)
I0503 06:18:23.552434 2050462464 solver.cpp:464] Iteration 7380, lr = 0.001
I0503 06:19:46.294164 2050462464 solver.cpp:189] Iteration 7400, loss = 0.00475075
I0503 06:19:46.294215 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00475068 (* 1 = 0.00475068 loss)
I0503 06:19:46.294224 2050462464 solver.cpp:464] Iteration 7400, lr = 0.001
I0503 06:21:09.159147 2050462464 solver.cpp:189] Iteration 7420, loss = 0.0410412
I0503 06:21:09.159194 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0410411 (* 1 = 0.0410411 loss)
I0503 06:21:09.159204 2050462464 solver.cpp:464] Iteration 7420, lr = 0.001
I0503 06:22:32.171607 2050462464 solver.cpp:189] Iteration 7440, loss = 0.00106654
I0503 06:22:32.171655 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00106646 (* 1 = 0.00106646 loss)
I0503 06:22:32.171664 2050462464 solver.cpp:464] Iteration 7440, lr = 0.001
I0503 06:23:54.650480 2050462464 solver.cpp:189] Iteration 7460, loss = 0.0102987
I0503 06:23:54.650537 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0102986 (* 1 = 0.0102986 loss)
I0503 06:23:54.650547 2050462464 solver.cpp:464] Iteration 7460, lr = 0.001
I0503 06:25:17.817332 2050462464 solver.cpp:189] Iteration 7480, loss = 0.00191961
I0503 06:25:17.817387 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00191953 (* 1 = 0.00191953 loss)
I0503 06:25:17.817397 2050462464 solver.cpp:464] Iteration 7480, lr = 0.001
I0503 06:26:37.298074 2050462464 solver.cpp:334] Snapshotting to caffenet_train_background_iter_7500.caffemodel
I0503 06:26:38.815855 2050462464 solver.cpp:342] Snapshotting solver state to caffenet_train_background_iter_7500.solverstate
I0503 06:26:43.966446 2050462464 solver.cpp:189] Iteration 7500, loss = 0.00143584
I0503 06:26:43.966486 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00143576 (* 1 = 0.00143576 loss)
I0503 06:26:43.966496 2050462464 solver.cpp:464] Iteration 7500, lr = 0.001
I0503 06:28:07.094894 2050462464 solver.cpp:189] Iteration 7520, loss = 0.0165333
I0503 06:28:07.095773 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0165333 (* 1 = 0.0165333 loss)
I0503 06:28:07.095787 2050462464 solver.cpp:464] Iteration 7520, lr = 0.001
I0503 06:29:29.847259 2050462464 solver.cpp:189] Iteration 7540, loss = 0.00532694
I0503 06:29:29.847314 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00532685 (* 1 = 0.00532685 loss)
I0503 06:29:29.847324 2050462464 solver.cpp:464] Iteration 7540, lr = 0.001
I0503 06:30:52.836685 2050462464 solver.cpp:189] Iteration 7560, loss = 0.0122431
I0503 06:30:52.836738 2050462464 solver.cpp:204]     Train net output #0: loss = 0.012243 (* 1 = 0.012243 loss)
I0503 06:30:52.836748 2050462464 solver.cpp:464] Iteration 7560, lr = 0.001
I0503 06:32:16.181485 2050462464 solver.cpp:189] Iteration 7580, loss = 0.000608418
I0503 06:32:16.181535 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000608332 (* 1 = 0.000608332 loss)
I0503 06:32:16.181545 2050462464 solver.cpp:464] Iteration 7580, lr = 0.001
I0503 06:33:39.128501 2050462464 solver.cpp:189] Iteration 7600, loss = 0.00129257
I0503 06:33:39.128556 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00129248 (* 1 = 0.00129248 loss)
I0503 06:33:39.128566 2050462464 solver.cpp:464] Iteration 7600, lr = 0.001
I0503 06:35:01.907433 2050462464 solver.cpp:189] Iteration 7620, loss = 0.00064511
I0503 06:35:01.907477 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000645018 (* 1 = 0.000645018 loss)
I0503 06:35:01.907487 2050462464 solver.cpp:464] Iteration 7620, lr = 0.001
I0503 06:36:25.033340 2050462464 solver.cpp:189] Iteration 7640, loss = 0.0134603
I0503 06:36:25.033393 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0134602 (* 1 = 0.0134602 loss)
I0503 06:36:25.033403 2050462464 solver.cpp:464] Iteration 7640, lr = 0.001
I0503 06:37:48.282387 2050462464 solver.cpp:189] Iteration 7660, loss = 0.00472064
I0503 06:37:48.282444 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00472054 (* 1 = 0.00472054 loss)
I0503 06:37:48.282454 2050462464 solver.cpp:464] Iteration 7660, lr = 0.001
I0503 06:39:11.375720 2050462464 solver.cpp:189] Iteration 7680, loss = 0.000776695
I0503 06:39:11.375777 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000776606 (* 1 = 0.000776606 loss)
I0503 06:39:11.375787 2050462464 solver.cpp:464] Iteration 7680, lr = 0.001
I0503 06:40:34.360565 2050462464 solver.cpp:189] Iteration 7700, loss = 0.0150852
I0503 06:40:34.360620 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0150851 (* 1 = 0.0150851 loss)
I0503 06:40:34.360630 2050462464 solver.cpp:464] Iteration 7700, lr = 0.001
I0503 06:41:57.117666 2050462464 solver.cpp:189] Iteration 7720, loss = 0.000272269
I0503 06:41:57.117722 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00027218 (* 1 = 0.00027218 loss)
I0503 06:41:57.117733 2050462464 solver.cpp:464] Iteration 7720, lr = 0.001
I0503 06:43:20.287401 2050462464 solver.cpp:189] Iteration 7740, loss = 0.00041971
I0503 06:43:20.287457 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000419622 (* 1 = 0.000419622 loss)
I0503 06:43:20.287468 2050462464 solver.cpp:464] Iteration 7740, lr = 0.001
I0503 06:44:43.132762 2050462464 solver.cpp:189] Iteration 7760, loss = 0.0091871
I0503 06:44:43.132829 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00918701 (* 1 = 0.00918701 loss)
I0503 06:44:43.132839 2050462464 solver.cpp:464] Iteration 7760, lr = 0.001
I0503 06:46:05.913892 2050462464 solver.cpp:189] Iteration 7780, loss = 0.0113339
I0503 06:46:05.913945 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0113339 (* 1 = 0.0113339 loss)
I0503 06:46:05.913956 2050462464 solver.cpp:464] Iteration 7780, lr = 0.001
I0503 06:47:29.110569 2050462464 solver.cpp:189] Iteration 7800, loss = 0.00503179
I0503 06:47:29.110644 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00503169 (* 1 = 0.00503169 loss)
I0503 06:47:29.110656 2050462464 solver.cpp:464] Iteration 7800, lr = 0.001
I0503 06:48:51.934633 2050462464 solver.cpp:189] Iteration 7820, loss = 0.00852502
I0503 06:48:51.934689 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00852492 (* 1 = 0.00852492 loss)
I0503 06:48:51.934700 2050462464 solver.cpp:464] Iteration 7820, lr = 0.001
I0503 06:50:15.224989 2050462464 solver.cpp:189] Iteration 7840, loss = 0.0275547
I0503 06:50:15.225045 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0275546 (* 1 = 0.0275546 loss)
I0503 06:50:15.225056 2050462464 solver.cpp:464] Iteration 7840, lr = 0.001
I0503 06:51:38.158370 2050462464 solver.cpp:189] Iteration 7860, loss = 0.0022773
I0503 06:51:38.158426 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00227719 (* 1 = 0.00227719 loss)
I0503 06:51:38.158435 2050462464 solver.cpp:464] Iteration 7860, lr = 0.001
I0503 06:53:00.925452 2050462464 solver.cpp:189] Iteration 7880, loss = 0.000356289
I0503 06:53:00.925505 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000356179 (* 1 = 0.000356179 loss)
I0503 06:53:00.925515 2050462464 solver.cpp:464] Iteration 7880, lr = 0.001
I0503 06:54:23.947465 2050462464 solver.cpp:189] Iteration 7900, loss = 0.0396128
I0503 06:54:23.947511 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0396127 (* 1 = 0.0396127 loss)
I0503 06:54:23.947520 2050462464 solver.cpp:464] Iteration 7900, lr = 0.001
I0503 06:55:47.004705 2050462464 solver.cpp:189] Iteration 7920, loss = 0.0132944
I0503 06:55:47.004776 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0132943 (* 1 = 0.0132943 loss)
I0503 06:55:47.004788 2050462464 solver.cpp:464] Iteration 7920, lr = 0.001
I0503 06:57:10.407004 2050462464 solver.cpp:189] Iteration 7940, loss = 0.00427244
I0503 06:57:10.407132 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00427233 (* 1 = 0.00427233 loss)
I0503 06:57:10.407143 2050462464 solver.cpp:464] Iteration 7940, lr = 0.001
I0503 06:58:33.503561 2050462464 solver.cpp:189] Iteration 7960, loss = 0.000459377
I0503 06:58:33.503617 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00045927 (* 1 = 0.00045927 loss)
I0503 06:58:33.503626 2050462464 solver.cpp:464] Iteration 7960, lr = 0.001
I0503 06:59:56.535169 2050462464 solver.cpp:189] Iteration 7980, loss = 0.00284086
I0503 06:59:56.535218 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00284075 (* 1 = 0.00284075 loss)
I0503 06:59:56.535228 2050462464 solver.cpp:464] Iteration 7980, lr = 0.001
I0503 07:01:16.363296 2050462464 solver.cpp:334] Snapshotting to caffenet_train_background_iter_8000.caffemodel
I0503 07:01:17.885030 2050462464 solver.cpp:342] Snapshotting solver state to caffenet_train_background_iter_8000.solverstate
I0503 07:01:19.140293 2050462464 solver.cpp:266] Iteration 8000, Testing net (#0)
I0503 07:28:54.326489 2050462464 solver.cpp:315]     Test net output #0: accuracy = 0.973922
I0503 07:28:54.326531 2050462464 solver.cpp:315]     Test net output #1: loss = 0.121043 (* 1 = 0.121043 loss)
I0503 07:28:58.251561 2050462464 solver.cpp:189] Iteration 8000, loss = 0.0758373
I0503 07:28:58.251601 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0758372 (* 1 = 0.0758372 loss)
I0503 07:28:58.251610 2050462464 solver.cpp:464] Iteration 8000, lr = 0.001
I0503 07:30:21.496554 2050462464 solver.cpp:189] Iteration 8020, loss = 0.00613616
I0503 07:30:21.496600 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00613605 (* 1 = 0.00613605 loss)
I0503 07:30:21.496610 2050462464 solver.cpp:464] Iteration 8020, lr = 0.001
I0503 07:31:44.605424 2050462464 solver.cpp:189] Iteration 8040, loss = 0.0211455
I0503 07:31:44.605471 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0211454 (* 1 = 0.0211454 loss)
I0503 07:31:44.605481 2050462464 solver.cpp:464] Iteration 8040, lr = 0.001
I0503 07:33:07.707386 2050462464 solver.cpp:189] Iteration 8060, loss = 0.00617267
I0503 07:33:07.707465 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00617257 (* 1 = 0.00617257 loss)
I0503 07:33:07.707476 2050462464 solver.cpp:464] Iteration 8060, lr = 0.001
I0503 07:34:30.549140 2050462464 solver.cpp:189] Iteration 8080, loss = 0.00281786
I0503 07:34:30.549199 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00281775 (* 1 = 0.00281775 loss)
I0503 07:34:30.549211 2050462464 solver.cpp:464] Iteration 8080, lr = 0.001
I0503 07:35:53.449509 2050462464 solver.cpp:189] Iteration 8100, loss = 0.00711064
I0503 07:35:53.449558 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00711052 (* 1 = 0.00711052 loss)
I0503 07:35:53.449568 2050462464 solver.cpp:464] Iteration 8100, lr = 0.001
I0503 07:37:16.947275 2050462464 solver.cpp:189] Iteration 8120, loss = 0.0470976
I0503 07:37:16.947322 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0470975 (* 1 = 0.0470975 loss)
I0503 07:37:16.947332 2050462464 solver.cpp:464] Iteration 8120, lr = 0.001
I0503 07:38:39.968283 2050462464 solver.cpp:189] Iteration 8140, loss = 0.0104897
I0503 07:38:39.968338 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0104896 (* 1 = 0.0104896 loss)
I0503 07:38:39.968348 2050462464 solver.cpp:464] Iteration 8140, lr = 0.001
I0503 07:40:03.149024 2050462464 solver.cpp:189] Iteration 8160, loss = 0.00418993
I0503 07:40:03.149076 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00418983 (* 1 = 0.00418983 loss)
I0503 07:40:03.149086 2050462464 solver.cpp:464] Iteration 8160, lr = 0.001
I0503 07:41:26.155894 2050462464 solver.cpp:189] Iteration 8180, loss = 0.000366354
I0503 07:41:26.155943 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000366246 (* 1 = 0.000366246 loss)
I0503 07:41:26.155954 2050462464 solver.cpp:464] Iteration 8180, lr = 0.001
I0503 07:42:49.193171 2050462464 solver.cpp:189] Iteration 8200, loss = 0.00711702
I0503 07:42:49.193228 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0071169 (* 1 = 0.0071169 loss)
I0503 07:42:49.193238 2050462464 solver.cpp:464] Iteration 8200, lr = 0.001
I0503 07:44:12.376047 2050462464 solver.cpp:189] Iteration 8220, loss = 0.00468361
I0503 07:44:12.376099 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0046835 (* 1 = 0.0046835 loss)
I0503 07:44:12.376109 2050462464 solver.cpp:464] Iteration 8220, lr = 0.001
I0503 07:45:35.139585 2050462464 solver.cpp:189] Iteration 8240, loss = 0.0258304
I0503 07:45:35.139633 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0258303 (* 1 = 0.0258303 loss)
I0503 07:45:35.139643 2050462464 solver.cpp:464] Iteration 8240, lr = 0.001
I0503 07:46:58.010679 2050462464 solver.cpp:189] Iteration 8260, loss = 1.66157e-05
I0503 07:46:58.010727 2050462464 solver.cpp:204]     Train net output #0: loss = 1.64989e-05 (* 1 = 1.64989e-05 loss)
I0503 07:46:58.010737 2050462464 solver.cpp:464] Iteration 8260, lr = 0.001
I0503 07:48:21.326026 2050462464 solver.cpp:189] Iteration 8280, loss = 0.0311209
I0503 07:48:21.326082 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0311207 (* 1 = 0.0311207 loss)
I0503 07:48:21.326092 2050462464 solver.cpp:464] Iteration 8280, lr = 0.001
I0503 07:49:43.987257 2050462464 solver.cpp:189] Iteration 8300, loss = 0.00578543
I0503 07:49:43.987313 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00578531 (* 1 = 0.00578531 loss)
I0503 07:49:43.987323 2050462464 solver.cpp:464] Iteration 8300, lr = 0.001
I0503 07:51:07.469236 2050462464 solver.cpp:189] Iteration 8320, loss = 0.00405985
I0503 07:51:07.469290 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00405973 (* 1 = 0.00405973 loss)
I0503 07:51:07.469300 2050462464 solver.cpp:464] Iteration 8320, lr = 0.001
I0503 07:52:30.592844 2050462464 solver.cpp:189] Iteration 8340, loss = 0.0350616
I0503 07:52:30.592897 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0350615 (* 1 = 0.0350615 loss)
I0503 07:52:30.592906 2050462464 solver.cpp:464] Iteration 8340, lr = 0.001
I0503 07:53:53.803011 2050462464 solver.cpp:189] Iteration 8360, loss = 0.0168686
I0503 07:53:53.803087 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0168685 (* 1 = 0.0168685 loss)
I0503 07:53:53.803098 2050462464 solver.cpp:464] Iteration 8360, lr = 0.001
I0503 07:55:16.920445 2050462464 solver.cpp:189] Iteration 8380, loss = 0.0238259
I0503 07:55:16.920500 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0238258 (* 1 = 0.0238258 loss)
I0503 07:55:16.920510 2050462464 solver.cpp:464] Iteration 8380, lr = 0.001
I0503 07:56:40.311738 2050462464 solver.cpp:189] Iteration 8400, loss = 0.000210588
I0503 07:56:40.311784 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000210471 (* 1 = 0.000210471 loss)
I0503 07:56:40.311794 2050462464 solver.cpp:464] Iteration 8400, lr = 0.001
I0503 07:58:03.479076 2050462464 solver.cpp:189] Iteration 8420, loss = 0.000127435
I0503 07:58:03.479223 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00012732 (* 1 = 0.00012732 loss)
I0503 07:58:03.479235 2050462464 solver.cpp:464] Iteration 8420, lr = 0.001
I0503 07:59:26.784740 2050462464 solver.cpp:189] Iteration 8440, loss = 0.0503888
I0503 07:59:26.784795 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0503887 (* 1 = 0.0503887 loss)
I0503 07:59:26.784806 2050462464 solver.cpp:464] Iteration 8440, lr = 0.001
I0503 08:00:50.060657 2050462464 solver.cpp:189] Iteration 8460, loss = 0.000629521
I0503 08:00:50.060711 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000629407 (* 1 = 0.000629407 loss)
I0503 08:00:50.060721 2050462464 solver.cpp:464] Iteration 8460, lr = 0.001
I0503 08:02:13.552613 2050462464 solver.cpp:189] Iteration 8480, loss = 0.0153576
I0503 08:02:13.552662 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0153575 (* 1 = 0.0153575 loss)
I0503 08:02:13.552672 2050462464 solver.cpp:464] Iteration 8480, lr = 0.001
I0503 08:03:32.861528 2050462464 solver.cpp:334] Snapshotting to caffenet_train_background_iter_8500.caffemodel
I0503 08:03:34.405015 2050462464 solver.cpp:342] Snapshotting solver state to caffenet_train_background_iter_8500.solverstate
I0503 08:03:39.583108 2050462464 solver.cpp:189] Iteration 8500, loss = 0.000743432
I0503 08:03:39.583148 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000743317 (* 1 = 0.000743317 loss)
I0503 08:03:39.583158 2050462464 solver.cpp:464] Iteration 8500, lr = 0.001
I0503 08:05:02.943452 2050462464 solver.cpp:189] Iteration 8520, loss = 0.00582031
I0503 08:05:02.943500 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00582019 (* 1 = 0.00582019 loss)
I0503 08:05:02.943511 2050462464 solver.cpp:464] Iteration 8520, lr = 0.001
I0503 08:06:26.529058 2050462464 solver.cpp:189] Iteration 8540, loss = 0.00048495
I0503 08:06:26.529113 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000484837 (* 1 = 0.000484837 loss)
I0503 08:06:26.529122 2050462464 solver.cpp:464] Iteration 8540, lr = 0.001
I0503 08:07:49.502713 2050462464 solver.cpp:189] Iteration 8560, loss = 0.000377519
I0503 08:07:49.502768 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000377408 (* 1 = 0.000377408 loss)
I0503 08:07:49.502779 2050462464 solver.cpp:464] Iteration 8560, lr = 0.001
I0503 08:09:12.967273 2050462464 solver.cpp:189] Iteration 8580, loss = 0.00244852
I0503 08:09:12.967327 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00244841 (* 1 = 0.00244841 loss)
I0503 08:09:12.967337 2050462464 solver.cpp:464] Iteration 8580, lr = 0.001
I0503 08:10:36.062590 2050462464 solver.cpp:189] Iteration 8600, loss = 0.0431328
I0503 08:10:36.062649 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0431327 (* 1 = 0.0431327 loss)
I0503 08:10:36.062659 2050462464 solver.cpp:464] Iteration 8600, lr = 0.001
I0503 08:11:59.126637 2050462464 solver.cpp:189] Iteration 8620, loss = 0.000341199
I0503 08:11:59.126684 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000341085 (* 1 = 0.000341085 loss)
I0503 08:11:59.126694 2050462464 solver.cpp:464] Iteration 8620, lr = 0.001
I0503 08:13:22.265624 2050462464 solver.cpp:189] Iteration 8640, loss = 0.0347716
I0503 08:13:22.265700 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0347715 (* 1 = 0.0347715 loss)
I0503 08:13:22.265710 2050462464 solver.cpp:464] Iteration 8640, lr = 0.001
I0503 08:14:45.347503 2050462464 solver.cpp:189] Iteration 8660, loss = 0.000801433
I0503 08:14:45.347561 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000801317 (* 1 = 0.000801317 loss)
I0503 08:14:45.347571 2050462464 solver.cpp:464] Iteration 8660, lr = 0.001
I0503 08:16:08.459779 2050462464 solver.cpp:189] Iteration 8680, loss = 0.00781182
I0503 08:16:08.459821 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0078117 (* 1 = 0.0078117 loss)
I0503 08:16:08.459831 2050462464 solver.cpp:464] Iteration 8680, lr = 0.001
I0503 08:17:31.687448 2050462464 solver.cpp:189] Iteration 8700, loss = 0.0030893
I0503 08:17:31.687502 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00308919 (* 1 = 0.00308919 loss)
I0503 08:17:31.687512 2050462464 solver.cpp:464] Iteration 8700, lr = 0.001
I0503 08:18:54.618363 2050462464 solver.cpp:189] Iteration 8720, loss = 0.00592143
I0503 08:18:54.618420 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00592131 (* 1 = 0.00592131 loss)
I0503 08:18:54.618430 2050462464 solver.cpp:464] Iteration 8720, lr = 0.001
I0503 08:20:17.870757 2050462464 solver.cpp:189] Iteration 8740, loss = 0.00153406
I0503 08:20:17.870813 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00153395 (* 1 = 0.00153395 loss)
I0503 08:20:17.870825 2050462464 solver.cpp:464] Iteration 8740, lr = 0.001
I0503 08:21:40.775787 2050462464 solver.cpp:189] Iteration 8760, loss = 0.000286471
I0503 08:21:40.775838 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000286356 (* 1 = 0.000286356 loss)
I0503 08:21:40.775848 2050462464 solver.cpp:464] Iteration 8760, lr = 0.001
I0503 08:23:03.880832 2050462464 solver.cpp:189] Iteration 8780, loss = 0.000449966
I0503 08:23:03.880887 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000449849 (* 1 = 0.000449849 loss)
I0503 08:23:03.880897 2050462464 solver.cpp:464] Iteration 8780, lr = 0.001
I0503 08:24:27.740128 2050462464 solver.cpp:189] Iteration 8800, loss = 0.00268571
I0503 08:24:27.740180 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00268559 (* 1 = 0.00268559 loss)
I0503 08:24:27.740190 2050462464 solver.cpp:464] Iteration 8800, lr = 0.001
I0503 08:25:50.852711 2050462464 solver.cpp:189] Iteration 8820, loss = 0.000808252
I0503 08:25:50.852758 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00080813 (* 1 = 0.00080813 loss)
I0503 08:25:50.852767 2050462464 solver.cpp:464] Iteration 8820, lr = 0.001
I0503 08:27:14.273942 2050462464 solver.cpp:189] Iteration 8840, loss = 0.00749897
I0503 08:27:14.273994 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00749885 (* 1 = 0.00749885 loss)
I0503 08:27:14.274004 2050462464 solver.cpp:464] Iteration 8840, lr = 0.001
I0503 08:28:37.219480 2050462464 solver.cpp:189] Iteration 8860, loss = 0.0023332
I0503 08:28:37.219527 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00233307 (* 1 = 0.00233307 loss)
I0503 08:28:37.219537 2050462464 solver.cpp:464] Iteration 8860, lr = 0.001
I0503 08:30:00.379050 2050462464 solver.cpp:189] Iteration 8880, loss = 0.000438373
I0503 08:30:00.379094 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000438253 (* 1 = 0.000438253 loss)
I0503 08:30:00.379104 2050462464 solver.cpp:464] Iteration 8880, lr = 0.001
I0503 08:31:23.651542 2050462464 solver.cpp:189] Iteration 8900, loss = 0.00454031
I0503 08:31:23.651592 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00454019 (* 1 = 0.00454019 loss)
I0503 08:31:23.651602 2050462464 solver.cpp:464] Iteration 8900, lr = 0.001
I0503 08:32:46.643049 2050462464 solver.cpp:189] Iteration 8920, loss = 0.00111389
I0503 08:32:46.643105 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00111377 (* 1 = 0.00111377 loss)
I0503 08:32:46.643115 2050462464 solver.cpp:464] Iteration 8920, lr = 0.001
I0503 08:34:09.780390 2050462464 solver.cpp:189] Iteration 8940, loss = 0.000760348
I0503 08:34:09.780450 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000760226 (* 1 = 0.000760226 loss)
I0503 08:34:09.780462 2050462464 solver.cpp:464] Iteration 8940, lr = 0.001
I0503 08:35:32.893872 2050462464 solver.cpp:189] Iteration 8960, loss = 0.00216531
I0503 08:35:32.893928 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00216519 (* 1 = 0.00216519 loss)
I0503 08:35:32.893937 2050462464 solver.cpp:464] Iteration 8960, lr = 0.001
I0503 08:36:55.997963 2050462464 solver.cpp:189] Iteration 8980, loss = 0.00129991
I0503 08:36:55.998013 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00129979 (* 1 = 0.00129979 loss)
I0503 08:36:55.998024 2050462464 solver.cpp:464] Iteration 8980, lr = 0.001
I0503 08:38:15.471396 2050462464 solver.cpp:334] Snapshotting to caffenet_train_background_iter_9000.caffemodel
I0503 08:38:16.994508 2050462464 solver.cpp:342] Snapshotting solver state to caffenet_train_background_iter_9000.solverstate
I0503 08:38:18.265650 2050462464 solver.cpp:266] Iteration 9000, Testing net (#0)
I0503 09:05:50.719290 2050462464 solver.cpp:315]     Test net output #0: accuracy = 0.974891
I0503 09:05:50.720211 2050462464 solver.cpp:315]     Test net output #1: loss = 0.115942 (* 1 = 0.115942 loss)
I0503 09:05:54.619076 2050462464 solver.cpp:189] Iteration 9000, loss = 6.77273e-05
I0503 09:05:54.619114 2050462464 solver.cpp:204]     Train net output #0: loss = 6.76038e-05 (* 1 = 6.76038e-05 loss)
I0503 09:05:54.619123 2050462464 solver.cpp:464] Iteration 9000, lr = 0.001
I0503 09:07:17.937579 2050462464 solver.cpp:189] Iteration 9020, loss = 0.00282421
I0503 09:07:17.937636 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00282408 (* 1 = 0.00282408 loss)
I0503 09:07:17.937647 2050462464 solver.cpp:464] Iteration 9020, lr = 0.001
I0503 09:08:40.547967 2050462464 solver.cpp:189] Iteration 9040, loss = 0.000470437
I0503 09:08:40.548022 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000470306 (* 1 = 0.000470306 loss)
I0503 09:08:40.548032 2050462464 solver.cpp:464] Iteration 9040, lr = 0.001
I0503 09:10:03.560259 2050462464 solver.cpp:189] Iteration 9060, loss = 0.00376357
I0503 09:10:03.560307 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00376343 (* 1 = 0.00376343 loss)
I0503 09:10:03.560317 2050462464 solver.cpp:464] Iteration 9060, lr = 0.001
I0503 09:11:26.601446 2050462464 solver.cpp:189] Iteration 9080, loss = 0.000752149
I0503 09:11:26.601502 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000752016 (* 1 = 0.000752016 loss)
I0503 09:11:26.601512 2050462464 solver.cpp:464] Iteration 9080, lr = 0.001
I0503 09:12:49.452471 2050462464 solver.cpp:189] Iteration 9100, loss = 0.00222339
I0503 09:12:49.452517 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00222326 (* 1 = 0.00222326 loss)
I0503 09:12:49.452528 2050462464 solver.cpp:464] Iteration 9100, lr = 0.001
I0503 09:14:12.725455 2050462464 solver.cpp:189] Iteration 9120, loss = 0.0016375
I0503 09:14:12.725505 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00163736 (* 1 = 0.00163736 loss)
I0503 09:14:12.725515 2050462464 solver.cpp:464] Iteration 9120, lr = 0.001
I0503 09:15:35.498093 2050462464 solver.cpp:189] Iteration 9140, loss = 0.00972909
I0503 09:15:35.498142 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00972896 (* 1 = 0.00972896 loss)
I0503 09:15:35.498152 2050462464 solver.cpp:464] Iteration 9140, lr = 0.001
I0503 09:16:58.578075 2050462464 solver.cpp:189] Iteration 9160, loss = 0.000447449
I0503 09:16:58.578130 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000447322 (* 1 = 0.000447322 loss)
I0503 09:16:58.578140 2050462464 solver.cpp:464] Iteration 9160, lr = 0.001
I0503 09:18:21.861130 2050462464 solver.cpp:189] Iteration 9180, loss = 0.00984439
I0503 09:18:21.861186 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00984426 (* 1 = 0.00984426 loss)
I0503 09:18:21.861196 2050462464 solver.cpp:464] Iteration 9180, lr = 0.001
I0503 09:19:45.016785 2050462464 solver.cpp:189] Iteration 9200, loss = 0.00180516
I0503 09:19:45.016851 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00180503 (* 1 = 0.00180503 loss)
I0503 09:19:45.016862 2050462464 solver.cpp:464] Iteration 9200, lr = 0.001
I0503 09:21:07.972380 2050462464 solver.cpp:189] Iteration 9220, loss = 0.000416057
I0503 09:21:07.972431 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000415931 (* 1 = 0.000415931 loss)
I0503 09:21:07.972441 2050462464 solver.cpp:464] Iteration 9220, lr = 0.001
I0503 09:22:30.889931 2050462464 solver.cpp:189] Iteration 9240, loss = 0.000448734
I0503 09:22:30.889986 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000448611 (* 1 = 0.000448611 loss)
I0503 09:22:30.889997 2050462464 solver.cpp:464] Iteration 9240, lr = 0.001
I0503 09:23:54.038044 2050462464 solver.cpp:189] Iteration 9260, loss = 6.07483e-05
I0503 09:23:54.038100 2050462464 solver.cpp:204]     Train net output #0: loss = 6.06288e-05 (* 1 = 6.06288e-05 loss)
I0503 09:23:54.038110 2050462464 solver.cpp:464] Iteration 9260, lr = 0.001
I0503 09:25:17.427136 2050462464 solver.cpp:189] Iteration 9280, loss = 3.27732e-05
I0503 09:25:17.427186 2050462464 solver.cpp:204]     Train net output #0: loss = 3.26534e-05 (* 1 = 3.26534e-05 loss)
I0503 09:25:17.427194 2050462464 solver.cpp:464] Iteration 9280, lr = 0.001
I0503 09:26:40.491397 2050462464 solver.cpp:189] Iteration 9300, loss = 0.00191721
I0503 09:26:40.491454 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00191709 (* 1 = 0.00191709 loss)
I0503 09:26:40.491463 2050462464 solver.cpp:464] Iteration 9300, lr = 0.001
I0503 09:28:03.498874 2050462464 solver.cpp:189] Iteration 9320, loss = 0.000256295
I0503 09:28:03.498926 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000256174 (* 1 = 0.000256174 loss)
I0503 09:28:03.498936 2050462464 solver.cpp:464] Iteration 9320, lr = 0.001
I0503 09:29:26.844432 2050462464 solver.cpp:189] Iteration 9340, loss = 0.00110155
I0503 09:29:26.844482 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00110143 (* 1 = 0.00110143 loss)
I0503 09:29:26.844492 2050462464 solver.cpp:464] Iteration 9340, lr = 0.001
I0503 09:30:49.938472 2050462464 solver.cpp:189] Iteration 9360, loss = 0.000106984
I0503 09:30:49.938529 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000106863 (* 1 = 0.000106863 loss)
I0503 09:30:49.938540 2050462464 solver.cpp:464] Iteration 9360, lr = 0.001
I0503 09:32:13.016646 2050462464 solver.cpp:189] Iteration 9380, loss = 0.000129572
I0503 09:32:13.016703 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000129452 (* 1 = 0.000129452 loss)
I0503 09:32:13.016715 2050462464 solver.cpp:464] Iteration 9380, lr = 0.001
I0503 09:33:36.207965 2050462464 solver.cpp:189] Iteration 9400, loss = 0.0100051
I0503 09:33:36.208019 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0100049 (* 1 = 0.0100049 loss)
I0503 09:33:36.208029 2050462464 solver.cpp:464] Iteration 9400, lr = 0.001
I0503 09:34:59.093240 2050462464 solver.cpp:189] Iteration 9420, loss = 0.00135326
I0503 09:34:59.093293 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00135313 (* 1 = 0.00135313 loss)
I0503 09:34:59.093303 2050462464 solver.cpp:464] Iteration 9420, lr = 0.001
I0503 09:36:22.356926 2050462464 solver.cpp:189] Iteration 9440, loss = 0.002918
I0503 09:36:22.356981 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00291788 (* 1 = 0.00291788 loss)
I0503 09:36:22.356992 2050462464 solver.cpp:464] Iteration 9440, lr = 0.001
I0503 09:37:45.435884 2050462464 solver.cpp:189] Iteration 9460, loss = 0.00133551
I0503 09:37:45.435936 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00133539 (* 1 = 0.00133539 loss)
I0503 09:37:45.435946 2050462464 solver.cpp:464] Iteration 9460, lr = 0.001
I0503 09:39:08.400279 2050462464 solver.cpp:189] Iteration 9480, loss = 0.000666123
I0503 09:39:08.400326 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000665997 (* 1 = 0.000665997 loss)
I0503 09:39:08.400337 2050462464 solver.cpp:464] Iteration 9480, lr = 0.001
I0503 09:40:27.635035 2050462464 solver.cpp:334] Snapshotting to caffenet_train_background_iter_9500.caffemodel
I0503 09:40:29.168906 2050462464 solver.cpp:342] Snapshotting solver state to caffenet_train_background_iter_9500.solverstate
I0503 09:40:34.409407 2050462464 solver.cpp:189] Iteration 9500, loss = 0.00225754
I0503 09:40:34.409448 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00225742 (* 1 = 0.00225742 loss)
I0503 09:40:34.409458 2050462464 solver.cpp:464] Iteration 9500, lr = 0.001
I0503 09:41:57.225757 2050462464 solver.cpp:189] Iteration 9520, loss = 0.00097641
I0503 09:41:57.225807 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000976282 (* 1 = 0.000976282 loss)
I0503 09:41:57.225817 2050462464 solver.cpp:464] Iteration 9520, lr = 0.001
I0503 09:43:20.357619 2050462464 solver.cpp:189] Iteration 9540, loss = 0.00824689
I0503 09:43:20.357666 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00824677 (* 1 = 0.00824677 loss)
I0503 09:43:20.357676 2050462464 solver.cpp:464] Iteration 9540, lr = 0.001
I0503 09:44:43.228641 2050462464 solver.cpp:189] Iteration 9560, loss = 0.000427961
I0503 09:44:43.228693 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000427831 (* 1 = 0.000427831 loss)
I0503 09:44:43.228703 2050462464 solver.cpp:464] Iteration 9560, lr = 0.001
I0503 09:46:06.151439 2050462464 solver.cpp:189] Iteration 9580, loss = 2.73064e-05
I0503 09:46:06.151494 2050462464 solver.cpp:204]     Train net output #0: loss = 2.71769e-05 (* 1 = 2.71769e-05 loss)
I0503 09:46:06.151505 2050462464 solver.cpp:464] Iteration 9580, lr = 0.001
I0503 09:47:29.372340 2050462464 solver.cpp:189] Iteration 9600, loss = 0.0158699
I0503 09:47:29.372393 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0158698 (* 1 = 0.0158698 loss)
I0503 09:47:29.372403 2050462464 solver.cpp:464] Iteration 9600, lr = 0.001
I0503 09:48:51.989542 2050462464 solver.cpp:189] Iteration 9620, loss = 0.00163404
I0503 09:48:51.989590 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00163391 (* 1 = 0.00163391 loss)
I0503 09:48:51.989601 2050462464 solver.cpp:464] Iteration 9620, lr = 0.001
I0503 09:50:14.829406 2050462464 solver.cpp:189] Iteration 9640, loss = 0.00282033
I0503 09:50:14.829459 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0028202 (* 1 = 0.0028202 loss)
I0503 09:50:14.829469 2050462464 solver.cpp:464] Iteration 9640, lr = 0.001
I0503 09:51:37.716967 2050462464 solver.cpp:189] Iteration 9660, loss = 0.0043781
I0503 09:51:37.717021 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00437797 (* 1 = 0.00437797 loss)
I0503 09:51:37.717031 2050462464 solver.cpp:464] Iteration 9660, lr = 0.001
I0503 09:53:00.373661 2050462464 solver.cpp:189] Iteration 9680, loss = 0.000200335
I0503 09:53:00.373713 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000200208 (* 1 = 0.000200208 loss)
I0503 09:53:00.373723 2050462464 solver.cpp:464] Iteration 9680, lr = 0.001
I0503 09:54:23.198961 2050462464 solver.cpp:189] Iteration 9700, loss = 0.0144587
I0503 09:54:23.199012 2050462464 solver.cpp:204]     Train net output #0: loss = 0.0144586 (* 1 = 0.0144586 loss)
I0503 09:54:23.199023 2050462464 solver.cpp:464] Iteration 9700, lr = 0.001
I0503 09:55:45.953707 2050462464 solver.cpp:189] Iteration 9720, loss = 0.000562732
I0503 09:55:45.953763 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00056261 (* 1 = 0.00056261 loss)
I0503 09:55:45.953773 2050462464 solver.cpp:464] Iteration 9720, lr = 0.001
I0503 09:57:09.213876 2050462464 solver.cpp:189] Iteration 9740, loss = 0.000382571
I0503 09:57:09.213928 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000382449 (* 1 = 0.000382449 loss)
I0503 09:57:09.213937 2050462464 solver.cpp:464] Iteration 9740, lr = 0.001
I0503 09:58:32.434577 2050462464 solver.cpp:189] Iteration 9760, loss = 0.0016493
I0503 09:58:32.434626 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00164918 (* 1 = 0.00164918 loss)
I0503 09:58:32.434635 2050462464 solver.cpp:464] Iteration 9760, lr = 0.001
I0503 09:59:55.350337 2050462464 solver.cpp:189] Iteration 9780, loss = 1.93277e-05
I0503 09:59:55.350504 2050462464 solver.cpp:204]     Train net output #0: loss = 1.9205e-05 (* 1 = 1.9205e-05 loss)
I0503 09:59:55.350515 2050462464 solver.cpp:464] Iteration 9780, lr = 0.001
I0503 10:01:18.453693 2050462464 solver.cpp:189] Iteration 9800, loss = 0.0041146
I0503 10:01:18.453752 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00411448 (* 1 = 0.00411448 loss)
I0503 10:01:18.453763 2050462464 solver.cpp:464] Iteration 9800, lr = 0.001
I0503 10:02:41.503653 2050462464 solver.cpp:189] Iteration 9820, loss = 4.502e-05
I0503 10:02:41.503703 2050462464 solver.cpp:204]     Train net output #0: loss = 4.48981e-05 (* 1 = 4.48981e-05 loss)
I0503 10:02:41.503713 2050462464 solver.cpp:464] Iteration 9820, lr = 0.001
I0503 10:04:04.488203 2050462464 solver.cpp:189] Iteration 9840, loss = 0.000758962
I0503 10:04:04.488255 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00075884 (* 1 = 0.00075884 loss)
I0503 10:04:04.488265 2050462464 solver.cpp:464] Iteration 9840, lr = 0.001
I0503 10:05:27.719677 2050462464 solver.cpp:189] Iteration 9860, loss = 0.003485
I0503 10:05:27.719733 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00348488 (* 1 = 0.00348488 loss)
I0503 10:05:27.719743 2050462464 solver.cpp:464] Iteration 9860, lr = 0.001
I0503 10:06:50.722477 2050462464 solver.cpp:189] Iteration 9880, loss = 0.000420958
I0503 10:06:50.722532 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000420835 (* 1 = 0.000420835 loss)
I0503 10:06:50.722542 2050462464 solver.cpp:464] Iteration 9880, lr = 0.001
I0503 10:08:13.562680 2050462464 solver.cpp:189] Iteration 9900, loss = 0.000464662
I0503 10:08:13.562733 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000464538 (* 1 = 0.000464538 loss)
I0503 10:08:13.562743 2050462464 solver.cpp:464] Iteration 9900, lr = 0.001
I0503 10:09:36.760710 2050462464 solver.cpp:189] Iteration 9920, loss = 0.00828621
I0503 10:09:36.760763 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00828609 (* 1 = 0.00828609 loss)
I0503 10:09:36.760773 2050462464 solver.cpp:464] Iteration 9920, lr = 0.001
I0503 10:10:59.901698 2050462464 solver.cpp:189] Iteration 9940, loss = 0.000262473
I0503 10:10:59.901751 2050462464 solver.cpp:204]     Train net output #0: loss = 0.000262351 (* 1 = 0.000262351 loss)
I0503 10:10:59.901762 2050462464 solver.cpp:464] Iteration 9940, lr = 0.001
I0503 10:12:23.116238 2050462464 solver.cpp:189] Iteration 9960, loss = 0.00148358
I0503 10:12:23.116289 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00148346 (* 1 = 0.00148346 loss)
I0503 10:12:23.116298 2050462464 solver.cpp:464] Iteration 9960, lr = 0.001
I0503 10:13:46.274391 2050462464 solver.cpp:189] Iteration 9980, loss = 0.00220324
I0503 10:13:46.274448 2050462464 solver.cpp:204]     Train net output #0: loss = 0.00220313 (* 1 = 0.00220313 loss)
I0503 10:13:46.274458 2050462464 solver.cpp:464] Iteration 9980, lr = 0.001
I0503 10:15:05.723402 2050462464 solver.cpp:334] Snapshotting to caffenet_train_background_iter_10000.caffemodel
I0503 10:15:07.319720 2050462464 solver.cpp:342] Snapshotting solver state to caffenet_train_background_iter_10000.solverstate
I0503 10:15:10.256652 2050462464 solver.cpp:248] Iteration 10000, loss = 0.00518596
I0503 10:15:10.256688 2050462464 solver.cpp:266] Iteration 10000, Testing net (#0)
I0503 10:42:47.136832 2050462464 solver.cpp:315]     Test net output #0: accuracy = 0.975219
I0503 10:42:47.136873 2050462464 solver.cpp:315]     Test net output #1: loss = 0.120748 (* 1 = 0.120748 loss)
I0503 10:42:47.136881 2050462464 solver.cpp:253] Optimization Done.
I0503 10:42:47.136888 2050462464 caffe.cpp:134] Optimization Done.
C02MX066FD58:rcc_net cusgadmin$ 
